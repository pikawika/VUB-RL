{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# CNN based DQN agent against fixed oponent\n",
    "\n",
    "As discussed in `5-improving-dqn-architecture.ipynb` we thought of three aspects that might be the root of the agent's not learning to play the game pleasingly:\n",
    "- Training two DQN agents simultaneously is known to be though, especially when starting from a random initialisation\n",
    "- The network used was a simple MLP\n",
    "- The training is not done over enough iterations\n",
    "\n",
    "In the notebooks `5-improving-dqn-architecture.ipynb` and `6-dqn-using-a-cnn.ipynb`, two alternative networks besides MLP were used.\n",
    "Whilst these give somewhat satisfactory results when trained for long enough and incentivising moves by giving a reward for making a move, it is still far from perfect.\n",
    "The iterations were also boosted to a couple of hours on a CUDA GPU, which didn't improve things all that much.\n",
    "\n",
    "Thus, what is most likely to be an issue is the fact that we are training two agents simultaneously.\n",
    "This makes it hard to get a good performing agent and makes the target non stationary as both agents evolve over time.\n",
    "An alternative to this is training an agent for a couple of epochs whilst freezing the other and alternating this between the agents.\n",
    "This makes the problem to learn more stationary and is known to make learning easier.\n",
    "What is also done, often in very complex games, is starting from a somewhat smart agent instead of a random one.\n",
    "\n",
    "Whilst some libraries such as Ray RL lib offer implementations of such a training strategy, the experimental notebook `4-rllib-for-more-learning-control.ipynb` found that even the Ray provided example results in error codes.\n",
    "Seeing their GitHub page has many open issues, the one we encountered being one of them, we refrain from using a different library considering Tianshou has many algorithms implemented and we have found a way to make things work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training two DQN agents on connect four Gym\n",
    "  - Building the environment\n",
    "  - Implementing the DQN policy\n",
    "  - Building agents\n",
    "  - Function for letting agents learn\n",
    "  - Function for watching learned agent\n",
    "  - Doing the experiment\n",
    "- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.21.0 recommended): 0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.12.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argparser\n",
    "import argparse\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.12.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# PPrint is a pretty print for variables\n",
    "from pprint import pprint\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(cfgym)\n",
    "\n",
    "# Time for allowing \"freezes\" in execution\n",
    "import time;\n",
    "\n",
    "# Allow for copying objects in a non reference manner\n",
    "import copy\n",
    "\n",
    "# Used for updating notebook display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training two DQN agents on connect four Gym\n",
    "\n",
    "Our connect four gym setup requires two agents, one for each player.\n",
    "To reduce complexity, agents will always play as the same player, e.g. always as player 1.\n",
    "It is important to note that connect four is a *solved game*.\n",
    "According to [The Washington Post](https://www.washingtonpost.com/news/wonk/wp/2015/05/08/how-to-win-any-popular-game-according-to-data-scientists/):\n",
    "\n",
    "> Connect Four is what mathematicians call a \"solved game,\" meaning you can play it perfectly every time, no matter what your opponent does. You will need to get the first move, but as long as you do so, you can always win within 41 moves.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Building the environment\n",
    "\n",
    "This code is taken from previous notebooks.\n",
    "We don't allow invalid moves to make the problem easier for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict(action_mask:Box([0 0 0 0 0 0 0], [1 1 1 1 1 1 1], (7,), int8), observation:Box([[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]], [[2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]], (6, 7), int8))\n",
      "\n",
      "Action space: Discrete(7)\n",
      "\n",
      " Initial player id:\n",
      "player_1\n",
      "\n",
      " Initial observation:\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " Initial mask:\n",
      "[True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CONNECT FOUR V2 ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "def get_env():\n",
    "    \"\"\"\n",
    "    Returns the connect four gym environment V2 altered for Tianshou and Petting Zoo compatibility.\n",
    "    Already wrapped with a ts.env.PettingZooEnv wrapper.\n",
    "    \"\"\"\n",
    "    return ts.env.PettingZooEnv(cfgym.env(reward_move= 1, # Set to 1 for reward to make moves (incentivise longer games)\n",
    "                                          reward_invalid= -3,\n",
    "                                          reward_draw= 100,\n",
    "                                          reward_win= 25,\n",
    "                                          reward_loss= -25,\n",
    "                                          allow_invalid_move= False))\n",
    "    \n",
    "    \n",
    "# Test the environment\n",
    "env = get_env()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"\\n Initial player id:\")\n",
    "print(observation[\"agent_id\"])\n",
    "\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation[\"obs\"])\n",
    "\n",
    "print(\"\\n Initial mask:\")\n",
    "print(observation[\"mask\"])\n",
    "\n",
    "# Clean unused variables\n",
    "del observation\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7e7da",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Implementing the DQN policy\n",
    "\n",
    "The DQN policy for the agent is configured and set up below.\n",
    "This is identical to the previous notebook with the added option of \"freezing\" an agent which corresponds to giving it an optimizer with learning rate 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2f43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN ARCHITECTURE\n",
    "####################################################\n",
    "\n",
    "class CNNBasedDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        # Number of input channels\n",
    "        input_channels_cnn = 1\n",
    "        output_channels_cnn = 32\n",
    "        flatten_size = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        output_size= np.prod(action_shape)\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(0,-1),\n",
    "            torch.nn.Unflatten(0, (1, flatten_size)),\n",
    "            torch.nn.Linear(flatten_size, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        \n",
    "        logits = self.model(obs)\n",
    "        return logits, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539b56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY\n",
    "####################################################\n",
    "\n",
    "def cf_cnn_dqn_policy(state_shape: tuple,\n",
    "                      action_shape: tuple,\n",
    "                      optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                      learning_rate: float =  0.0001,\n",
    "                      gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                      n_step: int = 4, # Number of steps to look ahead\n",
    "                      frozen: bool = False,\n",
    "                      target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CNNBasedDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # If we are frozen, we use an optimizer that has learning rate 0\n",
    "    if frozen:\n",
    "        optim = torch.optim.SGD(net.parameters(), lr= 0)\n",
    "        \n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd411318",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Building agents\n",
    "\n",
    "This is identical to the previous notebook with the added option of \"freezing\" an agent which corresponds to giving it an optimizer with learning rate 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ae27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT CREATION\n",
    "####################################################\n",
    "\n",
    "def get_agents(agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "               agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "               optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "               resume_path_player_1: str = '', # Path to file to resume agent training from\n",
    "               resume_path_player_2: str = '', \n",
    "               agent_player1_frozen: bool = False, # Freeze a player -> don't let it learn further\n",
    "               agent_player2_frozen: bool = False,\n",
    "               ) -> typing.Tuple[ts.policy.BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \"\"\"\n",
    "    Gets a multi agent policy manager, optimizer and player ids for the connect four V2 gym environment.\n",
    "    Per default this returns \n",
    "        - Multi agent manager for 2 agents using DQN\n",
    "        - Adam optimizer\n",
    "        - ['player_1', 'player_2'] from the connect four environment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the environment to play in (Connect four gym V2)\n",
    "    env = get_env()\n",
    "    \n",
    "    # Get the observation space from the environment, depending on typo of space (ternary operator)\n",
    "    observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "    \n",
    "    # Set the arguments\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    \n",
    "    # Configure agent player 1 to be a DQN if no policy is passed.\n",
    "    if agent_player1 is None:\n",
    "        # Our agent1 uses a DQN policy\n",
    "        agent_player1 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                                          action_shape= action_shape,\n",
    "                                          optim= optim,\n",
    "                                          frozen= agent_player1_frozen)\n",
    "        \n",
    "        # If we resume our agent we need to load the previous config\n",
    "        if resume_path_player_1:\n",
    "            agent_player1.load_state_dict(torch.load(resume_path_player_1))\n",
    "            \n",
    "    \n",
    "    # Configure agent player 2 to be a DQN if no policy is passed.\n",
    "    if agent_player2 is None:\n",
    "        # Our agent1 uses a DQN policy\n",
    "        agent_player2 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                                          action_shape= action_shape,\n",
    "                                          optim= optim,\n",
    "                                          frozen= agent_player2_frozen)\n",
    "        \n",
    "        # If we resume our agent we need to load the previous config\n",
    "        if resume_path_player_2:\n",
    "            agent_player2.load_state_dict(torch.load(resume_path_player_2))\n",
    "\n",
    "    # Both our agents are DQN agents by default\n",
    "    agents = [agent_player1, agent_player2]\n",
    "        \n",
    "    # Our policy depends on the order of the agents\n",
    "    policy = ts.policy.MultiAgentPolicyManager(agents, env)\n",
    "    \n",
    "    # Return our policy, optimizer and the available agents in the environment\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using DQN\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    \n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8f3a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for letting agents learn\n",
    "\n",
    "This is identical to the previous notebook with the added option of \"freezing\" an agent which corresponds to giving it an optimizer with learning rate 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a32c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT TRAINING\n",
    "####################################################\n",
    "\n",
    "def train_agent(filename: str = \"dqn_vs_dqn_cnn_based\",\n",
    "                agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                agent_player1_frozen: bool = False, # Freeze a player -> don't let it learn further\n",
    "                agent_player2_frozen: bool = False,\n",
    "                single_agent_score_as_reward: bool= False, # Uses non frozen agent's score as reward\n",
    "                optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                training_env_num: int = 1,\n",
    "                testing_env_num: int = 1,\n",
    "                buffer_size: int = 2^14,\n",
    "                batch_size: int = 1, \n",
    "                epochs: int = 50, #50\n",
    "                step_per_epoch: int = 1024, #1024\n",
    "                step_per_collect: int = 64, # transition before update\n",
    "                update_per_step: float = 0.1,\n",
    "                testing_eps: float = 0.05,\n",
    "                training_eps: float = 0.1,\n",
    "                ) -> typing.Tuple[dict, ts.policy.BasePolicy]:\n",
    "    \"\"\"\n",
    "    Trains two agents in the connect four V2 environment and saves their best model and logs.\n",
    "    Returns:\n",
    "        - result from offpolicy_trainer\n",
    "        - final version of agent 1\n",
    "        - final version of agent 2\n",
    "    \"\"\"\n",
    "\n",
    "    # ======== notebook specific =========\n",
    "    notebook_version = '7' # Used for foldering logs and models\n",
    "\n",
    "    # ======== environment setup =========\n",
    "    train_envs = ts.env.DummyVectorEnv([get_env for _ in range(training_env_num)])\n",
    "    test_envs = ts.env.DummyVectorEnv([get_env for _ in range(testing_env_num)])\n",
    "    \n",
    "    # set the seed for reproducibility\n",
    "    np.random.seed(1998)\n",
    "    torch.manual_seed(1998)\n",
    "    train_envs.seed(1998)\n",
    "    test_envs.seed(1998)\n",
    "\n",
    "    # ======== agent setup =========\n",
    "    # Gets our agents from the previously made function\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using DQN\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    policy, optim, agents = get_agents(agent_player1=agent_player1,\n",
    "                                       agent_player2=agent_player2,\n",
    "                                       agent_player1_frozen= agent_player1_frozen,\n",
    "                                       agent_player2_frozen= agent_player2_frozen,\n",
    "                                       optim=optim)\n",
    "\n",
    "    # ======== collector setup =========\n",
    "    # Make a collector for the training environments\n",
    "    train_collector = ts.data.Collector(policy= policy,\n",
    "                                        env= train_envs,\n",
    "                                        buffer= ts.data.VectorReplayBuffer(buffer_size, len(train_envs)),\n",
    "                                        exploration_noise= True)\n",
    "    \n",
    "    # Make a collector for the testing environments\n",
    "    test_collector = ts.data.Collector(policy= policy,\n",
    "                                       env= test_envs,\n",
    "                                       buffer= ts.data.VectorReplayBuffer(buffer_size, len(test_envs)),\n",
    "                                       exploration_noise= True)\n",
    "    \n",
    "    # Uncomment below if you want to set epsilon in epsilon policy\n",
    "    # policy.set_eps(1)\n",
    "    \n",
    "    # Collect data fot the training evnironments\n",
    "    train_collector.collect(n_step= batch_size * training_env_num)\n",
    "    \n",
    "    # ======== ensure folders exist =========\n",
    "    if not os.path.exists(os.path.join('./logs', 'paper_notebooks', notebook_version, filename)):\n",
    "        os.makedirs(os.path.join('./logs', 'paper_notebooks', notebook_version, filename))\n",
    "    if not os.path.exists(os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename)):\n",
    "        os.makedirs(os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename))\n",
    "\n",
    "    # ======== tensorboard logging setup =========\n",
    "    # Allows to save the training progress to tensorboard compatable logs\n",
    "    log_path = os.path.join('./logs', 'paper_notebooks', notebook_version, filename)\n",
    "    writer = torch.utils.tensorboard.SummaryWriter(log_path)\n",
    "    logger = ts.utils.TensorboardLogger(writer)\n",
    "\n",
    "    # ======== callback functions used during training =========\n",
    "    # We want to save our best policy\n",
    "    def save_best_fn(policy):\n",
    "        \"\"\"\n",
    "        Callback to save the best model\n",
    "        \"\"\"\n",
    "        # Save best agent 1\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'best_policy_agent1.pth')\n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save best agent 2\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'best_policy_agent2.pth')\n",
    "        torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save agent2\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        \"\"\"\n",
    "        Callback to stop training when we've reached the win rate\n",
    "        \"\"\"\n",
    "        return mean_rewards >= 7 # (win = 10, 70% win without invalid moves = mean of 7)\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback before training\n",
    "        \"\"\"        \n",
    "        # Before training we want to configure the epsilon for the agents\n",
    "        # In general more exploratory than the test case\n",
    "        policy.policies[agents[0]].set_eps(training_eps)\n",
    "        policy.policies[agents[1]].set_eps(training_eps)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback beore testing\n",
    "        \"\"\"        \n",
    "        # Before testing we want to configure the epsilon for the agents\n",
    "        # In general more greedy than the train case but not\n",
    "        #   to avoid getting stuck on invalid moves\n",
    "        policy.policies[agents[0]].set_eps(testing_eps)\n",
    "        policy.policies[agents[1]].set_eps(testing_eps)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        \"\"\"\n",
    "        Callback for reward collection\n",
    "        \"\"\"        \n",
    "        if agent_player2_frozen and single_agent_score_as_reward:\n",
    "            # agent 2 frozen, optimizing for agent 1\n",
    "            return rews[:, 0]\n",
    "        \n",
    "        if agent_player1_frozen and single_agent_score_as_reward:\n",
    "            # agent 1 frozen, optimizing for agent 2\n",
    "            return rews[:, 1]\n",
    "        \n",
    "        # Per default we are interested in optimizing both agents\n",
    "        return rews[:, 0] + rews[:, 1]\n",
    "    \n",
    "            \n",
    "\n",
    "    # trainer\n",
    "    result = ts.trainer.offpolicy_trainer(policy= policy,\n",
    "                                          train_collector= train_collector,\n",
    "                                          test_collector= test_collector,\n",
    "                                          max_epoch= epochs,\n",
    "                                          step_per_epoch= step_per_epoch,\n",
    "                                          step_per_collect= step_per_collect,\n",
    "                                          episode_per_test= testing_env_num,\n",
    "                                          batch_size= batch_size,\n",
    "                                          train_fn= train_fn,\n",
    "                                          test_fn= test_fn,\n",
    "                                          # Stop function to stop before specified amount of epochs\n",
    "                                          #stop_fn= stop_fn\n",
    "                                          save_best_fn= save_best_fn,\n",
    "                                          update_per_step= update_per_step,\n",
    "                                          logger= logger,\n",
    "                                          test_in_train= False,\n",
    "                                          reward_metric= reward_metric)\n",
    "    \n",
    "    # Save final agent 1\n",
    "    model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'final_policy_agent1.pth')\n",
    "    torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "\n",
    "    # Save final agent 2\n",
    "    model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'final_policy_agent2.pth')\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "    return result, policy.policies[agents[0]], policy.policies[agents[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161cc0f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for watching learned agent\n",
    "\n",
    "Identical to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# WATCHING THE LEARNED POLICY IN ACTION\n",
    "####################################################\n",
    "\n",
    "def watch(numer_of_games: int = 3,\n",
    "          agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          test_epsilon: float = 0.05, # For the watching we act completely greedy but low random for not getting stuck on invalid move\n",
    "          render_speed: float = 0.15, # Amount of seconds to update frame/ do a step\n",
    "          ) -> None:\n",
    "    \n",
    "    # Get the connect four V2 environment (must be a list)\n",
    "    env= ts.env.DummyVectorEnv([get_env])\n",
    "    \n",
    "    # Get the agents from the trained agents\n",
    "    policy, optim, agents = get_agents(agent_player1= agent_player1,\n",
    "                                       agent_player2= agent_player2)\n",
    "    \n",
    "    # Evaluate the policy\n",
    "    policy.eval()\n",
    "    \n",
    "    # Set the testing policy epsilon for our agents\n",
    "    policy.policies[agents[0]].set_eps(test_epsilon)\n",
    "    policy.policies[agents[1]].set_eps(test_epsilon)\n",
    "    \n",
    "    # Collect the test data\n",
    "    collector = ts.data.Collector(policy= policy,\n",
    "                                  env= env,\n",
    "                                  exploration_noise= True)\n",
    "    \n",
    "    # Render games in human mode to see how it plays\n",
    "    result = collector.collect(n_episode= numer_of_games, render= render_speed)\n",
    "    \n",
    "    # Close the environment aftering collecting the results\n",
    "    # This closes the pygame window after completion\n",
    "    env.close()\n",
    "    \n",
    "    # Get the rewards and length from the test trials\n",
    "    rewards, length = result[\"rews\"], result[\"lens\"]\n",
    "    \n",
    "    # Print the final reward for the first agent\n",
    "    print(f\"Average steps of game:  {length.mean()}\")\n",
    "    print(f\"Final mean reward agent 1: {rewards[:, 0].mean()}, std: {rewards[:, 0].std()}\")\n",
    "    print(f\"Final mean reward agent 2: {rewards[:, 1].mean()}, std: {rewards[:, 1].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31784e9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Doing the experiment\n",
    "\n",
    "We now do the experiment with using our previously created functions.\n",
    "We freeze one agent and initialize both agents from previous versions.\n",
    "\n",
    "1. Freeze agent 1, train agent 2:\n",
    "    - Model save name: `1-cnn_dqn_frozen_agent1` \n",
    "    - Agent 1 start: `./saved_variables/paper_notebooks/6/dqn_vs_dqn_cnn_based/best_policy_agent1.pth`\n",
    "    - Agent 2 start: `./saved_variables/paper_notebooks/6/dqn_vs_dqn_cnn_based/best_policy_agent2.pth`\n",
    "    - Learning rate: `0.0001`\n",
    "    - Training epsilon: `0.2`\n",
    "    - Look ahead steps: `4`\n",
    "    - Reward for move/invalid: `+1` / `-3`\n",
    "    - Epochs: `1000`\n",
    "    - Gamma: `0.9`\n",
    "    - Best epoch: `81` with test reward `2004`\n",
    "    - Scoring: sum of `both` agent's score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe8961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1025it [00:04, 222.68it/s, env_step=1024, len=17, n/ep=4, n/st=64, player_1/loss=1234.796, player_2/loss=1408.932, rew=165.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 189.000000 ± 0.000000, best_reward: 299.000000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1025it [00:03, 271.09it/s, env_step=2048, len=26, n/ep=3, n/st=64, player_1/loss=1219.976, player_2/loss=1028.928, rew=356.33]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 189.000000 ± 0.000000, best_reward: 299.000000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1025it [00:03, 258.84it/s, env_step=3072, len=33, n/ep=2, n/st=64, player_1/loss=1461.200, player_2/loss=1474.581, rew=580.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 377.000000 ± 0.000000, best_reward: 377.000000 ± 0.000000 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1025it [00:03, 271.49it/s, env_step=4096, len=27, n/ep=2, n/st=64, player_1/loss=1760.171, player_2/loss=1222.666, rew=596.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 740.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1025it [00:03, 258.01it/s, env_step=5120, len=34, n/ep=2, n/st=64, player_1/loss=1622.691, player_2/loss=1026.874, rew=626.50]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 209.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1025it [00:04, 254.01it/s, env_step=6144, len=36, n/ep=1, n/st=64, player_1/loss=1250.204, player_2/loss=1158.131, rew=665.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 405.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 1025it [00:04, 249.78it/s, env_step=7168, len=21, n/ep=3, n/st=64, player_1/loss=865.644, player_2/loss=1138.948, rew=251.00]                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 377.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 1025it [00:04, 248.34it/s, env_step=8192, len=25, n/ep=3, n/st=64, player_1/loss=950.181, player_2/loss=999.754, rew=333.00]                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 405.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1025it [00:03, 283.67it/s, env_step=9216, len=22, n/ep=3, n/st=64, player_1/loss=1297.844, player_2/loss=903.946, rew=284.33]                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 324.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1025it [00:03, 270.59it/s, env_step=10240, len=27, n/ep=3, n/st=64, player_1/loss=1761.046, player_2/loss=768.911, rew=386.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 350.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 1025it [00:04, 221.78it/s, env_step=11264, len=24, n/ep=3, n/st=64, player_1/loss=1820.466, player_2/loss=760.583, rew=349.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 377.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 1025it [00:04, 210.55it/s, env_step=12288, len=16, n/ep=4, n/st=64, player_1/loss=1360.327, player_2/loss=771.283, rew=168.75]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 1025it [00:04, 219.51it/s, env_step=13312, len=13, n/ep=5, n/st=64, player_1/loss=1015.844, player_2/loss=1182.389, rew=96.60]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 1025it [00:04, 240.35it/s, env_step=14336, len=12, n/ep=5, n/st=64, player_1/loss=1060.797, player_2/loss=1628.755, rew=83.60]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 1025it [00:03, 265.89it/s, env_step=15360, len=14, n/ep=4, n/st=64, player_1/loss=1333.978, player_2/loss=1193.047, rew=124.25]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 1025it [00:03, 263.19it/s, env_step=16384, len=12, n/ep=5, n/st=64, player_1/loss=826.650, player_2/loss=968.032, rew=82.60]                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 1025it [00:04, 256.18it/s, env_step=17408, len=18, n/ep=3, n/st=64, player_1/loss=1060.935, player_2/loss=782.600, rew=190.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 1025it [00:04, 255.41it/s, env_step=18432, len=26, n/ep=3, n/st=64, player_1/loss=906.470, player_2/loss=975.058, rew=359.67]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 1025it [00:03, 257.12it/s, env_step=19456, len=26, n/ep=2, n/st=64, player_1/loss=1159.118, player_2/loss=813.802, rew=363.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 135.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 1025it [00:04, 252.77it/s, env_step=20480, len=14, n/ep=5, n/st=64, player_1/loss=2086.208, player_2/loss=556.803, rew=121.60]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 189.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 1025it [00:04, 253.25it/s, env_step=21504, len=18, n/ep=3, n/st=64, player_1/loss=1708.147, player_2/loss=390.459, rew=197.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 1025it [00:04, 245.28it/s, env_step=22528, len=24, n/ep=3, n/st=64, player_1/loss=1374.800, player_2/loss=506.365, rew=307.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 1025it [00:04, 244.24it/s, env_step=23552, len=21, n/ep=2, n/st=64, player_1/loss=1657.106, player_2/loss=542.337, rew=232.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 1025it [00:03, 282.55it/s, env_step=24576, len=22, n/ep=3, n/st=64, player_1/loss=1290.094, player_2/loss=482.748, rew=267.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 1025it [00:04, 225.91it/s, env_step=25600, len=21, n/ep=3, n/st=64, player_1/loss=1025.170, player_2/loss=304.605, rew=231.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: 350.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 1025it [00:04, 235.31it/s, env_step=26624, len=20, n/ep=2, n/st=64, player_1/loss=1336.423, player_2/loss=230.248, rew=213.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: 527.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 1025it [00:04, 241.77it/s, env_step=27648, len=27, n/ep=3, n/st=64, player_1/loss=981.224, player_2/loss=330.145, rew=391.33]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 1025it [00:04, 239.26it/s, env_step=28672, len=31, n/ep=2, n/st=64, player_1/loss=817.051, player_2/loss=282.418, rew=499.50]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: 405.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 1025it [00:04, 255.04it/s, env_step=29696, len=26, n/ep=2, n/st=64, player_1/loss=655.710, player_2/loss=271.797, rew=358.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: 209.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 1025it [00:03, 262.14it/s, env_step=30720, len=26, n/ep=2, n/st=64, player_1/loss=1227.534, player_2/loss=433.824, rew=391.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 1025it [00:03, 268.76it/s, env_step=31744, len=23, n/ep=4, n/st=64, player_1/loss=1759.458, player_2/loss=457.118, rew=325.75]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: 299.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 1025it [00:03, 269.59it/s, env_step=32768, len=30, n/ep=3, n/st=64, player_1/loss=1566.264, player_2/loss=381.017, rew=484.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 1025it [00:03, 276.81it/s, env_step=33792, len=29, n/ep=2, n/st=64, player_1/loss=1576.034, player_2/loss=403.545, rew=450.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: 104.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 1025it [00:03, 265.95it/s, env_step=34816, len=22, n/ep=2, n/st=64, player_1/loss=1812.989, player_2/loss=275.546, rew=269.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: 324.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 1025it [00:03, 263.47it/s, env_step=35840, len=20, n/ep=3, n/st=64, player_1/loss=1670.487, player_2/loss=368.849, rew=211.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: 189.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 1025it [00:04, 246.08it/s, env_step=36864, len=25, n/ep=3, n/st=64, player_1/loss=1573.598, player_2/loss=473.556, rew=386.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: 495.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 1025it [00:04, 250.35it/s, env_step=37888, len=26, n/ep=3, n/st=64, player_1/loss=1447.991, player_2/loss=318.897, rew=360.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 1025it [00:04, 249.27it/s, env_step=38912, len=20, n/ep=4, n/st=64, player_1/loss=1064.615, player_2/loss=433.760, rew=226.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 1025it [00:04, 251.67it/s, env_step=39936, len=21, n/ep=3, n/st=64, player_1/loss=920.289, player_2/loss=358.639, rew=248.67]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: 324.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 1025it [00:03, 258.12it/s, env_step=40960, len=21, n/ep=3, n/st=64, player_1/loss=675.914, player_2/loss=404.411, rew=254.67]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: 324.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 1025it [00:04, 253.81it/s, env_step=41984, len=27, n/ep=2, n/st=64, player_1/loss=1005.679, player_2/loss=179.320, rew=392.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: 324.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 1025it [00:03, 262.97it/s, env_step=43008, len=23, n/ep=2, n/st=64, player_1/loss=2222.922, player_2/loss=280.671, rew=279.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 1025it [00:04, 244.35it/s, env_step=44032, len=34, n/ep=2, n/st=64, player_1/loss=2530.248, player_2/loss=342.873, rew=594.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: 275.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 1025it [00:04, 247.05it/s, env_step=45056, len=17, n/ep=3, n/st=64, player_1/loss=1737.153, player_2/loss=243.672, rew=196.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: 405.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 1025it [00:03, 256.70it/s, env_step=46080, len=23, n/ep=3, n/st=64, player_1/loss=2023.042, player_2/loss=482.002, rew=314.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: 377.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 1025it [00:04, 246.45it/s, env_step=47104, len=32, n/ep=2, n/st=64, player_1/loss=1371.614, player_2/loss=507.821, rew=544.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: 65.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 1025it [00:04, 242.98it/s, env_step=48128, len=21, n/ep=3, n/st=64, player_1/loss=1019.412, rew=247.67]                                                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: 189.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 1025it [00:04, 247.14it/s, env_step=49152, len=21, n/ep=4, n/st=64, player_1/loss=1847.482, player_2/loss=367.699, rew=236.75]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: 209.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 1025it [00:04, 242.85it/s, env_step=50176, len=33, n/ep=2, n/st=64, player_1/loss=1718.620, player_2/loss=477.948, rew=580.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: 527.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 1025it [00:04, 245.15it/s, env_step=51200, len=26, n/ep=3, n/st=64, player_1/loss=934.325, player_2/loss=621.043, rew=414.67]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50: test_reward: 230.000000 ± 0.000000, best_reward: 740.000000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 1025it [00:04, 243.13it/s, env_step=52224, len=29, n/ep=2, n/st=64, player_1/loss=870.883, player_2/loss=517.188, rew=452.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: 1102.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 1025it [00:04, 253.22it/s, env_step=53248, len=28, n/ep=2, n/st=64, player_1/loss=1508.164, player_2/loss=510.001, rew=455.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #52: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 1025it [00:03, 256.80it/s, env_step=54272, len=22, n/ep=3, n/st=64, player_1/loss=1850.652, player_2/loss=420.011, rew=281.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54: 1025it [00:04, 242.17it/s, env_step=55296, len=23, n/ep=3, n/st=64, player_1/loss=1589.145, player_2/loss=383.464, rew=285.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #54: test_reward: 252.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #55: 1025it [00:04, 253.11it/s, env_step=56320, len=21, n/ep=3, n/st=64, player_1/loss=1177.541, player_2/loss=213.640, rew=246.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #56: 1025it [00:03, 263.46it/s, env_step=57344, len=17, n/ep=4, n/st=64, player_1/loss=1332.755, player_2/loss=329.987, rew=174.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #56: test_reward: 170.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #57: 1025it [00:04, 253.36it/s, env_step=58368, len=16, n/ep=3, n/st=64, player_1/loss=2511.731, player_2/loss=478.852, rew=168.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #57: test_reward: 230.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #58: 1025it [00:04, 252.52it/s, env_step=59392, len=22, n/ep=3, n/st=64, player_1/loss=2885.547, player_2/loss=359.132, rew=253.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #58: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #59: 1025it [00:04, 243.85it/s, env_step=60416, len=20, n/ep=4, n/st=64, player_1/loss=1786.451, player_2/loss=168.735, rew=241.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #59: test_reward: 324.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #60: 1025it [00:04, 227.57it/s, env_step=61440, len=23, n/ep=2, n/st=64, player_1/loss=1558.627, player_2/loss=399.478, rew=323.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #60: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #61: 1025it [00:04, 235.49it/s, env_step=62464, len=21, n/ep=2, n/st=64, player_1/loss=1541.955, player_2/loss=504.358, rew=230.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #61: test_reward: 209.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #62: 1025it [00:04, 238.40it/s, env_step=63488, len=21, n/ep=3, n/st=64, player_1/loss=1207.144, player_2/loss=394.423, rew=231.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #62: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #63: 1025it [00:04, 242.15it/s, env_step=64512, len=19, n/ep=3, n/st=64, player_1/loss=1602.339, player_2/loss=428.225, rew=202.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #63: test_reward: 252.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #64: 1025it [00:04, 245.42it/s, env_step=65536, len=26, n/ep=2, n/st=64, player_1/loss=1546.849, player_2/loss=316.952, rew=366.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #64: test_reward: 170.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #65: 1025it [00:04, 253.61it/s, env_step=66560, len=24, n/ep=3, n/st=64, player_1/loss=1378.713, player_2/loss=223.837, rew=323.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #65: test_reward: 152.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #66: 1025it [00:04, 253.77it/s, env_step=67584, len=17, n/ep=3, n/st=64, player_1/loss=1388.300, player_2/loss=242.970, rew=152.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #66: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #67: 1025it [00:03, 260.53it/s, env_step=68608, len=18, n/ep=4, n/st=64, player_2/loss=327.920, rew=187.25]                                                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #67: test_reward: 135.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #68: 1025it [00:04, 247.65it/s, env_step=69632, len=11, n/ep=5, n/st=64, player_1/loss=765.544, player_2/loss=591.183, rew=75.80]                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #68: test_reward: 44.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #69: 1025it [00:04, 245.04it/s, env_step=70656, len=15, n/ep=4, n/st=64, player_1/loss=449.365, player_2/loss=467.045, rew=131.25]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #69: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #70: 1025it [00:04, 245.38it/s, env_step=71680, len=14, n/ep=4, n/st=64, player_1/loss=1261.878, player_2/loss=284.054, rew=121.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #70: test_reward: 104.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #71: 1025it [00:04, 248.28it/s, env_step=72704, len=16, n/ep=4, n/st=64, player_1/loss=1054.623, player_2/loss=234.881, rew=139.75]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #71: test_reward: 189.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #72: 1025it [00:04, 250.56it/s, env_step=73728, len=14, n/ep=5, n/st=64, player_1/loss=1611.807, player_2/loss=297.387, rew=108.40]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #72: test_reward: 135.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #73: 1025it [00:04, 237.11it/s, env_step=74752, len=18, n/ep=3, n/st=64, player_1/loss=1728.447, player_2/loss=309.022, rew=181.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #73: test_reward: 170.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #74: 1025it [00:04, 230.65it/s, env_step=75776, len=18, n/ep=3, n/st=64, player_1/loss=1260.564, player_2/loss=472.395, rew=207.33]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #74: test_reward: 299.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #75: 1025it [00:04, 221.94it/s, env_step=76800, len=31, n/ep=2, n/st=64, player_1/loss=1596.509, player_2/loss=531.687, rew=532.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #75: test_reward: 740.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #76: 1025it [00:03, 268.46it/s, env_step=77824, len=16, n/ep=4, n/st=64, player_1/loss=1283.918, player_2/loss=315.472, rew=175.75]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #76: test_reward: 1102.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #77: 1025it [00:03, 270.01it/s, env_step=78848, len=30, n/ep=2, n/st=64, player_1/loss=1107.359, player_2/loss=164.985, rew=480.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #77: test_reward: 594.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #78: 1025it [00:03, 267.17it/s, env_step=79872, len=16, n/ep=4, n/st=64, player_1/loss=1463.011, player_2/loss=185.715, rew=142.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #78: test_reward: 152.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #79: 1025it [00:03, 282.00it/s, env_step=80896, len=31, n/ep=2, n/st=64, player_1/loss=1330.507, player_2/loss=360.558, rew=499.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #79: test_reward: 405.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #80: 1025it [00:03, 283.27it/s, env_step=81920, len=21, n/ep=3, n/st=64, player_1/loss=1177.728, player_2/loss=562.289, rew=240.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #80: test_reward: 275.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #81: 1025it [00:03, 283.60it/s, env_step=82944, len=20, n/ep=3, n/st=64, player_1/loss=1469.440, player_2/loss=441.958, rew=219.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #81: test_reward: 230.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #82: 1025it [00:03, 282.65it/s, env_step=83968, len=19, n/ep=4, n/st=64, player_1/loss=1407.672, player_2/loss=196.418, rew=217.25]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #82: test_reward: 189.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #83: 1025it [00:03, 275.58it/s, env_step=84992, len=42, n/ep=1, n/st=64, player_1/loss=1980.899, player_2/loss=517.292, rew=1102.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #83: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #84: 1025it [00:04, 255.10it/s, env_step=86016, len=25, n/ep=2, n/st=64, player_1/loss=2042.680, player_2/loss=822.454, rew=408.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #84: test_reward: 527.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #85: 1025it [00:04, 235.02it/s, env_step=87040, len=27, n/ep=3, n/st=64, player_2/loss=549.560, rew=410.33]                                                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #85: test_reward: 464.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #86: 1025it [00:03, 265.39it/s, env_step=88064, len=29, n/ep=2, n/st=64, player_1/loss=1787.376, player_2/loss=408.316, rew=450.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #86: test_reward: 405.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #87: 1025it [00:04, 253.24it/s, env_step=89088, len=35, n/ep=2, n/st=64, player_1/loss=1318.141, player_2/loss=200.193, rew=631.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #87: test_reward: 1102.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #88: 1025it [00:04, 238.99it/s, env_step=90112, len=25, n/ep=3, n/st=64, player_1/loss=801.220, player_2/loss=170.787, rew=340.33]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #88: test_reward: 405.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #89: 1025it [00:04, 219.65it/s, env_step=91136, len=36, n/ep=2, n/st=64, player_1/loss=1418.512, player_2/loss=338.273, rew=667.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #89: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #90: 1025it [00:04, 227.12it/s, env_step=92160, len=15, n/ep=5, n/st=64, player_1/loss=1137.844, player_2/loss=370.342, rew=138.80]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #90: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #91: 1025it [00:04, 235.32it/s, env_step=93184, len=21, n/ep=3, n/st=64, player_1/loss=801.613, player_2/loss=317.654, rew=329.33]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #91: test_reward: 135.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #92: 1025it [00:04, 228.67it/s, env_step=94208, len=27, n/ep=3, n/st=64, player_1/loss=1228.839, player_2/loss=395.113, rew=430.67]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #92: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #93: 1025it [00:03, 258.84it/s, env_step=95232, len=29, n/ep=2, n/st=64, player_1/loss=1558.193, rew=618.50]                                                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #93: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #94: 1025it [00:03, 257.07it/s, env_step=96256, len=25, n/ep=3, n/st=64, player_1/loss=778.026, player_2/loss=312.903, rew=342.00]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #94: test_reward: 324.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #95: 1025it [00:03, 259.95it/s, env_step=97280, len=22, n/ep=4, n/st=64, player_1/loss=1085.865, player_2/loss=446.880, rew=329.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #95: test_reward: 54.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #96: 1025it [00:04, 232.24it/s, env_step=98304, len=28, n/ep=2, n/st=64, player_1/loss=1315.327, player_2/loss=480.123, rew=419.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #96: test_reward: 377.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #97: 1025it [00:04, 236.65it/s, env_step=99328, len=24, n/ep=3, n/st=64, player_1/loss=946.653, player_2/loss=425.581, rew=319.33]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #97: test_reward: 350.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #98: 1025it [00:04, 248.68it/s, env_step=100352, len=31, n/ep=2, n/st=64, player_1/loss=822.483, player_2/loss=625.786, rew=519.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #98: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #99: 1025it [00:04, 251.45it/s, env_step=101376, len=20, n/ep=2, n/st=64, player_1/loss=1046.026, player_2/loss=795.605, rew=285.50]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #99: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #100: 1025it [00:03, 281.07it/s, env_step=102400, len=30, n/ep=2, n/st=64, player_1/loss=1320.305, player_2/loss=526.747, rew=507.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #100: test_reward: 740.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #101: 1025it [00:03, 271.85it/s, env_step=103424, len=26, n/ep=3, n/st=64, player_1/loss=1482.013, player_2/loss=325.346, rew=358.33]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #101: test_reward: 464.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #102: 1025it [00:04, 255.09it/s, env_step=104448, len=32, n/ep=2, n/st=64, player_1/loss=1202.939, player_2/loss=283.745, rew=546.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #102: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #103: 1025it [00:03, 260.07it/s, env_step=105472, len=21, n/ep=3, n/st=64, player_1/loss=1210.296, player_2/loss=526.936, rew=321.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #103: test_reward: 135.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #104: 1025it [00:03, 269.94it/s, env_step=106496, len=31, n/ep=2, n/st=64, player_1/loss=1647.306, player_2/loss=432.823, rew=513.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #104: test_reward: 740.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #105: 1025it [00:03, 268.95it/s, env_step=107520, len=13, n/ep=5, n/st=64, player_1/loss=1553.827, player_2/loss=616.143, rew=92.60]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #105: test_reward: 104.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #106: 1025it [00:03, 256.94it/s, env_step=108544, len=9, n/ep=7, n/st=64, player_1/loss=1279.795, player_2/loss=477.355, rew=47.29]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #106: test_reward: 27.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #107: 1025it [00:03, 265.09it/s, env_step=109568, len=10, n/ep=4, n/st=64, player_1/loss=1457.463, player_2/loss=336.129, rew=61.75]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #107: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #108: 1025it [00:03, 262.63it/s, env_step=110592, len=16, n/ep=4, n/st=64, player_1/loss=1069.482, player_2/loss=446.602, rew=149.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #108: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #109: 1025it [00:04, 244.79it/s, env_step=111616, len=23, n/ep=3, n/st=64, player_2/loss=483.772, rew=286.33]                                                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #109: test_reward: 560.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #110: 1025it [00:04, 238.94it/s, env_step=112640, len=16, n/ep=3, n/st=64, player_2/loss=330.496, rew=146.33]                                                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #110: test_reward: 152.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #111: 1025it [00:04, 244.09it/s, env_step=113664, len=27, n/ep=2, n/st=64, player_1/loss=1901.505, player_2/loss=278.223, rew=395.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #111: test_reward: 464.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #112: 1025it [00:04, 235.13it/s, env_step=114688, len=24, n/ep=2, n/st=64, player_1/loss=2298.381, player_2/loss=496.783, rew=323.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #112: test_reward: 434.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #113: 1025it [00:04, 244.36it/s, env_step=115712, len=21, n/ep=3, n/st=64, player_1/loss=1187.388, player_2/loss=518.742, rew=231.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #113: test_reward: 189.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #114: 1025it [00:04, 245.88it/s, env_step=116736, len=22, n/ep=3, n/st=64, player_1/loss=1373.255, player_2/loss=449.408, rew=291.33]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #114: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #115: 1025it [00:04, 251.67it/s, env_step=117760, len=20, n/ep=4, n/st=64, player_1/loss=1253.415, player_2/loss=374.482, rew=287.25]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #115: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #116: 1025it [00:04, 231.37it/s, env_step=118784, len=29, n/ep=2, n/st=64, player_1/loss=1196.078, player_2/loss=373.220, rew=434.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #116: test_reward: 1102.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #117: 1025it [00:04, 225.35it/s, env_step=119808, len=36, n/ep=2, n/st=64, player_1/loss=923.663, player_2/loss=339.799, rew=798.50]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #117: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #118: 1025it [00:04, 237.19it/s, env_step=120832, len=28, n/ep=2, n/st=64, player_1/loss=1045.101, player_2/loss=355.212, rew=455.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #118: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #119: 1025it [00:04, 244.66it/s, env_step=121856, len=29, n/ep=1, n/st=64, player_1/loss=1530.201, player_2/loss=537.608, rew=434.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #119: test_reward: 779.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #120: 1025it [00:04, 248.44it/s, env_step=122880, len=38, n/ep=2, n/st=64, player_1/loss=1276.834, player_2/loss=449.189, rew=848.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #120: test_reward: 324.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #121: 1025it [00:04, 241.04it/s, env_step=123904, len=17, n/ep=3, n/st=64, player_1/loss=1217.719, rew=159.00]                                                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #121: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #122: 1025it [00:03, 268.08it/s, env_step=124928, len=30, n/ep=2, n/st=64, player_1/loss=862.074, player_2/loss=166.489, rew=500.50]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #122: test_reward: 350.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #123: 1025it [00:03, 329.93it/s, env_step=125952, len=29, n/ep=3, n/st=64, player_1/loss=1330.667, player_2/loss=281.066, rew=454.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #123: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #124: 1025it [00:03, 334.49it/s, env_step=126976, len=38, n/ep=2, n/st=64, player_1/loss=1737.476, player_2/loss=501.030, rew=759.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #124: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #125: 1025it [00:03, 338.56it/s, env_step=128000, len=20, n/ep=4, n/st=64, player_1/loss=2274.708, player_2/loss=515.247, rew=235.75]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #125: test_reward: 779.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #126: 1025it [00:03, 335.61it/s, env_step=129024, len=42, n/ep=1, n/st=64, player_1/loss=1207.149, player_2/loss=274.137, rew=1102.00]                                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #126: test_reward: 629.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #127: 1025it [00:03, 298.11it/s, env_step=130048, len=12, n/ep=5, n/st=64, player_1/loss=504.887, player_2/loss=547.718, rew=86.60]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #127: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #128: 1025it [00:03, 274.87it/s, env_step=131072, len=9, n/ep=7, n/st=64, player_1/loss=890.808, player_2/loss=622.274, rew=47.14]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #128: test_reward: 35.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #129: 1025it [00:03, 299.73it/s, env_step=132096, len=10, n/ep=6, n/st=64, player_1/loss=1245.455, player_2/loss=317.419, rew=67.83]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #129: test_reward: 35.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #130: 1025it [00:03, 267.38it/s, env_step=133120, len=12, n/ep=5, n/st=64, player_1/loss=1565.108, player_2/loss=178.702, rew=86.40]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #130: test_reward: 27.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #131: 1025it [00:04, 219.11it/s, env_step=134144, len=9, n/ep=6, n/st=64, player_1/loss=1032.062, player_2/loss=225.795, rew=51.83]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #131: test_reward: 27.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #132: 1025it [00:04, 243.15it/s, env_step=135168, len=9, n/ep=7, n/st=64, player_1/loss=742.586, player_2/loss=381.400, rew=47.14]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #132: test_reward: 35.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #133: 1025it [00:04, 233.61it/s, env_step=136192, len=14, n/ep=5, n/st=64, player_1/loss=841.433, player_2/loss=364.088, rew=111.60]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #133: test_reward: 54.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #134: 1025it [00:04, 255.37it/s, env_step=137216, len=13, n/ep=4, n/st=64, player_1/loss=772.985, player_2/loss=335.581, rew=102.25]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #134: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #135: 1025it [00:04, 244.75it/s, env_step=138240, len=11, n/ep=5, n/st=64, player_1/loss=644.991, player_2/loss=343.506, rew=80.00]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #135: test_reward: 90.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #136: 1025it [00:04, 233.61it/s, env_step=139264, len=9, n/ep=7, n/st=64, player_1/loss=573.882, player_2/loss=311.140, rew=58.14]                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #136: test_reward: 44.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #137: 1025it [00:04, 243.78it/s, env_step=140288, len=11, n/ep=3, n/st=64, player_1/loss=1375.608, player_2/loss=249.365, rew=69.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #137: test_reward: 54.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #138: 1025it [00:03, 261.97it/s, env_step=141312, len=8, n/ep=7, n/st=64, player_1/loss=1436.024, player_2/loss=259.395, rew=43.29]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #138: test_reward: 54.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #139: 1025it [00:04, 240.65it/s, env_step=142336, len=10, n/ep=6, n/st=64, player_1/loss=1622.670, player_2/loss=56.353, rew=57.50]                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #139: test_reward: 35.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #140: 1025it [00:04, 241.40it/s, env_step=143360, len=11, n/ep=6, n/st=64, player_1/loss=1913.261, player_2/loss=147.512, rew=76.33]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #140: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #141: 1025it [00:04, 229.29it/s, env_step=144384, len=13, n/ep=5, n/st=64, player_1/loss=1661.770, player_2/loss=196.169, rew=103.40]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #141: test_reward: 44.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #142: 1025it [00:04, 207.01it/s, env_step=145408, len=19, n/ep=4, n/st=64, player_1/loss=1254.148, player_2/loss=275.213, rew=198.75]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #142: test_reward: 230.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #143: 1025it [00:05, 198.16it/s, env_step=146432, len=12, n/ep=5, n/st=64, player_1/loss=1153.905, player_2/loss=341.201, rew=87.60]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #143: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #144: 1025it [00:04, 206.13it/s, env_step=147456, len=19, n/ep=3, n/st=64, player_1/loss=892.876, player_2/loss=336.543, rew=243.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #144: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #145: 1025it [00:03, 273.71it/s, env_step=148480, len=41, n/ep=1, n/st=64, player_1/loss=889.640, player_2/loss=286.799, rew=860.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #145: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #146: 1025it [00:03, 273.91it/s, env_step=149504, len=14, n/ep=5, n/st=64, player_1/loss=1141.410, player_2/loss=376.847, rew=112.60]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #146: test_reward: 44.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #147: 1025it [00:04, 254.37it/s, env_step=150528, len=15, n/ep=4, n/st=64, player_1/loss=1894.245, player_2/loss=373.557, rew=152.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #147: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #148: 1025it [00:04, 247.15it/s, env_step=151552, len=24, n/ep=2, n/st=64, player_1/loss=1586.859, rew=349.00]                                                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #148: test_reward: 779.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #149: 1025it [00:04, 255.17it/s, env_step=152576, len=24, n/ep=3, n/st=64, player_1/loss=1669.327, player_2/loss=352.698, rew=364.33]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #149: test_reward: 350.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #150: 1025it [00:04, 236.04it/s, env_step=153600, len=13, n/ep=5, n/st=64, player_1/loss=1659.270, player_2/loss=372.944, rew=93.60]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #150: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #151: 1025it [00:03, 265.07it/s, env_step=154624, len=12, n/ep=4, n/st=64, player_1/loss=1389.258, player_2/loss=270.128, rew=77.75]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #151: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #152: 1025it [00:04, 239.31it/s, env_step=155648, len=12, n/ep=5, n/st=64, player_1/loss=1012.902, player_2/loss=324.711, rew=87.40]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #152: test_reward: 560.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #153: 1025it [00:04, 227.48it/s, env_step=156672, len=27, n/ep=2, n/st=64, player_1/loss=1150.760, player_2/loss=393.041, rew=406.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #153: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #154: 1025it [00:04, 207.22it/s, env_step=157696, len=26, n/ep=3, n/st=64, player_1/loss=1307.490, player_2/loss=437.685, rew=371.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #154: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #155: 1025it [00:03, 282.21it/s, env_step=158720, len=16, n/ep=4, n/st=64, player_1/loss=1578.804, player_2/loss=339.928, rew=143.75]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #155: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #156: 1025it [00:04, 254.31it/s, env_step=159744, len=30, n/ep=2, n/st=64, player_1/loss=2163.943, player_2/loss=473.105, rew=464.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #156: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #157: 1025it [00:03, 265.04it/s, env_step=160768, len=31, n/ep=2, n/st=64, player_1/loss=2037.680, player_2/loss=635.237, rew=514.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #157: test_reward: 560.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #158: 1025it [00:04, 240.95it/s, env_step=161792, len=28, n/ep=3, n/st=64, player_1/loss=851.237, player_2/loss=624.751, rew=531.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #158: test_reward: 740.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #159: 1025it [00:04, 252.27it/s, env_step=162816, len=33, n/ep=2, n/st=64, player_1/loss=1091.615, player_2/loss=579.423, rew=578.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #159: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #160: 1025it [00:03, 265.69it/s, env_step=163840, len=37, n/ep=2, n/st=64, player_1/loss=732.334, player_2/loss=335.765, rew=721.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #160: test_reward: 119.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #161: 1025it [00:03, 292.40it/s, env_step=164864, len=39, n/ep=2, n/st=64, player_1/loss=633.849, player_2/loss=533.906, rew=902.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #161: test_reward: 819.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #162: 1025it [00:06, 166.98it/s, env_step=165888, len=23, n/ep=3, n/st=64, player_1/loss=481.144, player_2/loss=418.464, rew=305.33]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #162: test_reward: 740.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #163: 1025it [00:07, 143.50it/s, env_step=166912, len=25, n/ep=2, n/st=64, player_1/loss=783.106, player_2/loss=359.941, rew=428.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #163: test_reward: 77.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #164: 1025it [00:11, 88.90it/s, env_step=167936, len=19, n/ep=3, n/st=64, player_1/loss=1281.211, player_2/loss=533.494, rew=242.33]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #164: test_reward: 77.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #165: 1025it [00:05, 183.89it/s, env_step=168960, len=37, n/ep=2, n/st=64, player_1/loss=1605.236, player_2/loss=693.789, rew=721.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #165: test_reward: 740.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #166: 1025it [00:04, 223.34it/s, env_step=169984, len=28, n/ep=2, n/st=64, player_1/loss=914.445, player_2/loss=331.758, rew=455.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #166: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #167: 1025it [00:04, 222.91it/s, env_step=171008, len=27, n/ep=1, n/st=64, player_1/loss=1023.986, player_2/loss=292.404, rew=377.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #167: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #168: 1025it [00:03, 281.87it/s, env_step=172032, len=11, n/ep=3, n/st=64, player_1/loss=1901.038, player_2/loss=437.477, rew=69.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #168: test_reward: 65.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #169: 1025it [00:03, 267.94it/s, env_step=173056, len=42, n/ep=1, n/st=64, player_1/loss=2221.516, player_2/loss=479.161, rew=1102.00]                                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #169: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #170: 1025it [00:04, 226.12it/s, env_step=174080, len=37, n/ep=2, n/st=64, player_1/loss=1602.885, player_2/loss=621.637, rew=814.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #170: test_reward: 779.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #171: 1025it [00:04, 237.02it/s, env_step=175104, len=38, n/ep=2, n/st=64, player_1/loss=1306.275, player_2/loss=624.600, rew=740.50]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #171: test_reward: 434.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #172: 1025it [00:04, 248.00it/s, env_step=176128, len=29, n/ep=2, n/st=64, player_1/loss=1239.849, player_2/loss=383.682, rew=494.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #172: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #173: 1025it [00:04, 236.62it/s, env_step=177152, len=39, n/ep=1, n/st=64, player_1/loss=791.188, player_2/loss=439.584, rew=779.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #173: test_reward: 495.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #174: 1025it [00:04, 242.17it/s, env_step=178176, len=24, n/ep=3, n/st=64, player_1/loss=601.850, player_2/loss=571.447, rew=364.33]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #174: test_reward: 779.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #175: 1025it [00:04, 234.44it/s, env_step=179200, len=28, n/ep=3, n/st=64, player_1/loss=748.383, player_2/loss=583.941, rew=463.33]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #175: test_reward: 594.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #176: 1025it [00:04, 236.18it/s, env_step=180224, len=31, n/ep=2, n/st=64, player_1/loss=959.727, player_2/loss=498.302, rew=513.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #176: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #177: 1025it [00:04, 237.94it/s, env_step=181248, len=33, n/ep=2, n/st=64, player_1/loss=857.326, player_2/loss=311.372, rew=562.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #177: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #178: 1025it [00:04, 236.50it/s, env_step=182272, len=27, n/ep=2, n/st=64, player_1/loss=792.691, player_2/loss=298.177, rew=475.00]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #178: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #179: 1025it [00:04, 237.79it/s, env_step=183296, len=22, n/ep=3, n/st=64, player_1/loss=865.562, player_2/loss=513.059, rew=314.67]                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #179: test_reward: 1102.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #180: 1025it [00:04, 234.34it/s, env_step=184320, len=36, n/ep=2, n/st=64, player_1/loss=1753.469, player_2/loss=529.170, rew=665.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #180: test_reward: 702.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #181: 1025it [00:04, 234.66it/s, env_step=185344, len=34, n/ep=2, n/st=64, player_1/loss=1542.484, player_2/loss=468.448, rew=594.00]                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #181: test_reward: 594.000000 ± 0.000000, best_reward: 1102.000000 ± 0.000000 in #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #182:  12%|##########2                                                                       | 128/1024 [00:00<00:03, 252.65it/s, env_step=185472, len=36, n/ep=2, n/st=64, player_1/loss=1652.503, player_2/loss=460.163, rew=665.00]"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: TRAINING AGENTS\n",
    "####################################################\n",
    "\n",
    "# Configs for the agents\n",
    "freeze_agent1 = True\n",
    "agent1_starting_params = \"./saved_variables/paper_notebooks/6/dqn_vs_dqn_cnn_based/best_policy_agent1.pth\"\n",
    "\n",
    "freeze_agent2 = False\n",
    "agent2_starting_params = \"./saved_variables/paper_notebooks/6/dqn_vs_dqn_cnn_based/best_policy_agent2.pth\"\n",
    "\n",
    "single_agent_score_as_reward = False # To use combined reward or non frozen agent reward as scoring\n",
    "\n",
    "# Get the environment settings\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# Configure agent 1\n",
    "agent1 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                           action_shape= action_shape,\n",
    "                           gamma= 0.9,\n",
    "                           frozen= freeze_agent1,\n",
    "                           learning_rate = 0.0001,\n",
    "                           n_step= 4)\n",
    "\n",
    "if agent1_starting_params:\n",
    "    agent1.load_state_dict(torch.load(agent1_starting_params))\n",
    "\n",
    "# Configure agent 2\n",
    "agent2 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                           action_shape= action_shape,\n",
    "                           gamma= 0.9,\n",
    "                           frozen= freeze_agent2,\n",
    "                           learning_rate = 0.0001,\n",
    "                           n_step= 4)\n",
    "\n",
    "if agent2_starting_params:\n",
    "    agent2.load_state_dict(torch.load(agent2_starting_params))\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "off_policy_traininer_results, final_agent_player1, final_agent_player2 = train_agent(epochs= 1000,\n",
    "                                                                                     agent_player1= agent1,\n",
    "                                                                                     agent_player2= agent2,\n",
    "                                                                                     filename=\"1-cnn_dqn_frozen_agent1\",\n",
    "                                                                                     single_agent_score_as_reward = single_agent_score_as_reward,\n",
    "                                                                                     training_eps= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: VIEWING THE BEST LEARNED POLICY\n",
    "####################################################\n",
    "\n",
    "# Get the environment settings\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# Configure the best agent\n",
    "best_agent1 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                                action_shape= action_shape)\n",
    "best_agent1.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/7/1-cnn_dqn_frozen_agent1/best_policy_agent1.pth\"))\n",
    "best_agent1.set_eps(0)\n",
    "\n",
    "\n",
    "best_agent2 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                                action_shape= action_shape)\n",
    "best_agent2.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/7/1-cnn_dqn_frozen_agent1/best_policy_agent2.pth\"))\n",
    "best_agent2.set_eps(0)\n",
    "\n",
    "# Watch the best agent at work\n",
    "watch(numer_of_games= 3,\n",
    "      render_speed= 0.3,\n",
    "      agent_player1= best_agent1,\n",
    "      agent_player2= best_agent2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: VIEWING THE LAST LEARNED POLICY\n",
    "####################################################\n",
    "\n",
    "# Configure the final agent\n",
    "final_agent_player1 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                                        action_shape= action_shape)\n",
    "final_agent_player1.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/7/1-cnn_dqn_frozen_agent1/final_policy_agent1.pth\"))\n",
    "best_agent1.set_eps(0)\n",
    "\n",
    "final_agent_player2 = cf_cnn_dqn_policy(state_shape= state_shape,\n",
    "                                        action_shape= action_shape)\n",
    "final_agent_player2.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/7/1-cnn_dqn_frozen_agent1/final_policy_agent2.pth\"))\n",
    "best_agent2.set_eps(0)\n",
    "\n",
    "# Watch the best agent at work\n",
    "watch(numer_of_games= 3,\n",
    "      render_speed= 0.3,\n",
    "      agent_player1= final_agent_player1,\n",
    "      agent_player2= final_agent_player2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84478519",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The performance of this model based on a CNN is similar to the previous model used.\n",
    "We will address other difficult points in the next notebooks to build an appropriate bot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9110be",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN VARIABLES\n",
    "####################################################\n",
    "\n",
    "del action_shape\n",
    "del agent1\n",
    "del agent2\n",
    "del best_agent1\n",
    "del best_agent2\n",
    "del env\n",
    "del final_agent_player1\n",
    "del final_agent_player2\n",
    "del observation_space\n",
    "del off_policy_traininer_results\n",
    "del state_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d3da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
