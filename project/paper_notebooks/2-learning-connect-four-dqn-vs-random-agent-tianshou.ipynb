{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Connect four with Deep Q-Network vs Random agent from Tianshou\n",
    "\n",
    "In this notebook, the created custom gym environment of connect four is played using a Deep Q-Network (DQN) agent which plays against a random agent.\n",
    "We use the powerful [Tianshou library](https://github.com/thu-ml/tianshou) for this.\n",
    "Using Tianshou in a multi-agent environment like ours meant we had to create yet another version of the Gym environment now based on an `AECEnv` environment.\n",
    "\n",
    "The methodology used in this notebook is largely equal to those of the random agents in the `1-learning-connect-four-random-agents-tianshou.ipynb` notebook and the tutorial on [multi-agent tic-tac-toe learning from Tianshou](https://tianshou.readthedocs.io/en/master/tutorials/tictactoe.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training DQN vs Random agent on connect four Gym\n",
    "  - Create the Gym environment\n",
    "  - Building the environment\n",
    "  - Implementing the DQN policy\n",
    "  - Building agents\n",
    "  - Function for letting agents learn\n",
    "  - Function for watching learned agent\n",
    "  - Doing the experiment\n",
    "- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.21.0 recommended): 0.21.0\n",
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.12.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argparser\n",
    "import argparse\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.12.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# PPrint is a pretty print for variables\n",
    "from pprint import pprint\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(cfgym)\n",
    "\n",
    "# Time for allowing \"freezes\" in execution\n",
    "import time;\n",
    "\n",
    "# Allow for copying objects in a non reference manner\n",
    "import copy\n",
    "\n",
    "# Used for updating notebook display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training DQN vs Random agent on connect four Gym\n",
    "\n",
    "Our connect four gym setup requires two agents, one for each player.\n",
    "To reduce complexity, agents will always play as the same player, e.g. always as player 1.\n",
    "It is important to note that connect four is a *solved game*.\n",
    "According to [The Washington Post](https://www.washingtonpost.com/news/wonk/wp/2015/05/08/how-to-win-any-popular-game-according-to-data-scientists/):\n",
    "\n",
    "> Connect Four is what mathematicians call a \"solved game,\" meaning you can play it perfectly every time, no matter what your opponent does. You will need to get the first move, but as long as you do so, you can always win within 41 moves.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Create the Gym environment\n",
    "\n",
    "Whilst our first connect four implementation (V1) was playable in a multi-agent manner through a manual game loop, as is done for random agents in the experimental notebook `2-testing-custom-gym-environment.ipynb`, it is hard to use most of the popular RL libraries for this environment.\n",
    "That is because the Gym environment was originally made with single agent games in mind and there is no real standard on how to write multi-agent environments.\n",
    "Thus, Tianshou, which offers some multi-agent support, didn't work well with this version of the Gym environment.\n",
    "To tackle this, we created a V2, which is a rework of V1 to follow the standards of a *Petting Zoo* environment.\n",
    "[Petting zoo](https://www.pettingzoo.ml/) is a library that offers many Gym environments extended to be multi-agent which uses wrapper classes and base classes so that each multi-agent environment follows the same guidelines.\n",
    "\n",
    "These guidelines are exactly what V2 of our connect four implementation is adopted towards.\n",
    "The environment is now a subclass of `AECEnv` rather than a `gym.Env`, which follows a similar approach but requires far more attributes and more complex observation and action spaces so that each agent has its own, even if they are all equal.\n",
    "\n",
    "We note that we only found out about this library after running into trouble making our V1 work with libraries that support Gym training.\n",
    "We could have searched for online implementations of e.g. Deep Q-Network (DQN) that allows training using the gym gaming loop used in the experimental notebook `2-testing-custom-gym-environment.ipynb`. \n",
    "Nonetheless, this would likely require a lot of manual work which is not ideal for the goal of this research, which focuses on reducing work for game developers.\n",
    "Thus, the decision was made to create this V2 of the environment based on Petting Zoo environments.\n",
    "\n",
    "As a final note, we noticed there was a Connect Four implementation of Petting Zoo.\n",
    "This implementation differs from ours in several ways.\n",
    "Their observation space consists of multiple variants of the board, namely one with only the opponent's coin and the agent's coins and it uses action masks to not allow placing coins in full columns.\n",
    "We use a singular observation space of the complete board, which is the location of both agent's coins and the free spaces, corresponding to what a human would see.\n",
    "Adding to this, we also don't programmatically disallow placing a coin in a full column but rather punish the user for trying that action and leave the board unchanged and let the agent play again.\n",
    "There are more differences then the ones listed here and the visual game is also completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a65ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict(action_mask:Box([0 0 0 0 0 0 0], [1 1 1 1 1 1 1], (7,), int8), observation:Box([[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]], [[2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]], (6, 7), int8))\n",
      "\n",
      "Action space: Discrete(7)\n",
      "\n",
      " Initial player id:\n",
      "player_1\n",
      "\n",
      " Initial observation:\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " Initial mask:\n",
      "[True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# SETTING UP THE GYM ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "# Create an instance of the environment to be used\n",
    "# V2 is used as this contains edits for Tianshou\n",
    "# We use the Tianshou PettingZooEnv wrapper for multiagent support\n",
    "env = ts.env.PettingZooEnv(cfgym.env())\n",
    "\n",
    "# Get information about the environment\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"\\n Initial player id:\")\n",
    "print(observation[\"agent_id\"])\n",
    "\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation[\"obs\"])\n",
    "\n",
    "print(\"\\n Initial mask:\")\n",
    "print(observation[\"mask\"])\n",
    "\n",
    "# Clean unused variables\n",
    "del observation\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a7d4f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Building the environment\n",
    "\n",
    "We build a simple function to get the environment the agent will play in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CONNECT FOUR V2 ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "def get_env():\n",
    "    \"\"\"\n",
    "    Returns the connect four gym environment V2 altered for Tianshou and Petting Zoo compatibility.\n",
    "    Already wrapped with a ts.env.PettingZooEnv wrapper.\n",
    "    \"\"\"\n",
    "    return ts.env.PettingZooEnv(cfgym.env())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7e7da",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Implementing the DQN policy\n",
    "\n",
    "The DQN policy for the agent is configured and set up below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539b56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY\n",
    "####################################################\n",
    "\n",
    "def cf_dqn_policy(state_shape: tuple,\n",
    "                  action_shape: tuple,\n",
    "                  optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                  learning_rate: float =  0.0001,\n",
    "                  gamma: float = 0.7, # Smaller gamma favours \"faster\" win\n",
    "                  n_step: int = 3, # Number of steps to look ahead\n",
    "                  target_update_freq: int = 320,\n",
    "                  hidden_sizes: list = [128, 128, 128, 128]):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    ## Set to use the CUDA device if possible\n",
    "    net = ts.utils.net.common.Net(state_shape= state_shape,\n",
    "                                  action_shape= action_shape,\n",
    "                                  hidden_sizes= hidden_sizes,\n",
    "                                  device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd411318",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Building agents\n",
    "\n",
    "We can now adopt the `get_agents` function from Tianshou to build our DQN agents for the game.\n",
    "\n",
    "NOTE: agent 2 is random for now whilst agent 1 will use DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ae27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT CREATION\n",
    "####################################################\n",
    "\n",
    "def get_agents(agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "               agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "               optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "               resume_path_player_1: str = '', # Path to file to resume agent training from\n",
    "               ) -> typing.Tuple[ts.policy.BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \"\"\"\n",
    "    Gets a multi agent policy manager, optimizer and player ids for the connect four V2 gym environment.\n",
    "    Per default this returns \n",
    "        - Multi agent manager for agent 1 using DQN and agent 2 using random policy\n",
    "        - Adam optimizer\n",
    "        - ['player_1', 'player_2'] from the connect four environment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the environment to play in (Connect four gym V2)\n",
    "    env = get_env()\n",
    "    \n",
    "    # Get the observation space from the environment, depending on typo of space (ternary operator)\n",
    "    observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "    \n",
    "    # Set the arguments\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    \n",
    "    # Configure agent player 1 to be a DQN if no policy is passed.\n",
    "    if agent_player1 is None:\n",
    "        # Our agent1 uses a DQN policy\n",
    "        agent_player1 = cf_dqn_policy(state_shape= state_shape,\n",
    "                                      action_shape= action_shape,\n",
    "                                      optim= optim)\n",
    "        \n",
    "        # If we resume our agent we need to load the previous config\n",
    "        if resume_path_player_1:\n",
    "            agent_player1.load_state_dict(torch.load(resume_path_player_1))\n",
    "\n",
    "    # Configure agent player 2 to be a DQN if no policy is passed\n",
    "    #   or a copy of player 1 with the specified path.\n",
    "    if agent_player2 is None:\n",
    "        # Our agent2 uses a random policy per default for now\n",
    "        agent_player2 = ts.policy.RandomPolicy()\n",
    "\n",
    "    # Player 1 agent is DQN, other is random\n",
    "    agents = [agent_player1, agent_player2]\n",
    "        \n",
    "    # Our policy depends on the order of the agents\n",
    "    policy = ts.policy.MultiAgentPolicyManager(agents, env)\n",
    "    \n",
    "    # Return our policy, optimizer and the available agents in the environment\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for agent 1 using DQN and agent 2 using random policy\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    \n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8f3a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for letting agents learn\n",
    "\n",
    "We can now adopt the `train_agent` function from Tianshou to allow our DQN and random agent for the game to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a32c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT TRAINING\n",
    "####################################################\n",
    "\n",
    "def train_agent(filename: str = \"dqn_vs_random\",\n",
    "                agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                training_env_num: int = 10,\n",
    "                testing_env_num: int = 10,\n",
    "                buffer_size: int = 2^14, # 16 384\n",
    "                batch_size: int = 64,\n",
    "                epochs: int = 50,\n",
    "                step_per_epoch: int = 1000,\n",
    "                step_per_collect: int = 10,\n",
    "                update_per_step: float = 0.1,\n",
    "                testing_eps: float = 0.05,\n",
    "                training_eps: float = 0.1,\n",
    "                ) -> typing.Tuple[dict, ts.policy.BasePolicy]:\n",
    "    \"\"\"\n",
    "    Trains two agents in the connect four V2 environment and saves their best model and logs.\n",
    "    Returns:\n",
    "        - result from offpolicy_trainer\n",
    "        - final version of agent 1\n",
    "        - final version of agent 2\n",
    "    \"\"\"\n",
    "\n",
    "    # ======== environment setup =========\n",
    "    train_envs = ts.env.DummyVectorEnv([get_env for _ in range(training_env_num)])\n",
    "    test_envs = ts.env.DummyVectorEnv([get_env for _ in range(testing_env_num)])\n",
    "    \n",
    "    # set the seed for reproducibility\n",
    "    np.random.seed(1998)\n",
    "    torch.manual_seed(1998)\n",
    "    train_envs.seed(1998)\n",
    "    test_envs.seed(1998)\n",
    "\n",
    "    # ======== agent setup =========\n",
    "    # Gets our agents from the previously made function\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for agent 1 using DQN and agent 2 using random policy\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    policy, optim, agents = get_agents(agent_player1=agent_player1,\n",
    "                                       agent_player2=agent_player2,\n",
    "                                       optim=optim)\n",
    "\n",
    "    # ======== collector setup =========\n",
    "    # Make a collector for the training environments\n",
    "    train_collector = ts.data.Collector(policy= policy,\n",
    "                                        env= train_envs,\n",
    "                                        buffer= ts.data.VectorReplayBuffer(buffer_size, len(train_envs)),\n",
    "                                        exploration_noise= True)\n",
    "    \n",
    "    # Make a collector for the testing environments\n",
    "    test_collector = ts.data.Collector(policy= policy,\n",
    "                                       env= test_envs,\n",
    "                                       exploration_noise= True)\n",
    "    \n",
    "    # Uncomment below if you want to set epsilon in epsilon policy\n",
    "    # policy.set_eps(1)\n",
    "    \n",
    "    # Collect data fot the training evnironments\n",
    "    train_collector.collect(n_step= batch_size * training_env_num)\n",
    "    \n",
    "    # ======== ensure folders exist =========\n",
    "    if not os.path.exists(os.path.join('./logs', 'paper_notebooks', '2', filename)):\n",
    "        os.makedirs(os.path.join('./logs', 'paper_notebooks', '2', filename))\n",
    "    if not os.path.exists(os.path.join('./saved_variables', 'paper_notebooks', '2', filename)):\n",
    "        os.makedirs(os.path.join('./saved_variables', 'paper_notebooks', '2', filename))\n",
    "\n",
    "    # ======== tensorboard logging setup =========\n",
    "    # Allows to save the training progress to tensorboard compatable logs\n",
    "    log_path = os.path.join('./logs', 'paper_notebooks', '2', filename)\n",
    "    writer = torch.utils.tensorboard.SummaryWriter(log_path)\n",
    "    logger = ts.utils.TensorboardLogger(writer)\n",
    "\n",
    "    # ======== callback functions used during training =========\n",
    "    # We want to save our best policy\n",
    "    def save_best_fn(policy):\n",
    "        \"\"\"\n",
    "        Callback to save the best model\n",
    "        \"\"\"\n",
    "        # Save agent 1\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', '2', filename, 'best_policy_agent1.pth')\n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save agent 2\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', '2', filename, 'best_policy_agent2.pth')\n",
    "        torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save agent2\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        \"\"\"\n",
    "        Callback to stop training when we've reached the win rate\n",
    "        \"\"\"\n",
    "        return mean_rewards >= 7 # (win = 10, 70% win without invalid moves = mean of 7)\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback for training\n",
    "        \"\"\"        \n",
    "        # Agent 1\n",
    "        policy.policies[agents[0]].set_eps(training_eps)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback for testing\n",
    "        \"\"\"        \n",
    "        # Agent 1\n",
    "        policy.policies[agents[0]].set_eps(testing_eps)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        \"\"\"\n",
    "        Callback for reward collection\n",
    "        \"\"\"\n",
    "        # We are only interested in optimizing agent 1 and thus only agent 1 score\n",
    "        return rews[:, 0]\n",
    "\n",
    "    # trainer\n",
    "    result = ts.trainer.offpolicy_trainer(policy= policy,\n",
    "                                          train_collector= train_collector,\n",
    "                                          test_collector= test_collector,\n",
    "                                          max_epoch= epochs,\n",
    "                                          step_per_epoch= step_per_epoch,\n",
    "                                          step_per_collect= step_per_collect,\n",
    "                                          episode_per_test= testing_env_num,\n",
    "                                          batch_size= batch_size,\n",
    "                                          train_fn= train_fn,\n",
    "                                          test_fn= test_fn,\n",
    "                                          # Stop function to stop before specified amount of epochs\n",
    "                                          #stop_fn= stop_fn\n",
    "                                          save_best_fn= save_best_fn,\n",
    "                                          update_per_step= update_per_step,\n",
    "                                          logger= logger,\n",
    "                                          test_in_train= False,\n",
    "                                          reward_metric= reward_metric)\n",
    "\n",
    "    return result, policy.policies[agents[0]], policy.policies[agents[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161cc0f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for watching learned agent\n",
    "\n",
    "When an agent has learned, we can watch the learned policy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b526e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# WATCHING THE LEARNED POLICY IN ACTION\n",
    "####################################################\n",
    "\n",
    "def watch(numer_of_games:int = 3,\n",
    "          agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          test_epsilon: float = 0.05, # For the watching we act completely greedy but low random for not getting stuck on invalid move\n",
    "          render_speed: float = 0.15, # Amount of seconds to update frame/ do a step\n",
    "          ) -> None:\n",
    "    \n",
    "    # Get the connect four V2 environment (must be a list)\n",
    "    env= ts.env.DummyVectorEnv([get_env])\n",
    "    \n",
    "    # Get the agents from the trained agents\n",
    "    policy, optim, agents = get_agents(agent_player1= agent_player1,\n",
    "                                       agent_player2= agent_player2)\n",
    "    \n",
    "    # Evaluate the policy\n",
    "    policy.eval()\n",
    "    \n",
    "    # Set the testing policy epsilon for our first agent\n",
    "    policy.policies[agents[0]].set_eps(test_epsilon)\n",
    "    \n",
    "    # Collect the test data\n",
    "    collector = ts.data.Collector(policy= policy,\n",
    "                                  env= env,\n",
    "                                  exploration_noise= True)\n",
    "    \n",
    "    # Render games in human mode to see how it plays\n",
    "    result = collector.collect(n_episode= numer_of_games, render= render_speed)\n",
    "    \n",
    "    # Close the environment aftering collecting the results\n",
    "    # This closes the pygame window after completion\n",
    "    env.close()\n",
    "    \n",
    "    # Get the rewards and length from the test trials\n",
    "    rewards, length = result[\"rews\"], result[\"lens\"]\n",
    "    \n",
    "    # Print the final reward for the first agent\n",
    "    print(f\"Average steps of game:  {length.mean()}\")\n",
    "    print(f\"Final mean reward agent 1: {rewards[:, 0].mean()}, std: {rewards[:, 0].std()}\")\n",
    "    print(f\"Final mean reward agent 2: {rewards[:, 1].mean()}, std: {rewards[:, 1].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31784e9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Doing the experiment\n",
    "\n",
    "We now do the experiment with using our previously created functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93fe8961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 464.52it/s, env_step=1000, len=12, n/ep=0, n/st=10, player_1/loss=1191.006, rew=-10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -927.400000 ± 1791.873891, best_reward: -927.400000 ± 1791.873891 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:01, 813.42it/s, env_step=2000, len=9, n/ep=0, n/st=10, player_1/loss=70.863, rew=10.00]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -326.000000 ± 672.814387, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [00:01, 821.12it/s, env_step=3000, len=10, n/ep=0, n/st=10, player_1/loss=47.418, rew=10.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -3966.800000 ± 11930.400000, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:01, 819.74it/s, env_step=4000, len=7, n/ep=0, n/st=10, player_1/loss=49.881, rew=10.00]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -4253.700000 ± 12080.687373, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:01, 825.42it/s, env_step=5000, len=11, n/ep=1, n/st=10, player_1/loss=42.903, rew=10.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -3177.800000 ± 9563.400000, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1001it [00:01, 832.21it/s, env_step=6000, len=7, n/ep=1, n/st=10, player_1/loss=165.781, rew=10.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -7758.000000 ± 21111.768168, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 1001it [00:01, 838.35it/s, env_step=7000, len=7, n/ep=3, n/st=10, player_1/loss=260.359, rew=10.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -6835.100000 ± 12582.272962, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 1001it [00:01, 838.95it/s, env_step=8000, len=7, n/ep=0, n/st=10, player_1/loss=287.218, rew=10.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -3238.700000 ± 8137.768626, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1001it [00:01, 846.84it/s, env_step=9000, len=63, n/ep=1, n/st=10, player_1/loss=431.147, rew=-1373.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -4443.500000 ± 7329.832321, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1001it [00:01, 846.08it/s, env_step=10000, len=36, n/ep=0, n/st=10, player_1/loss=631.407, rew=-308.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -657.400000 ± 1988.882309, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 1001it [00:01, 848.27it/s, env_step=11000, len=130, n/ep=0, n/st=10, player_1/loss=641.186, rew=-6318.00]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -9930.200000 ± 22682.936251, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 1001it [00:01, 839.62it/s, env_step=12000, len=10, n/ep=2, n/st=10, player_1/loss=465.093, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: -14592.900000 ± 20469.948532, best_reward: -326.000000 ± 672.814387 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 1001it [00:01, 837.25it/s, env_step=13000, len=9, n/ep=0, n/st=10, player_1/loss=416.699, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -17.800000 ± 76.964667, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 1001it [00:01, 850.02it/s, env_step=14000, len=9, n/ep=0, n/st=10, player_1/loss=277.915, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -10777.000000 ± 26710.130520, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 1001it [00:01, 835.28it/s, env_step=15000, len=9, n/ep=0, n/st=10, player_1/loss=429.394, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: -13995.900000 ± 21188.798968, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 1001it [00:01, 835.54it/s, env_step=16000, len=7, n/ep=0, n/st=10, player_1/loss=315.852, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: -22279.100000 ± 32188.836793, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 1001it [00:01, 850.84it/s, env_step=17000, len=20, n/ep=0, n/st=10, player_1/loss=286.055, rew=-47.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -11477.100000 ± 27605.375080, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 1001it [00:01, 841.98it/s, env_step=18000, len=10, n/ep=1, n/st=10, player_1/loss=556.076, rew=-10.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: -907.500000 ± 2203.176219, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 1001it [00:01, 847.53it/s, env_step=19000, len=60, n/ep=1, n/st=10, player_1/loss=320.179, rew=-891.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -15361.500000 ± 38029.782298, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 1001it [00:01, 841.06it/s, env_step=20000, len=13, n/ep=1, n/st=10, player_1/loss=511.158, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: -31.500000 ± 124.500000, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 1001it [00:01, 848.08it/s, env_step=21000, len=12, n/ep=0, n/st=10, player_1/loss=744.236, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: -1640.500000 ± 4944.836929, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 1001it [00:01, 830.31it/s, env_step=22000, len=11, n/ep=0, n/st=10, player_1/loss=417.822, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: -2933.400000 ± 5841.504108, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 1001it [00:01, 837.87it/s, env_step=23000, len=68, n/ep=0, n/st=10, player_1/loss=304.897, rew=-1331.00]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: -900.900000 ± 2726.039855, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 1001it [00:01, 849.55it/s, env_step=24000, len=9, n/ep=0, n/st=10, player_1/loss=286.766, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: -15004.700000 ± 24844.540358, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 1001it [00:01, 834.43it/s, env_step=25000, len=9, n/ep=1, n/st=10, player_1/loss=485.533, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: -30465.500000 ± 62724.906397, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 1001it [00:01, 842.62it/s, env_step=26000, len=13, n/ep=0, n/st=10, player_1/loss=396.450, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: -369.500000 ± 1138.500000, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 1001it [00:01, 837.03it/s, env_step=27000, len=24, n/ep=1, n/st=10, player_1/loss=410.028, rew=-10.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: -4671.300000 ± 13224.626528, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 1001it [00:01, 834.48it/s, env_step=28000, len=20, n/ep=1, n/st=10, player_1/loss=483.736, rew=-10.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: -3957.600000 ± 8337.879481, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 1001it [00:01, 838.23it/s, env_step=29000, len=15, n/ep=1, n/st=10, player_1/loss=261.739, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: -880.700000 ± 2665.440003, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 1001it [00:01, 844.75it/s, env_step=30000, len=14, n/ep=0, n/st=10, player_1/loss=634.736, rew=-12.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: -270.900000 ± 842.700000, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 1001it [00:01, 836.59it/s, env_step=31000, len=9, n/ep=0, n/st=10, player_1/loss=531.759, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: -604.800000 ± 1573.099921, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 1001it [00:01, 814.03it/s, env_step=32000, len=14, n/ep=0, n/st=10, player_1/loss=350.409, rew=-10.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: -1128.000000 ± 3003.524896, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 1001it [00:01, 796.30it/s, env_step=33000, len=7, n/ep=0, n/st=10, player_1/loss=431.986, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: -205.600000 ± 549.324713, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 1001it [00:01, 807.35it/s, env_step=34000, len=8, n/ep=2, n/st=10, player_1/loss=409.936, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: -11487.400000 ± 27904.362351, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 1001it [00:01, 848.10it/s, env_step=35000, len=8, n/ep=0, n/st=10, player_1/loss=529.420, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: -2873.400000 ± 8594.945866, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 1001it [00:01, 852.65it/s, env_step=36000, len=7, n/ep=1, n/st=10, player_1/loss=1164.107, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: -4555.300000 ± 9600.979753, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 1001it [00:01, 850.94it/s, env_step=37000, len=7, n/ep=1, n/st=10, player_1/loss=578.868, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: -4854.100000 ± 14092.061158, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 1001it [00:01, 837.28it/s, env_step=38000, len=11, n/ep=3, n/st=10, player_1/loss=600.947, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: -1231.200000 ± 3716.938116, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 1001it [00:01, 850.36it/s, env_step=39000, len=10, n/ep=0, n/st=10, player_1/loss=466.308, rew=-10.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: -146.200000 ± 448.689157, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 1001it [00:01, 844.81it/s, env_step=40000, len=9, n/ep=0, n/st=10, player_1/loss=760.185, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: -2624.300000 ± 5391.149897, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 1001it [00:01, 840.00it/s, env_step=41000, len=7, n/ep=0, n/st=10, player_1/loss=415.416, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: -294.400000 ± 913.200000, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 1001it [00:01, 851.93it/s, env_step=42000, len=25, n/ep=0, n/st=10, player_1/loss=699.360, rew=-137.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: -898.400000 ± 2701.880649, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 1001it [00:01, 832.32it/s, env_step=43000, len=9, n/ep=0, n/st=10, player_1/loss=333.775, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: -10679.200000 ± 28522.650591, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 1001it [00:01, 850.36it/s, env_step=44000, len=7, n/ep=0, n/st=10, player_1/loss=997.477, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: -184.600000 ± 577.164136, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 1001it [00:01, 851.88it/s, env_step=45000, len=14, n/ep=0, n/st=10, player_1/loss=791.903, rew=-10.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: -1552.800000 ± 4675.073321, best_reward: -17.800000 ± 76.964667 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 1001it [00:01, 789.52it/s, env_step=46000, len=15, n/ep=0, n/st=10, player_1/loss=451.160, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: 8.000000 ± 6.000000, best_reward: 8.000000 ± 6.000000 in #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 1001it [00:01, 826.81it/s, env_step=47000, len=7, n/ep=0, n/st=10, player_1/loss=325.211, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: 6.000000 ± 8.000000, best_reward: 8.000000 ± 6.000000 in #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 1001it [00:01, 843.57it/s, env_step=48000, len=7, n/ep=0, n/st=10, player_1/loss=484.457, rew=10.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: -519.600000 ± 1284.527322, best_reward: 8.000000 ± 6.000000 in #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 1001it [00:01, 834.20it/s, env_step=49000, len=15, n/ep=0, n/st=10, player_1/loss=377.005, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: 10.000000 ± 0.000000, best_reward: 10.000000 ± 0.000000 in #49\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: TRAINING AGENTS\n",
    "####################################################\n",
    "\n",
    "# Train the agent\n",
    "off_policy_traininer_results, final_agent_player1, final_agent_player2 = train_agent(epochs= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8f3121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average steps of game:  16.333333333333332\n",
      "Final mean reward agent 1: -11.666666666666666, std: 30.64129385141706\n",
      "Final mean reward agent 2: -10.0, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: VIEWING THE LEARNED POLICY\n",
    "####################################################\n",
    "\n",
    "# Get the environment settings\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# Configure the best agent\n",
    "best_agent1 = cf_dqn_policy(state_shape= state_shape,\n",
    "                            action_shape= action_shape)\n",
    "best_agent1.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/2/dqn_vs_random/best_policy_agent1.pth\"))\n",
    "\n",
    "# Watch the best agetn at work\n",
    "watch(numer_of_games= 3,\n",
    "      agent_player1= best_agent1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84478519",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The DQN agent has learned to stack coins in the hope that the other agent won't stack in that column and thus he will get a quick win.\n",
    "This is a decent strategy against a random opponent but not that good against an actual human.\n",
    "This also results in the agent getting stuck in a situation where it keeps stacking coins onto a full row, collecting many negative rewards.\n",
    "This is likely to be resolved if more training is done but to make things more challenging for the smart DQN agent we will let him play against another DQN agent in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9110be",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN VARIABLES\n",
    "####################################################\n",
    "\n",
    "del action_shape\n",
    "del best_agent1\n",
    "del env\n",
    "del final_agent_player1\n",
    "del final_agent_player2\n",
    "del observation_space\n",
    "del off_policy_traininer_results\n",
    "del state_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d3da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
