{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Using the Rainbow algorithm to learn connect four\n",
    "\n",
    "The previous notebooks all used Deep Q-Networks (DQN) to learn to play connect four and no real desired behaviour was obtained.\n",
    "Since rainbow by [Hessel et al](https://doi.org/10.48550/arXiv.1710.02298) is known to outperform DQN in both sample efficiency and overall performance for Atari games, we explore its performance on the connect four game as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training two Rainbow agents on connect four Gym\n",
    "  - Building the environment\n",
    "  - Implementing the Rainbow policy\n",
    "  - Building agents\n",
    "  - Function for letting agents learn\n",
    "  - Function for watching learned agent\n",
    "  - Doing the experiment\n",
    "- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.21.0 recommended): 0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.12.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argparser\n",
    "import argparse\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.12.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# PPrint is a pretty print for variables\n",
    "from pprint import pprint\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(cfgym)\n",
    "\n",
    "# Time for allowing \"freezes\" in execution\n",
    "import time;\n",
    "\n",
    "# Allow for copying objects in a non reference manner\n",
    "import copy\n",
    "\n",
    "# Used for updating notebook display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training two Rainbow agents on connect four Gym\n",
    "\n",
    "Our connect four gym setup requires two agents, one for each player.\n",
    "To reduce complexity, agents will always play as the same player, e.g. always as player 1.\n",
    "It is important to note that connect four is a *solved game*.\n",
    "According to [The Washington Post](https://www.washingtonpost.com/news/wonk/wp/2015/05/08/how-to-win-any-popular-game-according-to-data-scientists/):\n",
    "\n",
    "> Connect Four is what mathematicians call a \"solved game,\" meaning you can play it perfectly every time, no matter what your opponent does. You will need to get the first move, but as long as you do so, you can always win within 41 moves.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Building the environment\n",
    "\n",
    "This code is taken from previous notebooks.\n",
    "We don't allow invalid moves to make the problem easier for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict(action_mask:Box([0 0 0 0 0 0 0], [1 1 1 1 1 1 1], (7,), int8), observation:Box([[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]], [[2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]], (6, 7), int8))\n",
      "\n",
      "Action space: Discrete(7)\n",
      "\n",
      " Initial player id:\n",
      "player_1\n",
      "\n",
      " Initial observation:\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " Initial mask:\n",
      "[True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CONNECT FOUR V2 ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "def get_env():\n",
    "    \"\"\"\n",
    "    Returns the connect four gym environment V2 altered for Tianshou and Petting Zoo compatibility.\n",
    "    Already wrapped with a ts.env.PettingZooEnv wrapper.\n",
    "    \"\"\"\n",
    "    return ts.env.PettingZooEnv(cfgym.env(reward_move= 1, # Set to 1 for reward to make moves (incentivise longer games)\n",
    "                                          reward_invalid= -3,\n",
    "                                          reward_draw= 15,\n",
    "                                          reward_win= 25,\n",
    "                                          reward_loss= -25,\n",
    "                                          allow_invalid_move= False))\n",
    "    \n",
    "    \n",
    "# Test the environment\n",
    "env = get_env()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"\\n Initial player id:\")\n",
    "print(observation[\"agent_id\"])\n",
    "\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation[\"obs\"])\n",
    "\n",
    "print(\"\\n Initial mask:\")\n",
    "print(observation[\"mask\"])\n",
    "\n",
    "# Clean unused variables\n",
    "del observation\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7e7da",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Implementing the Rainbow policy\n",
    "\n",
    "The Rainbow policy for the agent is configured and set up below.\n",
    "This is based on the Atari rainbow example by the [Tianshou doc](https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py).\n",
    "Tianshou uses a DQN as base class for Rainbow, so we will do this to but adopt both classes to better represent our previous DQN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e2f43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN ARCHITECTURE\n",
    "####################################################\n",
    "\n",
    "class DQNForRainbow(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN to be used as baseclass for the Rainbow algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        c: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        action_shape: typing.Sequence[int],\n",
    "        device: typing.Union[str, int, torch.device] = \"cpu\",\n",
    "        features_only: bool = False,\n",
    "        output_dim: typing.Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Number of input channels\n",
    "        input_channels_cnn = 1\n",
    "        output_channels_cnn = 32\n",
    "        self.output_dim = (h - 3) * (w - 3) * output_channels_cnn\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(0,-1),\n",
    "            torch.nn.Unflatten(0, (1, self.output_dim)),\n",
    "        )\n",
    "        if not features_only:\n",
    "            self.net = torch.nn.Sequential(\n",
    "                self.net, torch.nn.Linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Linear(512, np.prod(action_shape))\n",
    "            )\n",
    "            self.output_dim = np.prod(action_shape)\n",
    "        elif output_dim is not None:\n",
    "            self.net = torch.nn.Sequential(\n",
    "                self.net, torch.nn.Linear(self.output_dim, output_dim),\n",
    "                torch.nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "        state: typing.Optional[typing.Any] = None,\n",
    "        info: typing.Dict[str, typing.Any] = {},\n",
    "    ) -> typing.Tuple[torch.Tensor, typing.Any]:\n",
    "        r\"\"\"Mapping: s -> Q(s, \\*).\"\"\"\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        if (np.shape(obs)[0] != 1):\n",
    "            print(np.shape(obs))\n",
    "            obs = obs[:, None, :, :]\n",
    "            print(np.shape(obs))\n",
    "        \n",
    "        logits = self.net(obs)\n",
    "        return logits, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b137ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW ARCHITECTURE\n",
    "####################################################\n",
    "\n",
    "class Rainbow(DQNForRainbow):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        c: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        action_shape: typing.Sequence[int],\n",
    "        num_atoms: int = 51,\n",
    "        noisy_std: float = 0.5,\n",
    "        device: typing.Union[str, int, torch.device] = \"cpu\",\n",
    "        is_dueling: bool = True,\n",
    "        is_noisy: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__(c, h, w, action_shape, device, features_only=True)\n",
    "        self.action_num = np.prod(action_shape)\n",
    "        self.num_atoms = num_atoms\n",
    "\n",
    "        def linear(x, y):\n",
    "            if is_noisy:\n",
    "                return ts.utils.net.discrete.NoisyLinear(x, y, noisy_std)\n",
    "            else:\n",
    "                return torch.nn.Linear(x, y)\n",
    "\n",
    "        self.Q = torch.nn.Sequential(\n",
    "            linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "            linear(512, self.action_num * self.num_atoms)\n",
    "        )\n",
    "        self._is_dueling = is_dueling\n",
    "        if self._is_dueling:\n",
    "            self.V = torch.nn.Sequential(\n",
    "                linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                linear(512, self.num_atoms)\n",
    "            )\n",
    "        self.output_dim = self.action_num * self.num_atoms\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "        state: typing.Optional[typing.Any] = None,\n",
    "        info: typing.Dict[str, typing.Any] = {},\n",
    "    ) -> typing.Tuple[torch.Tensor, typing.Any]:\n",
    "        r\"\"\"Mapping: x -> Z(x, \\*).\"\"\"\n",
    "        obs, state = super().forward(obs)\n",
    "        q = self.Q(obs)\n",
    "        q = q.view(-1, self.action_num, self.num_atoms)\n",
    "        if self._is_dueling:\n",
    "            v = self.V(obs)\n",
    "            v = v.view(-1, 1, self.num_atoms)\n",
    "            logits = q - q.mean(dim=1, keepdim=True) + v\n",
    "        else:\n",
    "            logits = q\n",
    "        probs = logits.softmax(dim=2)\n",
    "        return probs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "539b56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW POLICY\n",
    "####################################################\n",
    "\n",
    "def rainbow_policy(state_shape: tuple,\n",
    "                  action_shape: tuple,\n",
    "                  optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                  learning_rate: float =  0.0000625,\n",
    "                  gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                  n_step: int = 1, # Number of steps to look ahead\n",
    "                  num_atoms: int = 51,\n",
    "                  target_update_freq: int = 500):\n",
    "    \n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Rainbow network to be used by policy\n",
    "    net = Rainbow(c= 1,\n",
    "                  h= state_shape[0],\n",
    "                  w= state_shape[1],\n",
    "                  action_shape= action_shape,                  \n",
    "                  device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.RainbowPolicy(model= net,\n",
    "                                   optim= optim,\n",
    "                                   discount_factor= gamma,\n",
    "                                   num_atoms= num_atoms,\n",
    "                                   estimation_step= n_step,\n",
    "                                   target_update_freq= target_update_freq).to(device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd411318",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Building agents\n",
    "\n",
    "Identical to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a06d25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT CREATION\n",
    "####################################################\n",
    "\n",
    "def get_agents(agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "               agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "               optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "               resume_path_player_1: str = '', # Path to file to resume agent training from\n",
    "               resume_path_player_2: str = '', \n",
    "               ) -> typing.Tuple[ts.policy.BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \"\"\"\n",
    "    Gets a multi agent policy manager, optimizer and player ids for the connect four V2 gym environment.\n",
    "    Per default this returns \n",
    "        - Multi agent manager for 2 agents using Rainbow\n",
    "        - Adam optimizer\n",
    "        - ['player_1', 'player_2'] from the connect four environment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the environment to play in (Connect four gym V2)\n",
    "    env = get_env()\n",
    "    \n",
    "    # Get the observation space from the environment, depending on typo of space (ternary operator)\n",
    "    observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "    \n",
    "    # Set the arguments\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    \n",
    "    # Configure agent player 1 to be a DQN if no policy is passed.\n",
    "    if agent_player1 is None:\n",
    "        # Our agent1 uses a DQN policy\n",
    "        agent_player1 = rainbow_policy(state_shape= state_shape,\n",
    "                                       action_shape= action_shape,\n",
    "                                       optim= optim)\n",
    "        \n",
    "        # If we resume our agent we need to load the previous config\n",
    "        if resume_path_player_1:\n",
    "            agent_player1.load_state_dict(torch.load(resume_path_player_1))\n",
    "    \n",
    "    # Configure agent player 2 to be a DQN if no policy is passed.\n",
    "    if agent_player2 is None:\n",
    "        # Our agent1 uses a DQN policy\n",
    "        agent_player2 = rainbow_policy(state_shape= state_shape,\n",
    "                                       action_shape= action_shape,\n",
    "                                       optim= optim)\n",
    "        \n",
    "        # If we resume our agent we need to load the previous config\n",
    "        if resume_path_player_2:\n",
    "            agent_player2.load_state_dict(torch.load(resume_path_player_2))\n",
    "\n",
    "    # Both our agents are DQN agents by default\n",
    "    agents = [agent_player1, agent_player2]\n",
    "        \n",
    "    # Our policy depends on the order of the agents\n",
    "    policy = ts.policy.MultiAgentPolicyManager(agents, env)\n",
    "    \n",
    "    # Return our policy, optimizer and the available agents in the environment\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using Rainbow\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    \n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8f3a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for letting agents learn\n",
    "\n",
    "Identical to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d364383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT TRAINING\n",
    "####################################################\n",
    "\n",
    "def train_agent(filename: str = \"rainbow_vs_rainbow\",\n",
    "                agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                training_env_num: int = 1,\n",
    "                testing_env_num: int = 1,\n",
    "                buffer_size: int = 2^14,\n",
    "                batch_size: int = 7, \n",
    "                epochs: int = 50, #50\n",
    "                step_per_epoch: int = 1024, #1024\n",
    "                step_per_collect: int = 64, # transition before update\n",
    "                update_per_step: float = 0.1,\n",
    "                testing_eps: float = 0.05,\n",
    "                training_eps: float = 0.1,\n",
    "                ) -> typing.Tuple[dict, ts.policy.BasePolicy]:\n",
    "    \"\"\"\n",
    "    Trains two agents in the connect four V2 environment and saves their best model and logs.\n",
    "    Returns:\n",
    "        - result from offpolicy_trainer\n",
    "        - final version of agent 1\n",
    "        - final version of agent 2\n",
    "    \"\"\"\n",
    "\n",
    "    # ======== notebook specific =========\n",
    "    notebook_version = '9' # Used for foldering logs and models\n",
    "\n",
    "    # ======== environment setup =========\n",
    "    train_envs = ts.env.DummyVectorEnv([get_env for _ in range(training_env_num)])\n",
    "    test_envs = ts.env.DummyVectorEnv([get_env for _ in range(testing_env_num)])\n",
    "    \n",
    "    # set the seed for reproducibility\n",
    "    np.random.seed(1998)\n",
    "    torch.manual_seed(1998)\n",
    "    train_envs.seed(1998)\n",
    "    test_envs.seed(1998)\n",
    "\n",
    "    # ======== agent setup =========\n",
    "    # Gets our agents from the previously made function\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using DQN\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    policy, optim, agents = get_agents(agent_player1=agent_player1,\n",
    "                                       agent_player2=agent_player2,\n",
    "                                       optim=optim)\n",
    "\n",
    "    # ======== collector setup =========\n",
    "    # Make a collector for the training environments\n",
    "    train_collector = ts.data.Collector(policy= policy,\n",
    "                                        env= train_envs,\n",
    "                                        buffer= ts.data.VectorReplayBuffer(buffer_size, len(train_envs)),\n",
    "                                        exploration_noise= True)\n",
    "    \n",
    "    # Make a collector for the testing environments\n",
    "    test_collector = ts.data.Collector(policy= policy,\n",
    "                                       env= test_envs,\n",
    "                                       buffer= ts.data.VectorReplayBuffer(buffer_size, len(test_envs)),\n",
    "                                       exploration_noise= True)\n",
    "    \n",
    "    # Uncomment below if you want to set epsilon in epsilon policy\n",
    "    # policy.set_eps(1)\n",
    "    \n",
    "    # Collect data fot the training evnironments\n",
    "    train_collector.collect(n_step= batch_size * training_env_num)\n",
    "    \n",
    "    # ======== ensure folders exist =========\n",
    "    if not os.path.exists(os.path.join('./logs', 'paper_notebooks', notebook_version, filename)):\n",
    "        os.makedirs(os.path.join('./logs', 'paper_notebooks', notebook_version, filename))\n",
    "    if not os.path.exists(os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename)):\n",
    "        os.makedirs(os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename))\n",
    "\n",
    "    # ======== tensorboard logging setup =========\n",
    "    # Allows to save the training progress to tensorboard compatable logs\n",
    "    log_path = os.path.join('./logs', 'paper_notebooks', notebook_version, filename)\n",
    "    writer = torch.utils.tensorboard.SummaryWriter(log_path)\n",
    "    logger = ts.utils.TensorboardLogger(writer)\n",
    "\n",
    "    # ======== callback functions used during training =========\n",
    "    # We want to save our best policy\n",
    "    def save_best_fn(policy):\n",
    "        \"\"\"\n",
    "        Callback to save the best model\n",
    "        \"\"\"\n",
    "        # Save best agent 1\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'best_policy_agent1.pth')\n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save best agent 2\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'best_policy_agent2.pth')\n",
    "        torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save agent2\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        \"\"\"\n",
    "        Callback to stop training when we've reached the win rate\n",
    "        \"\"\"\n",
    "        return mean_rewards >= 7 # (win = 10, 70% win without invalid moves = mean of 7)\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback before training\n",
    "        \"\"\"        \n",
    "        # Before training we want to configure the epsilon for the agents\n",
    "        # In general more exploratory than the test case\n",
    "        policy.policies[agents[0]].set_eps(training_eps)\n",
    "        policy.policies[agents[1]].set_eps(training_eps)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback beore testing\n",
    "        \"\"\"        \n",
    "        # Before testing we want to configure the epsilon for the agents\n",
    "        # In general more greedy than the train case but not\n",
    "        #   to avoid getting stuck on invalid moves\n",
    "        policy.policies[agents[0]].set_eps(testing_eps)\n",
    "        policy.policies[agents[1]].set_eps(testing_eps)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        \"\"\"\n",
    "        Callback for reward collection\n",
    "        \"\"\"\n",
    "        # We are interested in having a high total total reward,\n",
    "        #   as this would mean equally good agents.\n",
    "        return rews[:, 0] + rews[:, 1]\n",
    "\n",
    "    # trainer\n",
    "    result = ts.trainer.offpolicy_trainer(policy= policy,\n",
    "                                          train_collector= train_collector,\n",
    "                                          test_collector= test_collector,\n",
    "                                          max_epoch= epochs,\n",
    "                                          step_per_epoch= step_per_epoch,\n",
    "                                          step_per_collect= step_per_collect,\n",
    "                                          episode_per_test= testing_env_num,\n",
    "                                          batch_size= batch_size,\n",
    "                                          train_fn= train_fn,\n",
    "                                          test_fn= test_fn,\n",
    "                                          # Stop function to stop before specified amount of epochs\n",
    "                                          #stop_fn= stop_fn\n",
    "                                          save_best_fn= save_best_fn,\n",
    "                                          update_per_step= update_per_step,\n",
    "                                          logger= logger,\n",
    "                                          test_in_train= False,\n",
    "                                          reward_metric= reward_metric)\n",
    "    \n",
    "    # Save final agent 1\n",
    "    model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'final_policy_agent1.pth')\n",
    "    torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "\n",
    "    # Save final agent 2\n",
    "    model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'final_policy_agent2.pth')\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "    return result, policy.policies[agents[0]], policy.policies[agents[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161cc0f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for watching learned agent\n",
    "\n",
    "Identical to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b526e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# WATCHING THE LEARNED POLICY IN ACTION\n",
    "####################################################\n",
    "\n",
    "def watch(numer_of_games: int = 3,\n",
    "          agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          test_epsilon: float = 0.05, # For the watching we act completely greedy but low random for not getting stuck on invalid move\n",
    "          render_speed: float = 0.15, # Amount of seconds to update frame/ do a step\n",
    "          ) -> None:\n",
    "    \n",
    "    # Get the connect four V2 environment (must be a list)\n",
    "    env= ts.env.DummyVectorEnv([get_env])\n",
    "    \n",
    "    # Get the agents from the trained agents\n",
    "    policy, optim, agents = get_agents(agent_player1= agent_player1,\n",
    "                                       agent_player2= agent_player2)\n",
    "    \n",
    "    # Evaluate the policy\n",
    "    policy.eval()\n",
    "    \n",
    "    # Set the testing policy epsilon for our agents\n",
    "    policy.policies[agents[0]].set_eps(test_epsilon)\n",
    "    policy.policies[agents[1]].set_eps(test_epsilon)\n",
    "    \n",
    "    # Collect the test data\n",
    "    collector = ts.data.Collector(policy= policy,\n",
    "                                  env= env,\n",
    "                                  exploration_noise= True)\n",
    "    \n",
    "    # Render games in human mode to see how it plays\n",
    "    result = collector.collect(n_episode= numer_of_games, render= render_speed)\n",
    "    \n",
    "    # Close the environment aftering collecting the results\n",
    "    # This closes the pygame window after completion\n",
    "    env.close()\n",
    "    \n",
    "    # Get the rewards and length from the test trials\n",
    "    rewards, length = result[\"rews\"], result[\"lens\"]\n",
    "    \n",
    "    # Print the final reward for the first agent\n",
    "    print(f\"Average steps of game:  {length.mean()}\")\n",
    "    print(f\"Final mean reward agent 1: {rewards[:, 0].mean()}, std: {rewards[:, 0].std()}\")\n",
    "    print(f\"Final mean reward agent 2: {rewards[:, 1].mean()}, std: {rewards[:, 1].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31784e9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Doing the experiment\n",
    "\n",
    "We now do the experiment with using our previously created functions.\n",
    "We update some parameter settings to find if we can improve our DQN agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93fe8961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   6%|6         | 64/1024 [00:00<00:03, 295.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 7])\n",
      "torch.Size([4, 1, 6, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unflatten: Provided sizes [1, 384] don't multiply up to the size of dim 0 (1536) in the input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\fast_files\\GitHub\\VUB-RL\\project\\paper_notebooks\\9-rainbow.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000022?line=15'>16</a>\u001b[0m agent2 \u001b[39m=\u001b[39m rainbow_policy(state_shape\u001b[39m=\u001b[39m state_shape,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000022?line=16'>17</a>\u001b[0m                         action_shape\u001b[39m=\u001b[39m action_shape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000022?line=18'>19</a>\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000022?line=19'>20</a>\u001b[0m off_policy_traininer_results, final_agent_player1, final_agent_player2 \u001b[39m=\u001b[39m train_agent(epochs\u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000022?line=20'>21</a>\u001b[0m                                                                                      training_eps\u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m)\n",
      "\u001b[1;32mc:\\fast_files\\GitHub\\VUB-RL\\project\\paper_notebooks\\9-rainbow.ipynb Cell 19'\u001b[0m in \u001b[0;36mtrain_agent\u001b[1;34m(filename, agent_player1, agent_player2, optim, training_env_num, testing_env_num, buffer_size, batch_size, epochs, step_per_epoch, step_per_collect, update_per_step, testing_eps, training_eps)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=128'>129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rews[:, \u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m rews[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=130'>131</a>\u001b[0m \u001b[39m# trainer\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=131'>132</a>\u001b[0m result \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49moffpolicy_trainer(policy\u001b[39m=\u001b[39;49m policy,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=132'>133</a>\u001b[0m                                       train_collector\u001b[39m=\u001b[39;49m train_collector,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=133'>134</a>\u001b[0m                                       test_collector\u001b[39m=\u001b[39;49m test_collector,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=134'>135</a>\u001b[0m                                       max_epoch\u001b[39m=\u001b[39;49m epochs,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=135'>136</a>\u001b[0m                                       step_per_epoch\u001b[39m=\u001b[39;49m step_per_epoch,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=136'>137</a>\u001b[0m                                       step_per_collect\u001b[39m=\u001b[39;49m step_per_collect,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=137'>138</a>\u001b[0m                                       episode_per_test\u001b[39m=\u001b[39;49m testing_env_num,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=138'>139</a>\u001b[0m                                       batch_size\u001b[39m=\u001b[39;49m batch_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=139'>140</a>\u001b[0m                                       train_fn\u001b[39m=\u001b[39;49m train_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=140'>141</a>\u001b[0m                                       test_fn\u001b[39m=\u001b[39;49m test_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=141'>142</a>\u001b[0m                                       \u001b[39m# Stop function to stop before specified amount of epochs\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=142'>143</a>\u001b[0m                                       \u001b[39m#stop_fn= stop_fn\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=143'>144</a>\u001b[0m                                       save_best_fn\u001b[39m=\u001b[39;49m save_best_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=144'>145</a>\u001b[0m                                       update_per_step\u001b[39m=\u001b[39;49m update_per_step,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=145'>146</a>\u001b[0m                                       logger\u001b[39m=\u001b[39;49m logger,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=146'>147</a>\u001b[0m                                       test_in_train\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=147'>148</a>\u001b[0m                                       reward_metric\u001b[39m=\u001b[39;49m reward_metric)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=149'>150</a>\u001b[0m \u001b[39m# Save final agent 1\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000018?line=150'>151</a>\u001b[0m model_save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m./saved_variables\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpaper_notebooks\u001b[39m\u001b[39m'\u001b[39m, notebook_version, filename, \u001b[39m'\u001b[39m\u001b[39mfinal_policy_agent1.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:129\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=121'>122</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moffpolicy_trainer\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=122'>123</a>\u001b[0m     \u001b[39m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=123'>124</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=124'>125</a>\u001b[0m \u001b[39m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=125'>126</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=126'>127</a>\u001b[0m \u001b[39m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=127'>128</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=128'>129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m OffpolicyTrainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\trainer\\base.py:418\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=415'>416</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=416'>417</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=417'>418</a>\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=418'>419</a>\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=419'>420</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=420'>421</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=421'>422</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=422'>423</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\trainer\\base.py:281\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=277'>278</a>\u001b[0m         result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=278'>279</a>\u001b[0m         t\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=280'>281</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_update_fn(data, result)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=281'>282</a>\u001b[0m     t\u001b[39m.\u001b[39mset_postfix(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/base.py?line=283'>284</a>\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:118\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[1;34m(self, data, result)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=115'>116</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_per_step \u001b[39m*\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=116'>117</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=117'>118</a>\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_collector\u001b[39m.\u001b[39;49mbuffer)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/trainer/offpolicy.py?line=118'>119</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_update_data(data, losses)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\policy\\base.py:277\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[1;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/base.py?line=274'>275</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/base.py?line=275'>276</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_fn(batch, buffer, indices)\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/base.py?line=276'>277</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/base.py?line=277'>278</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_process_fn(batch, buffer, indices)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/base.py?line=278'>279</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\policy\\multiagent\\mapolicy.py:194\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.learn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/multiagent/mapolicy.py?line=191'>192</a>\u001b[0m data \u001b[39m=\u001b[39m batch[agent_id]\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/multiagent/mapolicy.py?line=192'>193</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data\u001b[39m.\u001b[39mis_empty():\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/multiagent/mapolicy.py?line=193'>194</a>\u001b[0m     out \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mlearn(batch\u001b[39m=\u001b[39;49mdata, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/multiagent/mapolicy.py?line=194'>195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m out\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/multiagent/mapolicy.py?line=195'>196</a>\u001b[0m         results[agent_id \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m k] \u001b[39m=\u001b[39m v\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\policy\\modelfree\\rainbow.py:39\u001b[0m, in \u001b[0;36mRainbowPolicy.learn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/rainbow.py?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target \u001b[39mand\u001b[39;00m sample_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_old):\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/rainbow.py?line=37'>38</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_old\u001b[39m.\u001b[39mtrain()  \u001b[39m# so that NoisyLinear takes effect\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/rainbow.py?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\policy\\modelfree\\c51.py:96\u001b[0m, in \u001b[0;36mC51Policy.learn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=93'>94</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=94'>95</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=95'>96</a>\u001b[0m     target_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_dist(batch)\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=96'>97</a>\u001b[0m weight \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=97'>98</a>\u001b[0m curr_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(batch)\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\policy\\modelfree\\c51.py:75\u001b[0m, in \u001b[0;36mC51Policy._target_dist\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_target_dist\u001b[39m(\u001b[39mself\u001b[39m, batch: Batch) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=73'>74</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target:\n\u001b[1;32m---> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=74'>75</a>\u001b[0m         act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(batch, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mobs_next\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mact\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=75'>76</a>\u001b[0m         next_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(batch, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_old\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobs_next\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mlogits\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/c51.py?line=76'>77</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tianshou\\policy\\modelfree\\dqn.py:157\u001b[0m, in \u001b[0;36mDQNPolicy.forward\u001b[1;34m(self, batch, state, model, input, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/dqn.py?line=154'>155</a>\u001b[0m obs \u001b[39m=\u001b[39m batch[\u001b[39minput\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/dqn.py?line=155'>156</a>\u001b[0m obs_next \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mobs \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obs, \u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m obs\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/dqn.py?line=156'>157</a>\u001b[0m logits, hidden \u001b[39m=\u001b[39m model(obs_next, state\u001b[39m=\u001b[39;49mstate, info\u001b[39m=\u001b[39;49mbatch\u001b[39m.\u001b[39;49minfo)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/dqn.py?line=157'>158</a>\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_q_value(logits, \u001b[39mgetattr\u001b[39m(obs, \u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/tianshou/policy/modelfree/dqn.py?line=158'>159</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax_action_num\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\fast_files\\GitHub\\VUB-RL\\project\\paper_notebooks\\9-rainbow.ipynb Cell 14'\u001b[0m in \u001b[0;36mRainbow.forward\u001b[1;34m(self, obs, state, info)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=45'>46</a>\u001b[0m     obs: typing\u001b[39m.\u001b[39mUnion[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=46'>47</a>\u001b[0m     state: typing\u001b[39m.\u001b[39mOptional[typing\u001b[39m.\u001b[39mAny] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=47'>48</a>\u001b[0m     info: typing\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, typing\u001b[39m.\u001b[39mAny] \u001b[39m=\u001b[39m {},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=48'>49</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mTuple[torch\u001b[39m.\u001b[39mTensor, typing\u001b[39m.\u001b[39mAny]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=49'>50</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Mapping: x -> Z(x, \\*).\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=50'>51</a>\u001b[0m     obs, state \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=51'>52</a>\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000013?line=52'>53</a>\u001b[0m     q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_num, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_atoms)\n",
      "\u001b[1;32mc:\\fast_files\\GitHub\\VUB-RL\\project\\paper_notebooks\\9-rainbow.ipynb Cell 13'\u001b[0m in \u001b[0;36mDQNForRainbow.forward\u001b[1;34m(self, obs, state, info)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000012?line=56'>57</a>\u001b[0m     obs \u001b[39m=\u001b[39m obs[:, \u001b[39mNone\u001b[39;00m, :, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000012?line=57'>58</a>\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mshape(obs))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000012?line=59'>60</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/fast_files/GitHub/VUB-RL/project/paper_notebooks/9-rainbow.ipynb#ch0000012?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, state\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:138\u001b[0m, in \u001b[0;36mUnflatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/flatten.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/nn/modules/flatten.py?line=137'>138</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49munflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munflattened_size)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\_tensor.py:992\u001b[0m, in \u001b[0;36mTensor.unflatten\u001b[1;34m(self, dim, sizes)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/_tensor.py?line=989'>990</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sizes, OrderedDict) \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(sizes, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(sizes[\u001b[39m0\u001b[39m], (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m))):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/_tensor.py?line=990'>991</a>\u001b[0m     names, sizes \u001b[39m=\u001b[39m unzip_namedshape(sizes)\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/rl-project/lib/site-packages/torch/_tensor.py?line=991'>992</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(Tensor, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49munflatten(dim, sizes, names)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unflatten: Provided sizes [1, 384] don't multiply up to the size of dim 0 (1536) in the input tensor"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: TRAINING AGENTS\n",
    "####################################################\n",
    "\n",
    "# Get the environment settings\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# Configure the agents\n",
    "agent1 = rainbow_policy(state_shape= state_shape,\n",
    "                        action_shape= action_shape)\n",
    "\n",
    "\n",
    "agent2 = rainbow_policy(state_shape= state_shape,\n",
    "                        action_shape= action_shape)\n",
    "\n",
    "# Train the agent\n",
    "off_policy_traininer_results, final_agent_player1, final_agent_player2 = train_agent(epochs= 500,\n",
    "                                                                                     training_eps= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: VIEWING THE BEST LEARNED POLICY\n",
    "####################################################\n",
    "\n",
    "# Get the environment settings\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# Configure the best agent\n",
    "best_agent1 = rainbow_policy(state_shape= state_shape,\n",
    "                             action_shape= action_shape)\n",
    "best_agent1.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/9/rainbow_vs_rainbow/best_policy_agent1.pth\"))\n",
    "best_agent1.set_eps(0)\n",
    "\n",
    "\n",
    "best_agent2 = rainbow_policy(state_shape= state_shape,\n",
    "                             action_shape= action_shape)\n",
    "best_agent2.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/9/rainbow_vs_rainbow/best_policy_agent2.pth\"))\n",
    "best_agent2.set_eps(0)\n",
    "\n",
    "# Watch the best agent at work\n",
    "watch(numer_of_games= 3,\n",
    "      render_speed= 0.3,\n",
    "      agent_player1= best_agent1,\n",
    "      agent_player2= best_agent2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: VIEWING THE LAST LEARNED POLICY\n",
    "####################################################\n",
    "\n",
    "# Configure the final agent\n",
    "final_agent_player1 = rainbow_policy(state_shape= state_shape,\n",
    "                                     action_shape= action_shape)\n",
    "final_agent_player1.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/9/rainbow_vs_rainbow/final_policy_agent1.pth\"))\n",
    "best_agent1.set_eps(0)\n",
    "\n",
    "final_agent_player2 = rainbow_policy(state_shape= state_shape,\n",
    "                                     action_shape= action_shape)\n",
    "final_agent_player2.load_state_dict(torch.load(\"./saved_variables/paper_notebooks/9/rainbow_vs_rainbow/final_policy_agent2.pth\"))\n",
    "best_agent2.set_eps(0)\n",
    "\n",
    "# Watch the best agent at work\n",
    "watch(numer_of_games= 3,\n",
    "      render_speed= 0.3,\n",
    "      agent_player1= final_agent_player1,\n",
    "      agent_player2= final_agent_player2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84478519",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The performance of the rainbow algorithm is near identical to DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9110be",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN VARIABLES\n",
    "####################################################\n",
    "\n",
    "del action_shape\n",
    "del agent1\n",
    "del agent2\n",
    "del best_agent1\n",
    "del best_agent2\n",
    "del env\n",
    "del final_agent_player1\n",
    "del final_agent_player2\n",
    "del observation_space\n",
    "del off_policy_traininer_results\n",
    "del state_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d3da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
