{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Bot vs random agent\n",
    "\n",
    "To test the performance of the bots in an objective manner, the win rate against a random agent is determined along with the average length of a game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Setting up the environment\n",
    "- Loading policies\n",
    "  - MLP based DQN\n",
    "  - CNN based DQN\n",
    "  - CNN based Rainbow\n",
    "  - MiniMax policy\n",
    "- Loading in torch dictionaries\n",
    "- Agent manager and win rate tester\n",
    "- Setup the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.21.0 recommended): 0.21.0\n",
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.12.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.12.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# PPrint is a pretty print for variables\n",
    "from pprint import pprint\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym\n",
    "import minimax_agent.minimax_agent as minimaxbot\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(cfgym);\n",
    "importlib.reload(minimaxbot);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Show current cuda device\n",
    "    print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "    # Show cuda device name\n",
    "    print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5830fc7",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cefae546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict(action_mask:Box([0 0 0 0 0 0 0], [1 1 1 1 1 1 1], (7,), int8), observation:Box([[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]], [[2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]], (6, 7), int8))\n",
      "\n",
      "Action space: Discrete(7)\n",
      "\n",
      " Initial player id:\n",
      "player_1\n",
      "\n",
      " Initial observation:\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " Initial mask:\n",
      "[True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CONNECT FOUR V2 ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "def get_env():\n",
    "    \"\"\"\n",
    "    Returns the connect four gym environment V2 altered for Tianshou and Petting Zoo compatibility.\n",
    "    Already wrapped with a ts.env.PettingZooEnv wrapper.\n",
    "    Set up for testing the win rate, thus only a reward is given for winning (10).\n",
    "    \"\"\"\n",
    "    return ts.env.PettingZooEnv(cfgym.env(reward_move= 0,\n",
    "                                          reward_blocking= 0,\n",
    "                                          reward_invalid= 0,\n",
    "                                          reward_draw= 0,\n",
    "                                          reward_win= 10,\n",
    "                                          reward_loss= 0,\n",
    "                                          allow_invalid_move= False))\n",
    "    \n",
    "    \n",
    "# Test the environment\n",
    "env = get_env()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"\\n Initial player id:\")\n",
    "print(observation[\"agent_id\"])\n",
    "\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation[\"obs\"])\n",
    "\n",
    "print(\"\\n Initial mask:\")\n",
    "print(observation[\"mask\"])\n",
    "\n",
    "# Clean unused variables\n",
    "del observation\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Loading policies\n",
    "\n",
    "We need to specify the policies the trained agent's weight are for.\n",
    "These are taken from previous notebooks.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### MLP based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a940ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY FROM PAPER NOTEBOOK 5\n",
    "####################################################\n",
    "\n",
    "class CustomDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(np.prod(state_shape), 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, np.prod(action_shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "def cf_custom_dqn_policy(state_shape: tuple,\n",
    "                         action_shape: tuple,\n",
    "                         learning_rate: float =  0.0001,\n",
    "                         gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                         n_step: int = 1, # Number of steps to look ahead\n",
    "                         target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CustomDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618df158",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### CNN based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae2eb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY FROM PAPER NOTEBOOK 7\n",
    "####################################################\n",
    "\n",
    "class CNNBasedDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        # Number of input channels\n",
    "        input_channels_cnn = 1\n",
    "        output_channels_cnn = 32\n",
    "        flatten_size = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        output_size= np.prod(action_shape)\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(0,-1),\n",
    "            torch.nn.Unflatten(0, (1, flatten_size)),\n",
    "            torch.nn.Linear(flatten_size, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        \n",
    "        logits = self.model(obs)\n",
    "        return logits, state\n",
    "\n",
    "    \n",
    "def cf_cnn_dqn_policy(state_shape: tuple,\n",
    "                      action_shape: tuple,\n",
    "                      optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                      learning_rate: float =  0.0001,\n",
    "                      gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                      n_step: int = 4, # Number of steps to look ahead\n",
    "                      frozen: bool = False,\n",
    "                      target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CNNBasedDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # If we are frozen, we use an optimizer that has learning rate 0\n",
    "    if frozen:\n",
    "        optim = torch.optim.SGD(net.parameters(), lr= 0)\n",
    "        \n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56467c69",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### CNN based Rainbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66219e45",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### CNN based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f748a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW POLICY FROM PAPER NOTEBOOK 9\n",
    "####################################################\n",
    "\n",
    "class CNNForRainbow(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN to be used as baseclass for the Rainbow algorithm.\n",
    "    Extracts \"feautures\" for the Rainbow algorithm by doing a 4x4 cnn kernel pass and providing 16 filters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \n",
    "        # Torch init\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store device to be used\n",
    "        self.device = device\n",
    "        \n",
    "        # The input layer is singular -> we have 1 board vector\n",
    "        input_channels_cnn = 1\n",
    "        \n",
    "        # We output 64/16 filters per kernel \n",
    "        output_channels_cnn = 64 # Increased from 16\n",
    "        \n",
    "        # We store the output dimension of the CNN \"feature\" layer\n",
    "        self.output_dim = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "                state: typing.Optional[typing.Any] = None,\n",
    "                info: typing.Dict[str, typing.Any] = {}):\n",
    "        # Make a torch instance (from regular vector of board)\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "            \n",
    "        # Tianshou bugs the batch output, reshape to work properly with our torch version\n",
    "        if (len(np.shape(obs)) != 4):\n",
    "            obs = obs[:, None, :, :]\n",
    "        \n",
    "        # Return what is needed (network output & state)\n",
    "        return self.net(obs), state\n",
    "\n",
    "class Rainbow(CNNForRainbow):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow algorithm making using of the CNNForRainbow baseclass.\n",
    "    Default parameters adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                 num_atoms: int = 51,\n",
    "                 is_noisy: bool = True,\n",
    "                 noisy_std: float = 0.1,\n",
    "                 is_dueling: bool = True):\n",
    "        \n",
    "        # Init CNN feature extraction parent class\n",
    "        super().__init__(state_shape= state_shape, device= device)\n",
    "        \n",
    "        # the amount of actions we have is just the action shape\n",
    "        self.action_num = np.prod(action_shape)\n",
    "        \n",
    "        # Store class specific info\n",
    "        self.num_atoms = num_atoms\n",
    "        self._is_dueling = is_dueling\n",
    "\n",
    "        # Our linear layer depends on wether or not we want to use a noisy environment\n",
    "        # Noisy implementation based on https://arxiv.org/abs/1706.10295\n",
    "        def linear(x, y):\n",
    "            if is_noisy:\n",
    "                return ts.utils.net.discrete.NoisyLinear(x, y, noisy_std)\n",
    "            else:\n",
    "                return torch.nn.Linear(x, y)\n",
    "            \n",
    "        # Specify Q and V based on wether or not agent is dueling\n",
    "        # Setting agent on dueling mode should help generalisation according to rainbow paper\n",
    "        # NOTE: this uses the output dim from the feature extraction CNN\n",
    "        self.Q = torch.nn.Sequential(\n",
    "            linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "            linear(512, self.action_num * self.num_atoms))\n",
    "        \n",
    "        if self._is_dueling:\n",
    "            self.V = torch.nn.Sequential(\n",
    "                linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                linear(512, self.num_atoms))\n",
    "            \n",
    "        # New output dim for this rainbow network\n",
    "        self.output_dim = self.action_num * self.num_atoms\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "                state: typing.Optional[typing.Any] = None,\n",
    "                info: typing.Dict[str, typing.Any] = {}):\n",
    "        \n",
    "        # Use our parent CNN based network to get \"features\"\n",
    "        obs, state = super().forward(obs)\n",
    "        \n",
    "        # Get our Rainbow specific values\n",
    "        q = self.Q(obs)\n",
    "        q = q.view(-1, self.action_num, self.num_atoms)\n",
    "        \n",
    "        if self._is_dueling:\n",
    "            v = self.V(obs)\n",
    "            v = v.view(-1, 1, self.num_atoms)\n",
    "            logits = q - q.mean(dim=1, keepdim=True) + v\n",
    "        else:\n",
    "            logits = q\n",
    "        \n",
    "        # We need to go from our logits to an accepted dimension of probability outputs\n",
    "        probs = logits.softmax(dim=2)\n",
    "        \n",
    "        return probs, state\n",
    "    \n",
    "    \n",
    "def rainbow_policy(state_shape: tuple,\n",
    "                   action_shape: tuple,\n",
    "                   optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                   learning_rate: float =  0.0000625,\n",
    "                   gamma: float = 0.9,\n",
    "                   n_step: int = 3, # Number of steps to look ahead\n",
    "                   num_atoms: int = 51,\n",
    "                   is_noisy: bool = True,\n",
    "                   noisy_std: float = 0.1,\n",
    "                   is_dueling: bool = True,\n",
    "                   target_update_freq: int = 500):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow policy.\n",
    "    Default parameters adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Rainbow network to be used by policy\n",
    "    net = Rainbow(state_shape= state_shape,\n",
    "                  action_shape= action_shape,\n",
    "                  device= device,\n",
    "                  num_atoms= num_atoms,\n",
    "                  is_noisy= is_noisy,\n",
    "                  noisy_std= noisy_std,\n",
    "                  is_dueling= is_dueling).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agents Rainbow policy\n",
    "    return ts.policy.RainbowPolicy(model= net,\n",
    "                                   optim= optim,\n",
    "                                   discount_factor= gamma,\n",
    "                                   num_atoms= num_atoms,\n",
    "                                   estimation_step= n_step,\n",
    "                                   target_update_freq= target_update_freq).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ffe2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### MiniMax policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4213b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CUSTOM MINIMAX TIANSHOU POLICY\n",
    "####################################################\n",
    "\n",
    "class TianshouMiniMaxConnectFourPolicy(ts.policy.BasePolicy):\n",
    "    \"\"\"\n",
    "    Tianshou compatible MiniMax policy for connect four.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 coin: int,\n",
    "                 oponent_coin: int,\n",
    "                 minimax_depth: int,\n",
    "                 column_count: int = 7,\n",
    "                 row_count: int = 6,\n",
    "                 **kwargs: typing.Any):\n",
    "        # Init base policy\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Configure minimax bot\n",
    "        self.bot = minimaxbot.MiniMaxConnectFourBot(coin= coin,\n",
    "                                                    oponent_coin= oponent_coin,\n",
    "                                                    column_count= column_count,\n",
    "                                                    row_count= row_count,\n",
    "                                                    minimax_depth= minimax_depth)\n",
    "\n",
    "    def forward(self,\n",
    "                batch: ts.data.Batch,\n",
    "                state: typing.Optional[typing.Union[dict, ts.data.Batch, np.ndarray]] = None,\n",
    "                **kwargs: typing.Any):\n",
    "        \"\"\"\n",
    "        Compute minimax action over the given batch data.\n",
    "        \"\"\"\n",
    "        boards = batch[\"obs\"]\n",
    "        \n",
    "        # Can be nested in Tianshou\n",
    "        while isinstance(boards, ts.data.Batch):\n",
    "            boards = boards[\"obs\"]\n",
    "        \n",
    "        preds = [None] * len(boards)        \n",
    "        \n",
    "        for i in range(len(boards)):\n",
    "            preds[i] = self.bot.predict(board= boards[i])\n",
    "            \n",
    "        \n",
    "        return ts.data.Batch(act=preds, state=state)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        # No learning needed\n",
    "        return {}\n",
    "    \n",
    "    def set_eps(self, eps):\n",
    "        # Not needed\n",
    "        return\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffc0e6",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Loading in torch dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a4c5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# FUNCTION FOR LOADING IN TORCH DICTIONARIIES\n",
    "####################################################\n",
    "\n",
    "def load_torch_dict(filename):\n",
    "    \"\"\"\n",
    "    Loads in torch dictionary using correct cuda settings for current device\n",
    "    \"\"\"   \n",
    "    if torch.cuda.is_available():\n",
    "        return torch.load(filename)\n",
    "    else:\n",
    "        return torch.load(filename, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9d840",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Agent manager and win rate tester\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e18cf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT CREATION\n",
    "####################################################\n",
    "\n",
    "def get_agent_manager(agent_player1: typing.Optional[ts.policy.BasePolicy],\n",
    "                      agent_player2: typing.Optional[ts.policy.BasePolicy]):\n",
    "    \"\"\"\n",
    "    Gets a multi agent policy manager for the provided agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the environment to play in (Connect four gym V2)\n",
    "    env = get_env()\n",
    "\n",
    "    # Default order of the agents\n",
    "    agents = [agent_player1, agent_player2]\n",
    "        \n",
    "    # Create the multi agent policy\n",
    "    policy = ts.policy.MultiAgentPolicyManager(agents, env)\n",
    "    \n",
    "    # Return our policy, optimizer and the available agents in the environment\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using Rainbow\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    \n",
    "    return policy, env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "459114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# WATCHING THE LEARNED POLICY IN ACTION\n",
    "####################################################\n",
    "\n",
    "def test_win_rate(numer_of_games: int,\n",
    "                  agent_player1: typing.Optional[ts.policy.BasePolicy],\n",
    "                  agent_player2: typing.Optional[ts.policy.BasePolicy]):\n",
    "    \n",
    "    # Get the connect four V2 environment (must be a list)\n",
    "    env= ts.env.DummyVectorEnv([get_env])\n",
    "    \n",
    "    # Get the agents from the trained agents\n",
    "    policy, agents = get_agent_manager(agent_player1= agent_player1,\n",
    "                                       agent_player2= agent_player2)\n",
    "    \n",
    "    # Evaluate the policy\n",
    "    policy.eval()\n",
    "    \n",
    "    # Collect the test data\n",
    "    collector = ts.data.Collector(policy= policy,\n",
    "                                  env= env,\n",
    "                                  exploration_noise= True)\n",
    "    \n",
    "    # Render games in human mode to see how it plays\n",
    "    result = collector.collect(n_episode= numer_of_games)\n",
    "    \n",
    "    # Close the environment aftering collecting the results\n",
    "    # This closes the pygame window after completion\n",
    "    env.close()\n",
    "    \n",
    "    # Get the rewards and length from the test trials\n",
    "    rewards, length = result[\"rews\"], result[\"lens\"]\n",
    "    \n",
    "    # Print the final reward for the first agent\n",
    "    print(f\"Played {numer_of_games} games.\")\n",
    "    print(f\"Average steps of game:  {length.mean()}, std: {length.std()}\")\n",
    "    print(f\"Final mean reward agent 1: {rewards[:, 0].mean()}, std: {rewards[:, 0].std()}\")\n",
    "    print(f\"Final mean reward agent 2: {rewards[:, 1].mean()}, std: {rewards[:, 1].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a4e6c",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Setup the experiment\n",
    "\n",
    "Results:\n",
    "- P1 MLP DQN vs random\n",
    "  - Score: 9.03, std: 2.9595776725742473\n",
    "  - Length: 11.351, std: 7.226465180155509\n",
    "- P2 MLP DQN vs random\n",
    "  - Score: 8.03, std: 3.9773232204587043\n",
    "  - Length: 12.321, std: 5.362644776600441\n",
    "- P1 CNN DQN vs random\n",
    "  - Score: 8.98, std: 3.0264831075028327\n",
    "  - Length: 11.37, std: 6.58825470060167\n",
    "- P2 CNN DQN vs random\n",
    "  - Score: 7.98, std: 4.014922166119787\n",
    "  - Length: 11.156, std: 5.5983626177660195\n",
    "- P1 rainbow vs random:\n",
    "  - Score: 9.49, std: 2.199977272609878\n",
    "  - Length: 11.193, std: 5.6767729389152075\n",
    "- P2 rainbow vs random\n",
    "  - Score: 9.16, std: 2.77387815161373\n",
    "  - Length: 11.812, std: 6.509120985202227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETUP THE GAME\n",
    "####################################################\n",
    "\n",
    "# Configure the rainbow agent\n",
    "random_agent = ts.policy.RandomPolicy()\n",
    "\n",
    "# Configure best MLP DQN player 1\n",
    "best_mlp_dqn_player1 = cf_custom_dqn_policy(state_shape= (6, 7),\n",
    "                                            action_shape= (7,))\n",
    "best_mlp_dqn_player1.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/8/7-20epoch_500loop/looping-iteration-498/best_policy_agent1.pth\"))\n",
    "best_mlp_dqn_player1.set_eps(0.001) # not zero in case agent got stuck\n",
    "\n",
    "# Configure best MLP DQN player 2\n",
    "best_mlp_dqn_player2 = cf_custom_dqn_policy(state_shape= (6, 7),\n",
    "                                            action_shape= (7,))\n",
    "best_mlp_dqn_player2.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/8/7-20epoch_500loop/looping-iteration-499/best_policy_agent2.pth\"))\n",
    "best_mlp_dqn_player2.set_eps(0.001) # not zero in case agent got stuck\n",
    "\n",
    "# Configure best MLP CNN player 1\n",
    "best_cnn_dqn_player1 = cf_cnn_dqn_policy(state_shape= (6, 7),\n",
    "                                         action_shape= (7,))\n",
    "best_cnn_dqn_player1.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/7/7-20epoch_500loop/7-looping-iteration-498/best_policy_agent1.pth\"))\n",
    "best_cnn_dqn_player1.set_eps(0.001) # not zero in case agent got stuck\n",
    "\n",
    "# Configure best MLP CNN player 2\n",
    "best_cnn_dqn_player2 = cf_cnn_dqn_policy(state_shape= (6, 7),\n",
    "                                         action_shape= (7,))\n",
    "best_cnn_dqn_player2.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/6/dqn_vs_dqn_cnn_based/best_policy_agent2.pth\"))\n",
    "best_cnn_dqn_player2.set_eps(0.001) # not zero in case agent got stuck\n",
    "\n",
    "# Configure best rainbow player 1\n",
    "best_rainbow_player1 = rainbow_policy(state_shape= (6, 7),\n",
    "                                      action_shape= (7,))\n",
    "best_rainbow_player1.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/9/rainbow_vs_rainbow_blocking_reward_complex_cnn/best_policy_agent1.pth\"))\n",
    "best_rainbow_player1.set_eps(0.001) # not zero in case agent got stuck\n",
    "\n",
    "# Configure best rainbow player 2\n",
    "best_rainbow_player2 = rainbow_policy(state_shape= (6, 7),\n",
    "                                      action_shape= (7,))\n",
    "best_rainbow_player2.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/9/rainbow_vs_rainbow_blocking_reward_complex_cnn/best_policy_agent2.pth\"))\n",
    "best_rainbow_player2.set_eps(0.001) # not zero in case agent got stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7d67e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games.\n",
      "Average steps of game:  11.351, std: 7.226465180155509\n",
      "Final mean reward agent 1: 9.03, std: 2.9595776725742473\n",
      "Final mean reward agent 2: 0.97, std: 2.9595776725742473\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# MLP DQN P1 TEST\n",
    "####################################################\n",
    "\n",
    "test_win_rate(numer_of_games=1000,\n",
    "              agent_player1= best_mlp_dqn_player1,\n",
    "              agent_player2= random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2493f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games.\n",
      "Average steps of game:  12.321, std: 5.362644776600441\n",
      "Final mean reward agent 1: 1.97, std: 3.977323220458704\n",
      "Final mean reward agent 2: 8.03, std: 3.9773232204587043\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# MLP DQN P2 TEST\n",
    "####################################################\n",
    "\n",
    "test_win_rate(numer_of_games=1000,\n",
    "              agent_player1= random_agent,\n",
    "              agent_player2= best_mlp_dqn_player2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c00bb6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games.\n",
      "Average steps of game:  11.37, std: 6.58825470060167\n",
      "Final mean reward agent 1: 8.98, std: 3.0264831075028327\n",
      "Final mean reward agent 2: 1.02, std: 3.0264831075028327\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CNN DQN P1 TEST\n",
    "####################################################\n",
    "\n",
    "test_win_rate(numer_of_games=1000,\n",
    "              agent_player1= best_cnn_dqn_player1,\n",
    "              agent_player2= random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8517602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games.\n",
      "Average steps of game:  11.156, std: 5.5983626177660195\n",
      "Final mean reward agent 1: 2.02, std: 4.014922166119787\n",
      "Final mean reward agent 2: 7.98, std: 4.014922166119787\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CNN DQN P2 TEST\n",
    "####################################################\n",
    "\n",
    "test_win_rate(numer_of_games=1000,\n",
    "              agent_player1= random_agent,\n",
    "              agent_player2= best_cnn_dqn_player2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7da8a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games.\n",
      "Average steps of game:  11.193, std: 5.6767729389152075\n",
      "Final mean reward agent 1: 9.49, std: 2.199977272609878\n",
      "Final mean reward agent 2: 0.5, std: 2.179449471770337\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# RAINBOW P1 TEST\n",
    "####################################################\n",
    "\n",
    "test_win_rate(numer_of_games=1000,\n",
    "              agent_player1= best_rainbow_player1,\n",
    "              agent_player2= random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d8949c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1000 games.\n",
      "Average steps of game:  11.812, std: 6.509120985202227\n",
      "Final mean reward agent 1: 0.84, std: 2.77387815161373\n",
      "Final mean reward agent 2: 9.16, std: 2.77387815161373\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# RAINBOW P2 TEST\n",
    "####################################################\n",
    "\n",
    "test_win_rate(numer_of_games=1000,\n",
    "              agent_player1= random_agent,\n",
    "              agent_player2= best_rainbow_player2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae87c85",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Plotting results\n",
    "\n",
    "- P1 MLP DQN vs random\n",
    "  - Score: 9.03, std: 2.9595776725742473\n",
    "  - Length: 11.351, std: 7.226465180155509\n",
    "- P2 MLP DQN vs random\n",
    "  - Score: 8.03, std: 3.9773232204587043\n",
    "  - Length: 12.321, std: 5.362644776600441\n",
    "- P1 CNN DQN vs random\n",
    "  - Score: 8.98, std: 3.0264831075028327\n",
    "  - Length: 11.37, std: 6.58825470060167\n",
    "- P2 CNN DQN vs random\n",
    "  - Score: 7.98, std: 4.014922166119787\n",
    "  - Length: 11.156, std: 5.5983626177660195\n",
    "- P1 rainbow vs random:\n",
    "  - Score: 9.49, std: 2.199977272609878\n",
    "  - Length: 11.193, std: 5.6767729389152075\n",
    "- P2 rainbow vs random\n",
    "  - Score: 9.16, std: 2.77387815161373\n",
    "  - Length: 11.812, std: 6.509120985202227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db16d376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "experiments = ['MLP DQN P1', 'MLP DQN P2', 'CNN DQN P1', 'CNN DQN P2', 'CNN Rainbow P1', 'CNN Rainbow P2']\n",
    "win_rates = [90.3, 80.3, 89.8, 79.8, 94.9, 91.6]\n",
    "ax.bar(experiments, win_rates)\n",
    "ax.set_ylim([0, 100])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Win rate for RL agent against random agent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa9e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
