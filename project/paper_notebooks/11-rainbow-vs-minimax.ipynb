{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# CNN based Rainbow vs minimax\n",
    "\n",
    "In the previous notebook, `10-rainbow-fixed-opponent.ipynb`, we performed an experiment where 2 rainbow agents fight against each otehr, freezing both agents alteratingly to simulate a league based system.\n",
    "This time around we train our rainbow agent against an increasingly smarter minimax agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training rainbow agent against frozen rainbow agent\n",
    "  - Building the environment\n",
    "  - Implementing the MiniMax policy\n",
    "  - Implementing the Rainbow policy\n",
    "  - Building agents\n",
    "  - Function for letting agents learn\n",
    "  - Function for watching learned agent\n",
    "  - Doing the experiment\n",
    "- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.21.0 recommended): 0.21.0\n",
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.12.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argparser\n",
    "import argparse\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.12.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# PPrint is a pretty print for variables\n",
    "from pprint import pprint\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym\n",
    "import minimax_agent.minimax_agent as minimaxbot\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(cfgym)\n",
    "importlib.reload(minimaxbot);\n",
    "\n",
    "# Time for allowing \"freezes\" in execution\n",
    "import time;\n",
    "\n",
    "# Allow for copying objects in a non reference manner\n",
    "import copy\n",
    "\n",
    "# Used for updating notebook display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7b6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# FUNCTION FOR LOADING IN TORCH DICTIONARIIES\n",
    "####################################################\n",
    "\n",
    "def load_torch_dict(filename):\n",
    "    \"\"\"\n",
    "    Loads in torch dictionary using correct cuda settings for current device\n",
    "    \"\"\"   \n",
    "    if torch.cuda.is_available():\n",
    "        return torch.load(filename)\n",
    "    else:\n",
    "        return torch.load(filename, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training rainbow agent against frozen rainbow agent\n",
    "\n",
    "Our connect four gym setup requires two agents, one for each player.\n",
    "To reduce complexity, agents will always play as the same player, e.g. always as player 1.\n",
    "It is important to note that connect four is a *solved game*.\n",
    "According to [The Washington Post](https://www.washingtonpost.com/news/wonk/wp/2015/05/08/how-to-win-any-popular-game-according-to-data-scientists/):\n",
    "\n",
    "> Connect Four is what mathematicians call a \"solved game,\" meaning you can play it perfectly every time, no matter what your opponent does. You will need to get the first move, but as long as you do so, you can always win within 41 moves.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Building the environment\n",
    "\n",
    "This code is identical to the notebook `9-rainbow.ipynb`, a reward for blocking moves is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict(action_mask:Box([0 0 0 0 0 0 0], [1 1 1 1 1 1 1], (7,), int8), observation:Box([[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]], [[2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]], (6, 7), int8))\n",
      "\n",
      "Action space: Discrete(7)\n",
      "\n",
      " Initial player id:\n",
      "player_1\n",
      "\n",
      " Initial observation:\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " Initial mask:\n",
      "[True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CONNECT FOUR V2 ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "def get_env():\n",
    "    \"\"\"\n",
    "    Returns the connect four gym environment V2 altered for Tianshou and Petting Zoo compatibility.\n",
    "    Already wrapped with a ts.env.PettingZooEnv wrapper.\n",
    "    \"\"\"\n",
    "    return ts.env.PettingZooEnv(cfgym.env(reward_move= 0, # Set to 1 for reward to make moves (incentivise longer games)\n",
    "                                          reward_blocking= 1, # Set to 1 for reward to make blocking moves (incentivise defensive games)\n",
    "                                          reward_invalid= -3,\n",
    "                                          reward_draw= 3,\n",
    "                                          reward_win= 5,\n",
    "                                          reward_loss= -5,\n",
    "                                          allow_invalid_move= False))\n",
    "    \n",
    "    \n",
    "# Test the environment\n",
    "env = get_env()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"\\n Initial player id:\")\n",
    "print(observation[\"agent_id\"])\n",
    "\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation[\"obs\"])\n",
    "\n",
    "print(\"\\n Initial mask:\")\n",
    "print(observation[\"mask\"])\n",
    "\n",
    "# Clean unused variables\n",
    "del observation\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2e15d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "Blocking move made by player 1: [1, 0]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# BLOCKING MOVE CHECK\n",
    "####################################################\n",
    "\n",
    "# Check if a reward is received for playing a blocking move\n",
    "\n",
    "env = get_env()\n",
    "env.reset()\n",
    "env.step(action= 0)\n",
    "print(env.rewards)\n",
    "env.step(action= 1)\n",
    "print(env.rewards)\n",
    "env.step(action= 0)\n",
    "print(env.rewards)\n",
    "env.step(action= 1)\n",
    "print(env.rewards)\n",
    "env.step(action= 0)\n",
    "print(env.rewards)\n",
    "env.step(action= 1)\n",
    "print(env.rewards)\n",
    "env.step(action= 1)\n",
    "print(f\"Blocking move made by player 1: {env.rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7e7da",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Implementing the MiniMax policy\n",
    "\n",
    "We provide the minimax algorithm as a Tianshou policy so that it can be used from within Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dca256",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CUSTOM MINIMAX TIANSHOU POLICY\n",
    "####################################################\n",
    "\n",
    "class TianshouMiniMaxConnectFourPolicy(ts.policy.BasePolicy):\n",
    "    \"\"\"\n",
    "    Tianshou compatible MiniMax policy for connect four.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 coin: int,\n",
    "                 oponent_coin: int,\n",
    "                 minimax_depth: int,\n",
    "                 column_count: int = 7,\n",
    "                 row_count: int = 6,\n",
    "                 **kwargs: typing.Any):\n",
    "        # Init base policy\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Configure minimax bot\n",
    "        self.bot = minimaxbot.MiniMaxConnectFourBot(coin= coin,\n",
    "                                                    oponent_coin= oponent_coin,\n",
    "                                                    column_count= column_count,\n",
    "                                                    row_count= row_count,\n",
    "                                                    minimax_depth= minimax_depth)\n",
    "\n",
    "    def forward(self,\n",
    "                batch: ts.data.Batch,\n",
    "                state: typing.Optional[typing.Union[dict, ts.data.Batch, np.ndarray]] = None,\n",
    "                **kwargs: typing.Any):\n",
    "        \"\"\"\n",
    "        Compute minimax action over the given batch data.\n",
    "        \"\"\"\n",
    "        boards = batch[\"obs\"]\n",
    "        \n",
    "        # Can be nested in Tianshou\n",
    "        while isinstance(boards, ts.data.Batch):\n",
    "            boards = boards[\"obs\"]\n",
    "        \n",
    "        preds = [None] * len(boards)        \n",
    "        \n",
    "        for i in range(len(boards)):\n",
    "            preds[i] = self.bot.predict(board= boards[i])\n",
    "            \n",
    "        \n",
    "        return ts.data.Batch(act=preds, state=state)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        # No learning needed\n",
    "        return {}\n",
    "    \n",
    "    def set_eps(self, eps):\n",
    "        # Not needed\n",
    "        return\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93544d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Implementing the Rainbow policy\n",
    "\n",
    "This code is identical to the notebook `9-rainbow.ipynb`, the defaults are changed so that they reflect the best found parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2f43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN ARCHITECTURE\n",
    "####################################################\n",
    "\n",
    "class CNNForRainbow(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN to be used as baseclass for the Rainbow algorithm.\n",
    "    Extracts \"feautures\" for the Rainbow algorithm by doing a 4x4 cnn kernel pass and providing 64 filters for each mask.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \n",
    "        # Torch init\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store device to be used\n",
    "        self.device = device\n",
    "        \n",
    "        # The input layer is singular -> we have 1 board vector\n",
    "        input_channels_cnn = 1\n",
    "        \n",
    "        # We output 64 filters per kernel \n",
    "        output_channels_cnn = 64 # Updated from previous 16\n",
    "        \n",
    "        # We store the output dimension of the CNN \"feature\" layer\n",
    "        self.output_dim = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "                state: typing.Optional[typing.Any] = None,\n",
    "                info: typing.Dict[str, typing.Any] = {}):\n",
    "        # Make a torch instance (from regular vector of board)\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "            \n",
    "        # Tianshou bugs the batch output, reshape to work properly with our torch version\n",
    "        if (len(np.shape(obs)) != 4):\n",
    "            obs = obs[:, None, :, :]\n",
    "        \n",
    "        # Return what is needed (network output & state)\n",
    "        return self.net(obs), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b137ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW ARCHITECTURE\n",
    "####################################################\n",
    "\n",
    "class Rainbow(CNNForRainbow):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow algorithm making using of the CNNForRainbow baseclass.\n",
    "    Default parameters adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                 num_atoms: int = 51,\n",
    "                 is_noisy: bool = True,\n",
    "                 noisy_std: float = 0.1,\n",
    "                 is_dueling: bool = True):\n",
    "        \n",
    "        # Init CNN feature extraction parent class\n",
    "        super().__init__(state_shape= state_shape, device= device)\n",
    "        \n",
    "        # the amount of actions we have is just the action shape\n",
    "        self.action_num = np.prod(action_shape)\n",
    "        \n",
    "        # Store class specific info\n",
    "        self.num_atoms = num_atoms\n",
    "        self._is_dueling = is_dueling\n",
    "\n",
    "        # Our linear layer depends on wether or not we want to use a noisy environment\n",
    "        # Noisy implementation based on https://arxiv.org/abs/1706.10295\n",
    "        def linear(x, y):\n",
    "            if is_noisy:\n",
    "                return ts.utils.net.discrete.NoisyLinear(x, y, noisy_std)\n",
    "            else:\n",
    "                return torch.nn.Linear(x, y)\n",
    "            \n",
    "        # Specify Q and V based on wether or not agent is dueling\n",
    "        # Setting agent on dueling mode should help generalisation according to rainbow paper\n",
    "        # NOTE: this uses the output dim from the feature extraction CNN\n",
    "        self.Q = torch.nn.Sequential(\n",
    "            linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "            linear(512, self.action_num * self.num_atoms))\n",
    "        \n",
    "        if self._is_dueling:\n",
    "            self.V = torch.nn.Sequential(\n",
    "                linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                linear(512, self.num_atoms))\n",
    "            \n",
    "        # New output dim for this rainbow network\n",
    "        self.output_dim = self.action_num * self.num_atoms\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "                state: typing.Optional[typing.Any] = None,\n",
    "                info: typing.Dict[str, typing.Any] = {}):\n",
    "        \n",
    "        # Use our parent CNN based network to get \"features\"\n",
    "        obs, state = super().forward(obs)\n",
    "        \n",
    "        # Get our Rainbow specific values\n",
    "        q = self.Q(obs)\n",
    "        q = q.view(-1, self.action_num, self.num_atoms)\n",
    "        \n",
    "        if self._is_dueling:\n",
    "            v = self.V(obs)\n",
    "            v = v.view(-1, 1, self.num_atoms)\n",
    "            logits = q - q.mean(dim=1, keepdim=True) + v\n",
    "        else:\n",
    "            logits = q\n",
    "        \n",
    "        # We need to go from our logits to an accepted dimension of probability outputs\n",
    "        probs = logits.softmax(dim=2)\n",
    "        \n",
    "        return probs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539b56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW POLICY\n",
    "####################################################\n",
    "\n",
    "def rainbow_policy(state_shape: tuple,\n",
    "                   action_shape: tuple,\n",
    "                   optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                   learning_rate: float =  0.0001, # Increased from 0000625\n",
    "                   gamma: float = 0.8, # Decreased from 0.9\n",
    "                   n_step: int = 3,\n",
    "                   num_atoms: int = 51,\n",
    "                   is_noisy: bool = True,\n",
    "                   noisy_std: float = 0.1,\n",
    "                   is_dueling: bool = True,\n",
    "                   frozen: bool = False, # Added to freeze an agent\n",
    "                   target_update_freq: int = 500):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow policy.\n",
    "    Default parameters adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Rainbow network to be used by policy\n",
    "    net = Rainbow(state_shape= state_shape,\n",
    "                  action_shape= action_shape,\n",
    "                  device= device,\n",
    "                  num_atoms= num_atoms,\n",
    "                  is_noisy= is_noisy,\n",
    "                  noisy_std= noisy_std,\n",
    "                  is_dueling= is_dueling).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # If we are frozen, we use an optimizer that has learning rate 0\n",
    "    if frozen:\n",
    "        optim = torch.optim.SGD(net.parameters(), lr= 0)\n",
    "        \n",
    "    # Our agents Rainbow policy\n",
    "    return ts.policy.RainbowPolicy(model= net,\n",
    "                                   optim= optim,\n",
    "                                   discount_factor= gamma,\n",
    "                                   num_atoms= num_atoms,\n",
    "                                   estimation_step= n_step,\n",
    "                                   target_update_freq= target_update_freq).to(device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd411318",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Building agents\n",
    "\n",
    "This code is identical to the notebook `9-rainbow.ipynb`, with the added option of \"freezing\" an agent which corresponds to giving it an optimizer with learning rate 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ae27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT CREATION\n",
    "####################################################\n",
    "\n",
    "def get_agent_manager(agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                      agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                      agent_player1_frozen: bool = False, # Freeze a player -> don't let it learn further\n",
    "                      agent_player2_frozen: bool = False,\n",
    "                      optim: typing.Optional[torch.optim.Optimizer] = None):\n",
    "    \"\"\"\n",
    "    Gets a multi agent policy manager, optimizer and player ids for the connect four V2 gym environment.\n",
    "    Per default this returns \n",
    "        - Multi agent manager for 2 agents using Rainbow\n",
    "        - Adam optimizer\n",
    "        - ['player_1', 'player_2'] from the connect four environment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the environment to play in (Connect four gym V2)\n",
    "    env = get_env()\n",
    "    \n",
    "    # Get the observation space from the environment, depending on typo of space (ternary operator)\n",
    "    observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "    \n",
    "    # Set the arguments\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    \n",
    "    # Configure agent player 1 to be a Rainbow if no policy is passed.\n",
    "    if agent_player1 is None:\n",
    "        # Our agent1 uses a Rainbow policy\n",
    "        agent_player1 = rainbow_policy(state_shape= state_shape,\n",
    "                                       action_shape= action_shape,\n",
    "                                       optim= optim,\n",
    "                                       frozen= agent_player1_frozen)\n",
    "    \n",
    "    # Configure agent player 2 to be a Rainbow if no policy is passed.\n",
    "    if agent_player2 is None:\n",
    "        # Our agent1 uses a Rainbow policy\n",
    "        agent_player2 = rainbow_policy(state_shape= state_shape,\n",
    "                                       action_shape= action_shape,\n",
    "                                       optim= optim,\n",
    "                                       frozen= agent_player2_frozen)\n",
    "\n",
    "    # Default order of the agents\n",
    "    agents = [agent_player1, agent_player2]\n",
    "        \n",
    "    # Create the multi agent policy\n",
    "    policy = ts.policy.MultiAgentPolicyManager(agents, env)\n",
    "    \n",
    "    # Return our policy, optimizer and the available agents in the environment\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using Rainbow\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    \n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8f3a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for letting agents learn\n",
    "\n",
    "This code is identical to the notebook `9-rainbow.ipynb`, but a stopping condition is added and the defaults are updated to the newly found best, the reward metric is also updated to relfect the score of the non frozen agent.\n",
    "The testing strategy is also updated to be on one environment using 10 trials.\n",
    "We also decay the epsilon faster and don't use epsilon decay on the frozen agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a32c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# AGENT TRAINING\n",
    "####################################################\n",
    "\n",
    "def train_agent(filename: str,\n",
    "                agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "                agent_player1_frozen: bool = False, # Freeze a player -> don't let it learn further\n",
    "                agent_player2_frozen: bool = False,\n",
    "                single_agent_score_as_reward: bool= False, # Uses non frozen agent's score as reward\n",
    "                optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                training_env_num: int = 10,\n",
    "                testing_env_num: int = 10,\n",
    "                episode_per_test: int = 10,\n",
    "                stopping_threshold: float = 7,\n",
    "                buffer_size: int = 10000, # Default 100000\n",
    "                batch_size: int = 64, # Default 32\n",
    "                epochs: int = 500, # Default 50\n",
    "                step_per_epoch: int = 10000,\n",
    "                step_per_collect: int = 10, # Should be multiple of the test/training envs\n",
    "                update_per_step: float = 0.1,\n",
    "                testing_eps: float = 0.005,\n",
    "                training_eps_init: float = 1,\n",
    "                training_eps_final: float = 0.2): # Default 0.05\n",
    "    \"\"\"\n",
    "    Trains two agents in the connect four V2 environment and saves their best model and logs.\n",
    "    Returns:\n",
    "        - result from offpolicy_trainer\n",
    "        - final version of agent 1\n",
    "        - final version of agent 2\n",
    "    Defaults adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "\n",
    "    # ======== notebook specific =========\n",
    "    notebook_version = '11' # Used for foldering logs and models\n",
    "\n",
    "    # ======== environment setup =========\n",
    "    train_envs = ts.env.DummyVectorEnv([get_env for _ in range(training_env_num)])\n",
    "    test_envs = ts.env.DummyVectorEnv([get_env for _ in range(testing_env_num)])\n",
    "    \n",
    "    # set the seed for reproducibility\n",
    "    np.random.seed(1998)\n",
    "    torch.manual_seed(1998)\n",
    "    train_envs.seed(1998)\n",
    "    test_envs.seed(1998)\n",
    "\n",
    "    # ======== agent setup =========\n",
    "    # Gets our agents from the previously made function\n",
    "    # Per default: \n",
    "    #   - Multi agent manager for 2 agents using Rainbow\n",
    "    #   - Adam optimizer\n",
    "    #   - ['player_1', 'player_2'] from the connect four environment\n",
    "    policy, optim, agents = get_agent_manager(agent_player1=agent_player1,\n",
    "                                              agent_player2=agent_player2,\n",
    "                                              agent_player1_frozen= agent_player1_frozen,\n",
    "                                              agent_player2_frozen= agent_player2_frozen,\n",
    "                                              optim=optim)\n",
    "\n",
    "    # ======== collector setup =========\n",
    "    # Make a collector for the training environments\n",
    "    buffer= ts.data.VectorReplayBuffer(total_size= buffer_size,\n",
    "                                       buffer_num=len(train_envs))\n",
    "    \n",
    "    train_collector = ts.data.Collector(policy= policy,\n",
    "                                        env= train_envs,\n",
    "                                        buffer= buffer,\n",
    "                                        exploration_noise= True)\n",
    "    \n",
    "    # Make a collector for the testing environments\n",
    "    test_collector = ts.data.Collector(policy= policy,\n",
    "                                       env= test_envs,\n",
    "                                       exploration_noise= True)\n",
    "    \n",
    "    # ======== ensure folders exist =========\n",
    "    if not os.path.exists(os.path.join('./logs', 'paper_notebooks', notebook_version, filename)):\n",
    "        os.makedirs(os.path.join('./logs', 'paper_notebooks', notebook_version, filename))\n",
    "    if not os.path.exists(os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename)):\n",
    "        os.makedirs(os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename))\n",
    "\n",
    "    # ======== tensorboard logging setup =========\n",
    "    # Allows to save the training progress to tensorboard compatable logs\n",
    "    log_path = os.path.join('./logs', 'paper_notebooks', notebook_version, filename)\n",
    "    writer = torch.utils.tensorboard.SummaryWriter(log_path)\n",
    "    logger = ts.utils.TensorboardLogger(writer)\n",
    "\n",
    "    # ======== callback functions used during training =========\n",
    "    # We want to save our best policy\n",
    "    def save_best_fn(policy):\n",
    "        \"\"\"\n",
    "        Callback to save the best model\n",
    "        \"\"\"\n",
    "        # Save best agent 1\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'best_policy_agent1.pth')\n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save best agent 2\n",
    "        model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'best_policy_agent2.pth')\n",
    "        torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "        \n",
    "        # Save agent2\n",
    "\n",
    "    def stop_fn(average_rews):\n",
    "        \"\"\"\n",
    "        Callback to stop training when we've reached the desired reward.\n",
    "        Reward is the test average return value of the reward_metric function.\n",
    "        \"\"\"\n",
    "        if single_agent_score_as_reward:\n",
    "            # Get singular episode mean reward\n",
    "            episode_reward= average_rews / episode_per_test\n",
    "            stop= episode_reward >= stopping_threshold\n",
    "            print(f\"testing for stop: {episode_reward} >= {stopping_threshold} -> {stop}\")\n",
    "            # Agent is seen as \"trained enough\"\n",
    "            return stop\n",
    "        else:\n",
    "            return False # Not implemented\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback before training, sets the training epsilon in a decaying manner.\n",
    "        Adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "        \"\"\"        \n",
    "        # Nature DQN setting to have a \"linear decaying epsilon\" for the first 50 thousand iterations\n",
    "        if env_step <= 50000:\n",
    "            training_eps = training_eps_init - env_step / 1000000 * (training_eps_init - training_eps_final)\n",
    "        else:\n",
    "            training_eps = training_eps_final\n",
    "            \n",
    "            \n",
    "        # Set epsilon\n",
    "        policy.policies[agents[0]].set_eps(training_eps)\n",
    "        policy.policies[agents[1]].set_eps(training_eps)\n",
    "        \n",
    "        # If frozen we don't have a large epsilon\n",
    "        if agent_player1_frozen:\n",
    "            policy.policies[agents[0]].set_eps(training_eps_final)\n",
    "        if agent_player2_frozen:\n",
    "            policy.policies[agents[1]].set_eps(training_eps_final)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        \"\"\"\n",
    "        Callback beore testing, sets the testing epsilon.\n",
    "        \"\"\"        \n",
    "        # Before testing we want to configure the epsilon for the agents\n",
    "        # In general more greedy than the train case but not\n",
    "        #   to avoid getting stuck on invalid moves\n",
    "        policy.policies[agents[0]].set_eps(testing_eps)\n",
    "        policy.policies[agents[1]].set_eps(testing_eps)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        \"\"\"\n",
    "        Callback for reward collection.\n",
    "        Currently the reward is the sum of both agents.\n",
    "        \"\"\"        \n",
    "        if agent_player2_frozen and single_agent_score_as_reward:\n",
    "            # agent 2 frozen, optimizing for agent 1\n",
    "            return rews[:, 0]\n",
    "        \n",
    "        if agent_player1_frozen and single_agent_score_as_reward:\n",
    "            # agent 1 frozen, optimizing for agent 2\n",
    "            return rews[:, 1]\n",
    "        \n",
    "        # Per default we are interested in optimizing both agents\n",
    "        return rews[:, 0] + rews[:, 1]\n",
    "\n",
    "    # ======== Training =========\n",
    "    # off policy training\n",
    "    result = ts.trainer.offpolicy_trainer(policy= policy,\n",
    "                                          train_collector= train_collector,\n",
    "                                          test_collector= test_collector,\n",
    "                                          max_epoch= epochs,\n",
    "                                          step_per_epoch= step_per_epoch,\n",
    "                                          step_per_collect= step_per_collect,\n",
    "                                          episode_per_test= episode_per_test,\n",
    "                                          batch_size= batch_size,\n",
    "                                          train_fn= train_fn,\n",
    "                                          test_fn= test_fn,\n",
    "                                          stop_fn= stop_fn,\n",
    "                                          save_best_fn= save_best_fn,\n",
    "                                          update_per_step= update_per_step,\n",
    "                                          logger= logger,\n",
    "                                          test_in_train= False,\n",
    "                                          reward_metric= reward_metric)\n",
    "    \n",
    "    # Save final agent 1\n",
    "    model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'final_policy_agent1.pth')\n",
    "    torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "\n",
    "    # Save final agent 2\n",
    "    model_save_path = os.path.join('./saved_variables', 'paper_notebooks', notebook_version, filename, 'final_policy_agent2.pth')\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "    return result, policy.policies[agents[0]], policy.policies[agents[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161cc0f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function for watching learned agent\n",
    "\n",
    "Identical to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "459114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# WATCHING THE LEARNED POLICY IN ACTION\n",
    "####################################################\n",
    "\n",
    "def watch(numer_of_games: int = 3,\n",
    "          agent_player1: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          agent_player2: typing.Optional[ts.policy.BasePolicy] = None,\n",
    "          test_epsilon: float = 0.005, # For the watching we act completely greedy but low random for not getting stuck on invalid move\n",
    "          render_speed: float = 0.15, # Amount of seconds to update frame/ do a step\n",
    "          ) -> None:\n",
    "    \n",
    "    # Get the connect four V2 environment (must be a list)\n",
    "    env= ts.env.DummyVectorEnv([get_env])\n",
    "    \n",
    "    # Get the agents from the trained agents\n",
    "    policy, optim, agents = get_agent_manager(agent_player1= agent_player1,\n",
    "                                              agent_player2= agent_player2)\n",
    "    \n",
    "    # Evaluate the policy\n",
    "    policy.eval()\n",
    "    \n",
    "    # Set the testing policy epsilon for our agents\n",
    "    policy.policies[agents[0]].set_eps(test_epsilon)\n",
    "    policy.policies[agents[1]].set_eps(test_epsilon)\n",
    "    \n",
    "    # Collect the test data\n",
    "    collector = ts.data.Collector(policy= policy,\n",
    "                                  env= env,\n",
    "                                  exploration_noise= True)\n",
    "    \n",
    "    # Render games in human mode to see how it plays\n",
    "    result = collector.collect(n_episode= numer_of_games, render= render_speed)\n",
    "    \n",
    "    # Close the environment aftering collecting the results\n",
    "    # This closes the pygame window after completion\n",
    "    env.close()\n",
    "    \n",
    "    # Get the rewards and length from the test trials\n",
    "    rewards, length = result[\"rews\"], result[\"lens\"]\n",
    "    \n",
    "    # Print the final reward for the first agent\n",
    "    print(f\"Average steps of game:  {length.mean()}\")\n",
    "    print(f\"Final mean reward agent 1: {rewards[:, 0].mean()}, std: {rewards[:, 0].std()}\")\n",
    "    print(f\"Final mean reward agent 2: {rewards[:, 1].mean()}, std: {rewards[:, 1].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31784e9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Doing the experiment\n",
    "\n",
    "To test if we can better train our agents when playing against the minimax agent, we play against it in increasing minimax depth as to simulate increasing difficulty.\n",
    "\n",
    "1. We play as player 1 so that we could potentially learn the complete connect four game. We changed the epsilon values as well as to not loose to much info either. The stopping criteria is 10.\n",
    "\n",
    "| **MiniMax depth** | **Test score**         | **Epoch** |\n",
    "|-------------------|------------------------|-----------|\n",
    "| 1                 | 108.200000 ± 2.400000  | 182       |\n",
    "| 2                 | 107.000000 ± 0.000000  | 45        |\n",
    "| 3                 | 113.000000 ± 0.000000  | 11        |\n",
    "| 4                 | 104.800000 ± 36.600000 | 6         |\n",
    "| 5                 | 109                    | 6         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93fe8961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started training agent player 1 against minimax with depth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:34, 287.83it/s, env_step=10000, len=14, n/ep=0, n/st=10, player_1/loss=1.533, rew=3.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 11.000000 ± 0.000000, best_reward: 11.000000 ± 0.000000 in #1\n",
      "testing for stop: 1.1 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [00:34, 291.84it/s, env_step=20000, len=22, n/ep=0, n/st=10, player_1/loss=1.495, rew=27.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 11.000000 ± 0.000000, best_reward: 11.000000 ± 0.000000 in #1\n",
      "testing for stop: 1.1 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [00:34, 293.67it/s, env_step=30000, len=10, n/ep=1, n/st=10, player_1/loss=1.423, rew=-5.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 55.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [00:34, 292.16it/s, env_step=40000, len=20, n/ep=1, n/st=10, player_1/loss=1.366, rew=17.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 29.100000 ± 5.700000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [00:31, 321.79it/s, env_step=50000, len=10, n/ep=1, n/st=10, player_1/loss=1.330, rew=-5.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 5.800000 ± 2.400000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [00:33, 300.82it/s, env_step=60000, len=32, n/ep=1, n/st=10, player_1/loss=1.113, rew=19.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 12.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10001it [00:34, 291.91it/s, env_step=70000, len=18, n/ep=1, n/st=10, player_1/loss=1.134, rew=3.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 5.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10001it [00:34, 292.29it/s, env_step=80000, len=18, n/ep=1, n/st=10, player_1/loss=1.092, rew=3.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 10.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10001it [00:33, 294.18it/s, env_step=90000, len=18, n/ep=0, n/st=10, player_1/loss=1.067, rew=3.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 4.600000 ± 4.800000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10001it [00:34, 290.05it/s, env_step=100000, len=16, n/ep=1, n/st=10, player_1/loss=1.084, rew=-5.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 10.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 10001it [00:34, 289.53it/s, env_step=110000, len=13, n/ep=0, n/st=10, player_1/loss=1.084, rew=12.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 10.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 10001it [00:34, 289.61it/s, env_step=120000, len=18, n/ep=2, n/st=10, player_1/loss=1.066, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 2.400000 ± 1.800000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 10001it [00:34, 289.47it/s, env_step=130000, len=18, n/ep=0, n/st=10, player_1/loss=1.000, rew=1.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 3.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 10001it [00:34, 293.01it/s, env_step=140000, len=22, n/ep=1, n/st=10, player_1/loss=0.994, rew=7.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 10.000000 ± 0.000000, best_reward: 55.000000 ± 0.000000 in #3\n",
      "testing for stop: 5.5 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 10001it [00:33, 294.20it/s, env_step=150000, len=23, n/ep=0, n/st=10, player_1/loss=0.972, rew=23.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 92.200000 ± 32.400000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 10001it [00:33, 297.19it/s, env_step=160000, len=17, n/ep=1, n/st=10, player_1/loss=0.912, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 3.000000 ± 0.000000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 10001it [00:33, 295.75it/s, env_step=170000, len=19, n/ep=2, n/st=10, player_1/loss=0.946, rew=2.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 3.000000 ± 0.000000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 10001it [00:33, 295.96it/s, env_step=180000, len=30, n/ep=2, n/st=10, player_1/loss=0.917, rew=68.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 3.000000 ± 0.000000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 10001it [00:33, 295.37it/s, env_step=190000, len=15, n/ep=0, n/st=10, player_1/loss=0.972, rew=7.50]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 3.000000 ± 0.000000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 10001it [00:33, 294.73it/s, env_step=200000, len=18, n/ep=1, n/st=10, player_1/loss=0.983, rew=9.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 4.100000 ± 3.300000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 10001it [00:34, 293.39it/s, env_step=210000, len=19, n/ep=1, n/st=10, player_1/loss=0.971, rew=14.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: 4.000000 ± 3.000000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 10001it [00:33, 294.84it/s, env_step=220000, len=14, n/ep=1, n/st=10, player_1/loss=0.950, rew=-5.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: 4.000000 ± 3.000000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 10001it [00:33, 297.57it/s, env_step=230000, len=28, n/ep=1, n/st=10, player_1/loss=0.882, rew=25.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: 5.200000 ± 6.600000, best_reward: 92.200000 ± 32.400000 in #15\n",
      "testing for stop: 9.22 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 10001it [00:33, 299.03it/s, env_step=240000, len=35, n/ep=2, n/st=10, player_1/loss=0.986, rew=52.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: 97.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 10001it [00:33, 300.12it/s, env_step=250000, len=18, n/ep=0, n/st=10, player_1/loss=0.910, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 10001it [00:33, 298.01it/s, env_step=260000, len=18, n/ep=0, n/st=10, player_1/loss=0.913, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 10001it [00:33, 295.32it/s, env_step=270000, len=18, n/ep=0, n/st=10, player_1/loss=0.944, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 10001it [00:34, 293.57it/s, env_step=280000, len=12, n/ep=1, n/st=10, player_1/loss=0.951, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 10001it [00:33, 301.87it/s, env_step=290000, len=19, n/ep=1, n/st=10, player_1/loss=0.837, rew=25.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: 63.600000 ± 18.374983, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 10001it [00:33, 294.83it/s, env_step=300000, len=18, n/ep=1, n/st=10, player_1/loss=0.895, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 10001it [00:33, 295.83it/s, env_step=310000, len=22, n/ep=1, n/st=10, player_1/loss=0.974, rew=17.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 10001it [00:33, 298.64it/s, env_step=320000, len=34, n/ep=0, n/st=10, player_1/loss=0.893, rew=47.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: 14.000000 ± 33.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 10001it [00:33, 297.02it/s, env_step=330000, len=17, n/ep=0, n/st=10, player_1/loss=0.886, rew=5.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: 11.500000 ± 25.500000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 10001it [00:33, 298.30it/s, env_step=340000, len=18, n/ep=0, n/st=10, player_1/loss=0.830, rew=11.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 10001it [00:33, 298.88it/s, env_step=350000, len=18, n/ep=1, n/st=10, player_1/loss=0.804, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 10001it [00:33, 295.59it/s, env_step=360000, len=18, n/ep=1, n/st=10, player_1/loss=0.870, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: 7.800000 ± 14.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 10001it [00:33, 295.39it/s, env_step=370000, len=18, n/ep=1, n/st=10, player_1/loss=0.870, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 10001it [00:33, 296.29it/s, env_step=380000, len=18, n/ep=0, n/st=10, player_1/loss=0.870, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 10001it [00:33, 300.14it/s, env_step=390000, len=18, n/ep=1, n/st=10, player_1/loss=1.019, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 10001it [00:34, 291.34it/s, env_step=400000, len=19, n/ep=1, n/st=10, player_1/loss=0.925, rew=5.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: 2.200000 ± 2.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 10001it [00:33, 294.66it/s, env_step=410000, len=29, n/ep=0, n/st=10, player_1/loss=0.824, rew=43.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 10001it [00:33, 298.53it/s, env_step=420000, len=16, n/ep=0, n/st=10, player_1/loss=0.818, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 10001it [00:32, 303.19it/s, env_step=430000, len=20, n/ep=0, n/st=10, player_1/loss=0.682, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 10001it [00:33, 299.35it/s, env_step=440000, len=32, n/ep=1, n/st=10, player_1/loss=0.780, rew=43.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: 29.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 10001it [00:33, 297.35it/s, env_step=450000, len=28, n/ep=0, n/st=10, player_1/loss=0.805, rew=29.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: 34.400000 ± 16.200000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 10001it [00:33, 300.73it/s, env_step=460000, len=18, n/ep=0, n/st=10, player_1/loss=0.767, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 10001it [00:32, 304.86it/s, env_step=470000, len=28, n/ep=0, n/st=10, player_1/loss=0.672, rew=27.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: 68.800000 ± 29.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 10001it [00:33, 302.43it/s, env_step=480000, len=18, n/ep=0, n/st=10, player_1/loss=0.780, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 10001it [00:32, 309.25it/s, env_step=490000, len=26, n/ep=0, n/st=10, player_1/loss=0.603, rew=19.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 10001it [00:32, 305.53it/s, env_step=500000, len=40, n/ep=0, n/st=10, player_1/loss=0.714, rew=83.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50: test_reward: 45.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 10001it [00:32, 308.88it/s, env_step=510000, len=40, n/ep=0, n/st=10, player_1/loss=0.622, rew=83.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: 68.700000 ± 28.642800, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 10001it [00:32, 304.04it/s, env_step=520000, len=23, n/ep=2, n/st=10, player_1/loss=0.787, rew=24.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #52: test_reward: 70.200000 ± 25.771302, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 10001it [00:33, 303.04it/s, env_step=530000, len=24, n/ep=2, n/st=10, player_1/loss=0.786, rew=24.50]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: 5.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54: 10001it [00:33, 302.64it/s, env_step=540000, len=16, n/ep=0, n/st=10, player_1/loss=0.818, rew=4.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #54: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #55: 10001it [00:32, 305.18it/s, env_step=550000, len=34, n/ep=0, n/st=10, player_1/loss=0.673, rew=53.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55: test_reward: 83.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #56: 10001it [00:32, 307.37it/s, env_step=560000, len=32, n/ep=0, n/st=10, player_1/loss=0.645, rew=43.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #56: test_reward: 77.700000 ± 15.900000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #57: 10001it [00:32, 307.58it/s, env_step=570000, len=40, n/ep=0, n/st=10, player_1/loss=0.621, rew=83.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #57: test_reward: 77.800000 ± 15.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #58: 10001it [00:32, 307.65it/s, env_step=580000, len=31, n/ep=2, n/st=10, player_1/loss=0.626, rew=53.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #58: test_reward: 4.400000 ± 3.583295, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #59: 10001it [00:33, 302.96it/s, env_step=590000, len=18, n/ep=0, n/st=10, player_1/loss=0.714, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #59: test_reward: 3.500000 ± 1.500000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #60: 10001it [00:33, 295.68it/s, env_step=600000, len=15, n/ep=0, n/st=10, player_1/loss=0.854, rew=15.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #60: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #61: 10001it [00:33, 294.72it/s, env_step=610000, len=18, n/ep=0, n/st=10, player_1/loss=0.886, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #61: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #62: 10001it [00:33, 297.17it/s, env_step=620000, len=18, n/ep=1, n/st=10, player_1/loss=0.917, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #62: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #63: 10001it [00:30, 328.60it/s, env_step=630000, len=26, n/ep=2, n/st=10, player_1/loss=0.828, rew=26.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #63: test_reward: 74.800000 ± 24.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #64: 10001it [00:30, 332.49it/s, env_step=640000, len=40, n/ep=1, n/st=10, player_1/loss=0.831, rew=83.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #64: test_reward: 83.000000 ± 6.260990, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #65: 10001it [00:29, 344.31it/s, env_step=650000, len=29, n/ep=0, n/st=10, player_1/loss=0.581, rew=46.33]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #65: test_reward: 83.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #66: 10001it [00:31, 315.89it/s, env_step=660000, len=36, n/ep=0, n/st=10, player_1/loss=0.599, rew=61.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #66: test_reward: 84.400000 ± 4.200000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #67: 10001it [00:33, 298.72it/s, env_step=670000, len=16, n/ep=0, n/st=10, player_1/loss=0.694, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #67: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #68: 10001it [00:33, 298.34it/s, env_step=680000, len=20, n/ep=0, n/st=10, player_1/loss=0.769, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #68: test_reward: 31.800000 ± 9.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #69: 10001it [00:33, 298.17it/s, env_step=690000, len=18, n/ep=0, n/st=10, player_1/loss=0.752, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #69: test_reward: 33.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #70: 10001it [00:33, 296.87it/s, env_step=700000, len=20, n/ep=0, n/st=10, player_1/loss=0.787, rew=11.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #70: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #71: 10001it [00:34, 294.06it/s, env_step=710000, len=18, n/ep=0, n/st=10, player_1/loss=0.888, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #71: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #72: 10001it [00:33, 297.66it/s, env_step=720000, len=29, n/ep=0, n/st=10, player_1/loss=0.760, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #72: test_reward: 31.800000 ± 9.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #73: 10001it [00:33, 298.96it/s, env_step=730000, len=27, n/ep=0, n/st=10, player_1/loss=0.857, rew=31.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #73: test_reward: 29.200000 ± 5.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #74: 10001it [00:33, 299.47it/s, env_step=740000, len=27, n/ep=0, n/st=10, player_1/loss=0.856, rew=32.33]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #74: test_reward: 10.200000 ± 21.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #75: 10001it [00:33, 295.73it/s, env_step=750000, len=18, n/ep=0, n/st=10, player_1/loss=0.981, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #75: test_reward: 3.200000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #76: 10001it [00:34, 293.29it/s, env_step=760000, len=36, n/ep=1, n/st=10, player_1/loss=0.973, rew=69.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #76: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #77: 10001it [00:34, 292.68it/s, env_step=770000, len=18, n/ep=0, n/st=10, player_1/loss=0.975, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #77: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #78: 10001it [00:33, 294.74it/s, env_step=780000, len=18, n/ep=0, n/st=10, player_1/loss=0.915, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #78: test_reward: 31.800000 ± 9.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #79: 10001it [00:33, 295.91it/s, env_step=790000, len=27, n/ep=1, n/st=10, player_1/loss=0.812, rew=31.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #79: test_reward: 10.600000 ± 20.274121, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #80: 10001it [00:33, 295.02it/s, env_step=800000, len=18, n/ep=0, n/st=10, player_1/loss=0.815, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #80: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #81: 10001it [00:33, 298.21it/s, env_step=810000, len=29, n/ep=0, n/st=10, player_1/loss=0.772, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #81: test_reward: 34.200000 ± 2.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #82: 10001it [00:33, 297.12it/s, env_step=820000, len=29, n/ep=1, n/st=10, player_1/loss=0.812, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #82: test_reward: 3.200000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #83: 10001it [00:33, 298.07it/s, env_step=830000, len=16, n/ep=1, n/st=10, player_1/loss=0.849, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #83: test_reward: 31.000000 ± 6.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #84: 10001it [00:33, 297.04it/s, env_step=840000, len=30, n/ep=0, n/st=10, player_1/loss=0.816, rew=17.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #84: test_reward: 9.200000 ± 5.173007, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #85: 10001it [00:33, 298.47it/s, env_step=850000, len=30, n/ep=1, n/st=10, player_1/loss=0.932, rew=15.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #85: test_reward: 9.700000 ± 0.900000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #86: 10001it [00:33, 297.41it/s, env_step=860000, len=17, n/ep=0, n/st=10, player_1/loss=0.805, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #86: test_reward: 10.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #87: 10001it [00:33, 299.93it/s, env_step=870000, len=17, n/ep=1, n/st=10, player_1/loss=0.744, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #87: test_reward: 31.200000 ± 11.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #88: 10001it [00:33, 300.77it/s, env_step=880000, len=27, n/ep=1, n/st=10, player_1/loss=0.704, rew=31.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #88: test_reward: 35.300000 ± 0.900000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #89: 10001it [00:33, 298.84it/s, env_step=890000, len=17, n/ep=0, n/st=10, player_1/loss=0.816, rew=10.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #89: test_reward: 10.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #90: 10001it [00:33, 299.73it/s, env_step=900000, len=30, n/ep=0, n/st=10, player_1/loss=0.782, rew=33.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #90: test_reward: 32.000000 ± 3.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #91: 10001it [00:33, 303.00it/s, env_step=910000, len=36, n/ep=0, n/st=10, player_1/loss=0.835, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #91: test_reward: 79.200000 ± 11.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #92: 10001it [00:33, 299.96it/s, env_step=920000, len=34, n/ep=0, n/st=10, player_1/loss=0.944, rew=23.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #92: test_reward: 60.800000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #93: 10001it [00:33, 297.97it/s, env_step=930000, len=19, n/ep=2, n/st=10, player_1/loss=0.985, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #93: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #94: 10001it [00:34, 292.81it/s, env_step=940000, len=16, n/ep=0, n/st=10, player_1/loss=0.928, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #94: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #95: 10001it [00:34, 293.31it/s, env_step=950000, len=16, n/ep=1, n/st=10, player_1/loss=0.866, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #95: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #96: 10001it [00:34, 293.36it/s, env_step=960000, len=24, n/ep=0, n/st=10, player_1/loss=0.804, rew=38.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #96: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #97: 10001it [00:33, 299.65it/s, env_step=970000, len=16, n/ep=1, n/st=10, player_1/loss=0.900, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #97: test_reward: 33.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #98: 10001it [00:32, 307.10it/s, env_step=980000, len=28, n/ep=1, n/st=10, player_1/loss=0.911, rew=19.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #98: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #99: 10001it [00:32, 308.92it/s, env_step=990000, len=37, n/ep=0, n/st=10, player_1/loss=0.770, rew=60.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #99: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #100: 10001it [00:33, 298.33it/s, env_step=1000000, len=36, n/ep=2, n/st=10, player_1/loss=0.772, rew=64.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #100: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #101: 10001it [00:33, 296.94it/s, env_step=1010000, len=18, n/ep=0, n/st=10, player_1/loss=0.694, rew=17.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #101: test_reward: 74.200000 ± 20.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #102: 10001it [00:33, 297.70it/s, env_step=1020000, len=25, n/ep=2, n/st=10, player_1/loss=0.691, rew=49.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #102: test_reward: 73.800000 ± 21.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #103: 10001it [00:33, 296.50it/s, env_step=1030000, len=18, n/ep=0, n/st=10, player_1/loss=0.821, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #103: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #104: 10001it [00:33, 296.67it/s, env_step=1040000, len=21, n/ep=0, n/st=10, player_1/loss=0.870, rew=16.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #104: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #105: 10001it [00:33, 296.46it/s, env_step=1050000, len=19, n/ep=0, n/st=10, player_1/loss=0.829, rew=10.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #105: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #106: 10001it [00:33, 297.30it/s, env_step=1060000, len=18, n/ep=0, n/st=10, player_1/loss=0.868, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #106: test_reward: 2.200000 ± 2.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #107: 10001it [00:33, 297.07it/s, env_step=1070000, len=14, n/ep=0, n/st=10, player_1/loss=0.842, rew=-5.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #107: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #108: 10001it [00:33, 297.58it/s, env_step=1080000, len=18, n/ep=1, n/st=10, player_1/loss=0.876, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #108: test_reward: 8.200000 ± 15.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #109: 10001it [00:33, 297.00it/s, env_step=1090000, len=34, n/ep=1, n/st=10, player_1/loss=0.850, rew=37.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #109: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #110: 10001it [00:33, 298.65it/s, env_step=1100000, len=21, n/ep=0, n/st=10, player_1/loss=0.849, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #110: test_reward: 83.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #111: 10001it [00:32, 304.22it/s, env_step=1110000, len=29, n/ep=0, n/st=10, player_1/loss=0.728, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #111: test_reward: 30.200000 ± 2.400000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #112: 10001it [00:32, 303.35it/s, env_step=1120000, len=25, n/ep=0, n/st=10, player_1/loss=0.767, rew=32.33]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #112: test_reward: 33.800000 ± 9.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #113: 10001it [00:33, 300.53it/s, env_step=1130000, len=18, n/ep=0, n/st=10, player_1/loss=0.825, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #113: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #114: 10001it [00:33, 303.00it/s, env_step=1140000, len=29, n/ep=1, n/st=10, player_1/loss=0.727, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #114: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #115: 10001it [00:32, 303.99it/s, env_step=1150000, len=14, n/ep=0, n/st=10, player_1/loss=0.702, rew=-5.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #115: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #116: 10001it [00:32, 303.92it/s, env_step=1160000, len=18, n/ep=1, n/st=10, player_1/loss=0.754, rew=1.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #116: test_reward: 34.400000 ± 1.800000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #117: 10001it [00:32, 304.19it/s, env_step=1170000, len=23, n/ep=0, n/st=10, player_1/loss=0.692, rew=18.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #117: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #118: 10001it [00:32, 303.46it/s, env_step=1180000, len=29, n/ep=0, n/st=10, player_1/loss=0.711, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #118: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #119: 10001it [00:32, 303.42it/s, env_step=1190000, len=20, n/ep=0, n/st=10, player_1/loss=0.706, rew=15.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #119: test_reward: 27.800000 ± 14.427751, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #120: 10001it [00:32, 303.51it/s, env_step=1200000, len=29, n/ep=0, n/st=10, player_1/loss=0.713, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #120: test_reward: 32.000000 ± 9.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #121: 10001it [00:32, 303.90it/s, env_step=1210000, len=29, n/ep=0, n/st=10, player_1/loss=0.720, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #121: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #122: 10001it [00:32, 303.83it/s, env_step=1220000, len=23, n/ep=0, n/st=10, player_1/loss=0.713, rew=19.67]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #122: test_reward: 33.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #123: 10001it [00:33, 302.42it/s, env_step=1230000, len=14, n/ep=1, n/st=10, player_1/loss=0.708, rew=-5.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #123: test_reward: 37.000000 ± 11.349009, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #124: 10001it [00:33, 302.56it/s, env_step=1240000, len=29, n/ep=1, n/st=10, player_1/loss=0.718, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #124: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #125: 10001it [00:32, 303.20it/s, env_step=1250000, len=29, n/ep=0, n/st=10, player_1/loss=0.707, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #125: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #126: 10001it [00:32, 303.67it/s, env_step=1260000, len=18, n/ep=0, n/st=10, player_1/loss=0.778, rew=-5.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #126: test_reward: 33.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #127: 10001it [00:32, 303.86it/s, env_step=1270000, len=29, n/ep=1, n/st=10, player_1/loss=0.742, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #127: test_reward: 30.700000 ± 6.900000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #128: 10001it [00:32, 303.65it/s, env_step=1280000, len=29, n/ep=1, n/st=10, player_1/loss=0.771, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #128: test_reward: 31.000000 ± 10.545141, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #129: 10001it [00:33, 302.90it/s, env_step=1290000, len=15, n/ep=2, n/st=10, player_1/loss=0.796, rew=4.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #129: test_reward: 31.800000 ± 9.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #130: 10001it [00:33, 302.38it/s, env_step=1300000, len=29, n/ep=0, n/st=10, player_1/loss=0.775, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #130: test_reward: 32.800000 ± 6.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #131: 10001it [00:33, 300.04it/s, env_step=1310000, len=17, n/ep=0, n/st=10, player_1/loss=0.870, rew=1.50]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #131: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #132: 10001it [00:33, 298.39it/s, env_step=1320000, len=16, n/ep=0, n/st=10, player_1/loss=0.859, rew=13.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #132: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #133: 10001it [00:33, 299.56it/s, env_step=1330000, len=16, n/ep=0, n/st=10, player_1/loss=0.910, rew=13.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #133: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #134: 10001it [00:33, 298.16it/s, env_step=1340000, len=18, n/ep=1, n/st=10, player_1/loss=0.875, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #134: test_reward: 2.800000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #135: 10001it [00:33, 297.03it/s, env_step=1350000, len=18, n/ep=1, n/st=10, player_1/loss=0.862, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #135: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #136: 10001it [00:33, 296.85it/s, env_step=1360000, len=18, n/ep=0, n/st=10, player_1/loss=0.839, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #136: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #137: 10001it [00:33, 297.18it/s, env_step=1370000, len=18, n/ep=0, n/st=10, player_1/loss=0.833, rew=9.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #137: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #138: 10001it [00:33, 298.30it/s, env_step=1380000, len=18, n/ep=0, n/st=10, player_1/loss=0.809, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #138: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #139: 10001it [00:33, 298.76it/s, env_step=1390000, len=18, n/ep=0, n/st=10, player_1/loss=0.831, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #139: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #140: 10001it [00:33, 297.54it/s, env_step=1400000, len=29, n/ep=0, n/st=10, player_1/loss=0.823, rew=33.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #140: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #141: 10001it [00:33, 296.83it/s, env_step=1410000, len=18, n/ep=0, n/st=10, player_1/loss=0.829, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #141: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #142: 10001it [00:33, 298.35it/s, env_step=1420000, len=29, n/ep=0, n/st=10, player_1/loss=0.870, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #142: test_reward: 34.800000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #143: 10001it [00:32, 304.05it/s, env_step=1430000, len=26, n/ep=1, n/st=10, player_1/loss=0.731, rew=53.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #143: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #144: 10001it [00:32, 304.10it/s, env_step=1440000, len=29, n/ep=0, n/st=10, player_1/loss=0.742, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #144: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #145: 10001it [00:32, 303.43it/s, env_step=1450000, len=10, n/ep=0, n/st=10, player_1/loss=0.721, rew=-5.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #145: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #146: 10001it [00:32, 303.86it/s, env_step=1460000, len=29, n/ep=0, n/st=10, player_1/loss=0.794, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #146: test_reward: 33.400000 ± 4.800000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #147: 10001it [00:33, 303.00it/s, env_step=1470000, len=24, n/ep=2, n/st=10, player_1/loss=0.736, rew=17.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #147: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #148: 10001it [00:32, 303.37it/s, env_step=1480000, len=29, n/ep=0, n/st=10, player_1/loss=0.702, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #148: test_reward: 31.600000 ± 10.200000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #149: 10001it [00:32, 304.03it/s, env_step=1490000, len=13, n/ep=0, n/st=10, player_1/loss=0.669, rew=5.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #149: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #150: 10001it [00:32, 303.77it/s, env_step=1500000, len=29, n/ep=0, n/st=10, player_1/loss=0.669, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #150: test_reward: 32.900000 ± 6.300000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #151: 10001it [00:33, 299.24it/s, env_step=1510000, len=29, n/ep=1, n/st=10, player_1/loss=0.821, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #151: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #152: 10001it [00:33, 297.70it/s, env_step=1520000, len=18, n/ep=0, n/st=10, player_1/loss=0.869, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #152: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #153: 10001it [00:33, 297.01it/s, env_step=1530000, len=18, n/ep=0, n/st=10, player_1/loss=0.896, rew=2.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #153: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #154: 10001it [00:33, 297.16it/s, env_step=1540000, len=18, n/ep=0, n/st=10, player_1/loss=0.836, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #154: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #155: 10001it [00:33, 299.98it/s, env_step=1550000, len=20, n/ep=0, n/st=10, player_1/loss=0.771, rew=15.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #155: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #156: 10001it [00:33, 298.76it/s, env_step=1560000, len=20, n/ep=2, n/st=10, player_1/loss=0.793, rew=13.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #156: test_reward: 13.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #157: 10001it [00:33, 298.29it/s, env_step=1570000, len=29, n/ep=1, n/st=10, player_1/loss=0.780, rew=35.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #157: test_reward: 39.700000 ± 14.100000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #158: 10001it [00:33, 300.04it/s, env_step=1580000, len=18, n/ep=0, n/st=10, player_1/loss=0.737, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #158: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #159: 10001it [00:33, 298.93it/s, env_step=1590000, len=14, n/ep=1, n/st=10, player_1/loss=0.771, rew=-5.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #159: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #160: 10001it [00:34, 293.67it/s, env_step=1600000, len=18, n/ep=1, n/st=10, player_1/loss=0.766, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #160: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #161: 10001it [00:34, 293.65it/s, env_step=1610000, len=19, n/ep=1, n/st=10, player_1/loss=0.805, rew=5.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #161: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #162: 10001it [00:34, 293.27it/s, env_step=1620000, len=20, n/ep=1, n/st=10, player_1/loss=0.755, rew=11.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #162: test_reward: 3.300000 ± 4.290688, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #163: 10001it [00:34, 293.73it/s, env_step=1630000, len=18, n/ep=0, n/st=10, player_1/loss=0.832, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #163: test_reward: 2.800000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #164: 10001it [00:34, 293.67it/s, env_step=1640000, len=18, n/ep=0, n/st=10, player_1/loss=0.858, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #164: test_reward: 3.200000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #165: 10001it [00:33, 294.27it/s, env_step=1650000, len=18, n/ep=1, n/st=10, player_1/loss=0.923, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #165: test_reward: 8.600000 ± 16.800000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #166: 10001it [00:34, 293.87it/s, env_step=1660000, len=18, n/ep=1, n/st=10, player_1/loss=0.891, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #166: test_reward: 3.200000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #167: 10001it [00:34, 293.79it/s, env_step=1670000, len=18, n/ep=1, n/st=10, player_1/loss=0.814, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #167: test_reward: 3.200000 ± 0.600000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #168: 10001it [00:34, 293.61it/s, env_step=1680000, len=34, n/ep=1, n/st=10, player_1/loss=0.829, rew=45.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #168: test_reward: 16.000000 ± 39.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #169: 10001it [00:33, 299.55it/s, env_step=1690000, len=27, n/ep=0, n/st=10, player_1/loss=0.792, rew=31.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #169: test_reward: 35.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #170: 10001it [00:33, 295.61it/s, env_step=1700000, len=30, n/ep=1, n/st=10, player_1/loss=0.785, rew=33.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #170: test_reward: 33.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #171: 10001it [00:33, 297.94it/s, env_step=1710000, len=18, n/ep=0, n/st=10, player_1/loss=0.754, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #171: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #172: 10001it [00:33, 294.58it/s, env_step=1720000, len=19, n/ep=1, n/st=10, player_1/loss=0.843, rew=5.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #172: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #173: 10001it [00:34, 293.91it/s, env_step=1730000, len=18, n/ep=1, n/st=10, player_1/loss=0.848, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #173: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #174: 10001it [00:33, 294.36it/s, env_step=1740000, len=19, n/ep=0, n/st=10, player_1/loss=0.882, rew=5.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #174: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #175: 10001it [00:34, 293.26it/s, env_step=1750000, len=18, n/ep=0, n/st=10, player_1/loss=0.861, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #175: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #176: 10001it [00:33, 294.20it/s, env_step=1760000, len=34, n/ep=0, n/st=10, player_1/loss=0.863, rew=53.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #176: test_reward: 4.000000 ± 3.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #177: 10001it [00:33, 294.33it/s, env_step=1770000, len=18, n/ep=1, n/st=10, player_1/loss=0.851, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #177: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #178: 10001it [00:34, 294.00it/s, env_step=1780000, len=20, n/ep=1, n/st=10, player_1/loss=0.826, rew=15.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #178: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #179: 10001it [00:34, 293.73it/s, env_step=1790000, len=18, n/ep=0, n/st=10, player_1/loss=0.818, rew=1.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #179: test_reward: 3.000000 ± 0.000000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #180: 10001it [00:33, 294.21it/s, env_step=1800000, len=18, n/ep=2, n/st=10, player_1/loss=0.828, rew=3.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #180: test_reward: 7.600000 ± 15.900943, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #181: 10001it [00:33, 298.88it/s, env_step=1810000, len=42, n/ep=0, n/st=10, player_1/loss=0.864, rew=89.00]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #181: test_reward: 83.100000 ± 23.700000, best_reward: 97.000000 ± 0.000000 in #24\n",
      "testing for stop: 9.7 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #182: 10001it [00:32, 308.45it/s, env_step=1820000, len=19, n/ep=0, n/st=10, player_1/loss=0.975, rew=5.00]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #182: test_reward: 108.200000 ± 2.400000, best_reward: 108.200000 ± 2.400000 in #182\n",
      "testing for stop: 10.82 >= 10 -> True\n",
      "\n",
      "Started training agent player 1 against minimax with depth 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:58, 171.67it/s, env_step=10000, len=28, n/ep=1, n/st=10, player_1/loss=1.450, rew=-5.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 35.000000 ± 0.000000, best_reward: 49.200000 ± 15.759442 in #0\n",
      "testing for stop: 4.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [00:57, 173.60it/s, env_step=20000, len=10, n/ep=1, n/st=10, player_1/loss=1.494, rew=-5.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 17.000000 ± 0.000000, best_reward: 49.200000 ± 15.759442 in #0\n",
      "testing for stop: 4.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [00:57, 174.07it/s, env_step=30000, len=20, n/ep=0, n/st=10, player_1/loss=1.453, rew=15.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 5.000000 ± 0.000000, best_reward: 49.200000 ± 15.759442 in #0\n",
      "testing for stop: 4.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [00:57, 174.93it/s, env_step=40000, len=22, n/ep=0, n/st=10, player_1/loss=1.361, rew=-5.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 35.000000 ± 0.000000, best_reward: 49.200000 ± 15.759442 in #0\n",
      "testing for stop: 4.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [00:57, 174.80it/s, env_step=50000, len=24, n/ep=0, n/st=10, player_1/loss=1.393, rew=23.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 33.000000 ± 0.000000, best_reward: 49.200000 ± 15.759442 in #0\n",
      "testing for stop: 4.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [00:53, 185.91it/s, env_step=60000, len=34, n/ep=0, n/st=10, player_1/loss=1.384, rew=13.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 24.200000 ± 5.878775, best_reward: 49.200000 ± 15.759442 in #0\n",
      "testing for stop: 4.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10001it [00:52, 189.47it/s, env_step=70000, len=26, n/ep=0, n/st=10, player_1/loss=1.134, rew=13.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 89.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10001it [00:51, 193.11it/s, env_step=80000, len=42, n/ep=1, n/st=10, player_1/loss=1.023, rew=85.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 79.600000 ± 28.200000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10001it [00:52, 191.42it/s, env_step=90000, len=30, n/ep=0, n/st=10, player_1/loss=0.930, rew=23.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 82.200000 ± 20.400000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10001it [00:52, 189.54it/s, env_step=100000, len=28, n/ep=0, n/st=10, player_1/loss=0.960, rew=17.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 49.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 10001it [00:52, 189.67it/s, env_step=110000, len=36, n/ep=0, n/st=10, player_1/loss=0.928, rew=29.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 69.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 10001it [00:53, 186.10it/s, env_step=120000, len=42, n/ep=1, n/st=10, player_1/loss=1.209, rew=81.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 33.800000 ± 3.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 10001it [00:53, 185.95it/s, env_step=130000, len=42, n/ep=0, n/st=10, player_1/loss=0.970, rew=81.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 23.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 10001it [00:54, 183.88it/s, env_step=140000, len=34, n/ep=0, n/st=10, player_1/loss=1.050, rew=23.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 89.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 10001it [00:51, 193.69it/s, env_step=150000, len=35, n/ep=2, n/st=10, player_1/loss=1.080, rew=27.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 17.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 10001it [00:51, 192.53it/s, env_step=160000, len=34, n/ep=0, n/st=10, player_1/loss=1.204, rew=23.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 26.400000 ± 1.800000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 10001it [00:54, 184.40it/s, env_step=170000, len=34, n/ep=0, n/st=10, player_1/loss=1.109, rew=23.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 47.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 10001it [00:53, 187.61it/s, env_step=180000, len=34, n/ep=0, n/st=10, player_1/loss=0.996, rew=35.67]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 20.200000 ± 8.400000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 10001it [00:54, 183.00it/s, env_step=190000, len=32, n/ep=0, n/st=10, player_1/loss=1.052, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 23.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 10001it [00:53, 186.76it/s, env_step=200000, len=38, n/ep=2, n/st=10, player_1/loss=1.214, rew=69.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 60.400000 ± 7.800000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 10001it [00:53, 186.15it/s, env_step=210000, len=36, n/ep=2, n/st=10, player_1/loss=1.021, rew=63.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: 62.000000 ± 3.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 10001it [00:54, 182.46it/s, env_step=220000, len=38, n/ep=1, n/st=10, player_1/loss=0.890, rew=65.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: 63.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 10001it [00:53, 185.79it/s, env_step=230000, len=40, n/ep=1, n/st=10, player_1/loss=0.846, rew=49.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: 63.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 10001it [00:52, 190.09it/s, env_step=240000, len=32, n/ep=0, n/st=10, player_1/loss=0.906, rew=21.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: 17.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 10001it [00:54, 183.65it/s, env_step=250000, len=36, n/ep=0, n/st=10, player_1/loss=0.876, rew=63.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: 61.600000 ± 3.583295, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 10001it [00:55, 181.08it/s, env_step=260000, len=24, n/ep=1, n/st=10, player_1/loss=0.794, rew=13.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: 35.800000 ± 3.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 10001it [00:54, 182.35it/s, env_step=270000, len=23, n/ep=0, n/st=10, player_1/loss=0.984, rew=12.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: 23.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 10001it [00:55, 181.17it/s, env_step=280000, len=8, n/ep=1, n/st=10, player_1/loss=0.974, rew=-5.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: 33.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 10001it [00:54, 182.48it/s, env_step=290000, len=26, n/ep=0, n/st=10, player_1/loss=1.034, rew=23.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: 67.600000 ± 22.200000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 10001it [00:54, 183.03it/s, env_step=300000, len=32, n/ep=0, n/st=10, player_1/loss=1.069, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: 33.200000 ± 5.400000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 10001it [00:53, 188.33it/s, env_step=310000, len=26, n/ep=0, n/st=10, player_1/loss=0.827, rew=22.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: 34.800000 ± 0.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 10001it [00:52, 190.25it/s, env_step=320000, len=16, n/ep=0, n/st=10, player_1/loss=1.398, rew=-3.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: 1.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 10001it [00:51, 192.78it/s, env_step=330000, len=16, n/ep=0, n/st=10, player_1/loss=1.405, rew=-5.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: 7.800000 ± 3.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 10001it [00:54, 184.60it/s, env_step=340000, len=20, n/ep=1, n/st=10, player_1/loss=1.119, rew=17.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: 35.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 10001it [00:54, 183.06it/s, env_step=350000, len=34, n/ep=0, n/st=10, player_1/loss=1.380, rew=31.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: 14.800000 ± 6.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 10001it [00:56, 176.83it/s, env_step=360000, len=24, n/ep=1, n/st=10, player_1/loss=1.260, rew=25.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: 25.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 10001it [00:54, 183.95it/s, env_step=370000, len=32, n/ep=0, n/st=10, player_1/loss=1.148, rew=35.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: 40.600000 ± 15.615377, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 10001it [00:53, 188.19it/s, env_step=380000, len=32, n/ep=0, n/st=10, player_1/loss=1.071, rew=21.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: 22.200000 ± 3.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 10001it [00:53, 186.22it/s, env_step=390000, len=42, n/ep=0, n/st=10, player_1/loss=0.986, rew=83.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: 83.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 10001it [00:54, 183.82it/s, env_step=400000, len=30, n/ep=0, n/st=10, player_1/loss=1.099, rew=5.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: 3.200000 ± 0.600000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 10001it [00:54, 184.66it/s, env_step=410000, len=16, n/ep=0, n/st=10, player_1/loss=1.265, rew=3.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: 3.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 10001it [00:52, 188.94it/s, env_step=420000, len=22, n/ep=0, n/st=10, player_1/loss=1.139, rew=15.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: 35.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 10001it [00:54, 184.12it/s, env_step=430000, len=42, n/ep=0, n/st=10, player_1/loss=0.842, rew=57.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: 39.800000 ± 14.400000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 10001it [00:54, 184.82it/s, env_step=440000, len=16, n/ep=0, n/st=10, player_1/loss=0.935, rew=-5.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: 85.000000 ± 0.000000, best_reward: 89.000000 ± 0.000000 in #7\n",
      "testing for stop: 8.9 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 10001it [00:52, 192.14it/s, env_step=450000, len=24, n/ep=0, n/st=10, player_1/loss=0.956, rew=7.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: 107.000000 ± 0.000000, best_reward: 107.000000 ± 0.000000 in #45\n",
      "testing for stop: 10.7 >= 10 -> True\n",
      "\n",
      "Started training agent player 1 against minimax with depth 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [03:31, 47.33it/s, env_step=10000, len=24, n/ep=3, n/st=10, player_1/loss=1.533, rew=29.67]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 19.000000 ± 0.000000, best_reward: 54.400000 ± 4.200000 in #0\n",
      "testing for stop: 5.4399999999999995 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [03:32, 47.13it/s, env_step=20000, len=30, n/ep=0, n/st=10, player_1/loss=1.488, rew=17.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 17.000000 ± 0.000000, best_reward: 54.400000 ± 4.200000 in #0\n",
      "testing for stop: 5.4399999999999995 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [03:26, 48.47it/s, env_step=30000, len=12, n/ep=0, n/st=10, player_1/loss=1.451, rew=-5.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 29.000000 ± 0.000000, best_reward: 54.400000 ± 4.200000 in #0\n",
      "testing for stop: 5.4399999999999995 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [03:28, 47.98it/s, env_step=40000, len=16, n/ep=3, n/st=10, player_1/loss=1.470, rew=5.67]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 13.700000 ± 5.100000, best_reward: 54.400000 ± 4.200000 in #0\n",
      "testing for stop: 5.4399999999999995 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [03:26, 48.50it/s, env_step=50000, len=30, n/ep=1, n/st=10, player_1/loss=1.427, rew=7.00]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 17.900000 ± 17.700000, best_reward: 54.400000 ± 4.200000 in #0\n",
      "testing for stop: 5.4399999999999995 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [03:08, 53.09it/s, env_step=60000, len=38, n/ep=0, n/st=10, player_1/loss=1.197, rew=77.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 71.800000 ± 15.600000, best_reward: 71.800000 ± 15.600000 in #6\n",
      "testing for stop: 7.18 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10001it [03:03, 54.59it/s, env_step=70000, len=12, n/ep=0, n/st=10, player_1/loss=1.029, rew=3.00]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 71.000000 ± 0.000000, best_reward: 71.800000 ± 15.600000 in #6\n",
      "testing for stop: 7.18 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10001it [02:54, 57.40it/s, env_step=80000, len=42, n/ep=0, n/st=10, player_1/loss=0.734, rew=105.00]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 57.000000 ± 18.000000, best_reward: 71.800000 ± 15.600000 in #6\n",
      "testing for stop: 7.18 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10001it [02:56, 56.60it/s, env_step=90000, len=34, n/ep=0, n/st=10, player_1/loss=0.757, rew=55.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 79.200000 ± 17.400000, best_reward: 79.200000 ± 17.400000 in #9\n",
      "testing for stop: 7.92 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10001it [03:02, 54.77it/s, env_step=100000, len=40, n/ep=0, n/st=10, player_1/loss=0.709, rew=99.00]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 87.700000 ± 36.160890, best_reward: 87.700000 ± 36.160890 in #10\n",
      "testing for stop: 8.77 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 10001it [02:52, 57.98it/s, env_step=110000, len=42, n/ep=1, n/st=10, player_1/loss=0.644, rew=113.00]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 113.000000 ± 0.000000, best_reward: 113.000000 ± 0.000000 in #11\n",
      "testing for stop: 11.3 >= 10 -> True\n",
      "\n",
      "Started training agent player 1 against minimax with depth 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [09:01, 18.46it/s, env_step=10000, len=14, n/ep=1, n/st=10, player_1/loss=1.405, rew=-5.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 51.000000 ± 0.000000, best_reward: 67.200000 ± 9.897474 in #0\n",
      "testing for stop: 6.720000000000001 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [08:50, 18.84it/s, env_step=20000, len=28, n/ep=2, n/st=10, player_1/loss=1.335, rew=10.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 39.800000 ± 2.400000, best_reward: 67.200000 ± 9.897474 in #0\n",
      "testing for stop: 6.720000000000001 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [08:55, 18.66it/s, env_step=30000, len=17, n/ep=0, n/st=10, player_1/loss=1.316, rew=3.00]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 39.000000 ± 0.000000, best_reward: 67.200000 ± 9.897474 in #0\n",
      "testing for stop: 6.720000000000001 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [08:52, 18.79it/s, env_step=40000, len=18, n/ep=1, n/st=10, player_1/loss=1.380, rew=11.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 45.400000 ± 13.290598, best_reward: 67.200000 ± 9.897474 in #0\n",
      "testing for stop: 6.720000000000001 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [08:51, 18.83it/s, env_step=50000, len=20, n/ep=2, n/st=10, player_1/loss=1.376, rew=2.00]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 23.000000 ± 0.000000, best_reward: 67.200000 ± 9.897474 in #0\n",
      "testing for stop: 6.720000000000001 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [06:39, 25.04it/s, env_step=60000, len=38, n/ep=0, n/st=10, player_1/loss=0.971, rew=59.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 104.800000 ± 36.600000, best_reward: 104.800000 ± 36.600000 in #6\n",
      "testing for stop: 10.48 >= 10 -> True\n",
      "\n",
      "Started training agent player 1 against minimax with depth 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [36:12,  4.60it/s, env_step=10000, len=30, n/ep=0, n/st=10, player_1/loss=1.419, rew=11.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 7.000000 ± 0.000000, best_reward: 53.000000 ± 9.838699 in #0\n",
      "testing for stop: 5.3 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [35:23,  4.71it/s, env_step=20000, len=20, n/ep=0, n/st=10, player_1/loss=1.495, rew=13.67]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 11.000000 ± 0.000000, best_reward: 53.000000 ± 9.838699 in #0\n",
      "testing for stop: 5.3 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [36:49,  4.53it/s, env_step=30000, len=17, n/ep=0, n/st=10, player_1/loss=1.439, rew=12.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 11.000000 ± 0.000000, best_reward: 53.000000 ± 9.838699 in #0\n",
      "testing for stop: 5.3 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [36:22,  4.58it/s, env_step=40000, len=30, n/ep=0, n/st=10, player_1/loss=1.463, rew=17.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 41.200000 ± 4.935585, best_reward: 53.000000 ± 9.838699 in #0\n",
      "testing for stop: 5.3 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [36:23,  4.58it/s, env_step=50000, len=21, n/ep=0, n/st=10, player_1/loss=1.428, rew=14.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 9.000000 ± 0.000000, best_reward: 53.000000 ± 9.838699 in #0\n",
      "testing for stop: 5.3 >= 10 -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [27:59,  5.96it/s, env_step=60000, len=28, n/ep=0, n/st=10, player_1/loss=1.123, rew=17.00]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 109.000000 ± 0.000000, best_reward: 109.000000 ± 0.000000 in #6\n",
      "testing for stop: 10.9 >= 10 -> True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: TRAINING AGENTS\n",
    "####################################################\n",
    "\n",
    "# Select agent for minimax\n",
    "agent1_is_minimax = False\n",
    "\n",
    "# Specify starter for rainbow\n",
    "rainbow_starting_params = \"./saved_variables/paper_notebooks/9/rainbow_vs_rainbow_blocking_reward_complex_cnn/best_policy_agent1.pth\"\n",
    "\n",
    "# Experiment settings\n",
    "epochs = 250\n",
    "loops = 5\n",
    "stopping_threshold = 10\n",
    "training_eps_init = 0.4\n",
    "training_eps_final = 0.05\n",
    "\n",
    "# Filename prefix\n",
    "filename_prefix = \"1-250epoch_5loop/looping-iteration-\"\n",
    "\n",
    "for loop_idx in range(loops):\n",
    "    # Depth is loop index +1 \n",
    "    depth = loop_idx + 1\n",
    "    \n",
    "    # Filename\n",
    "    filename = filename_prefix + str(loop_idx)\n",
    "    \n",
    "    # Use provided starting params in first loop, the one from previous iteration in next\n",
    "    if loop_idx > 0:\n",
    "        if agent1_is_minimax:\n",
    "            rainbow_starting_params = \"./saved_variables/paper_notebooks/11/\" + filename_prefix + str(loop_idx - 1) + \"/best_policy_agent2.pth\"\n",
    "        else:\n",
    "            rainbow_starting_params = \"./saved_variables/paper_notebooks/11/\" + filename_prefix + str(loop_idx - 1) + \"/best_policy_agent1.pth\"\n",
    "    \n",
    "    \n",
    "    # Show info\n",
    "    print()\n",
    "    training_agent = \"2\" if agent1_is_minimax else \"1\"\n",
    "    print(f\"Started training agent player {training_agent} against minimax with depth {depth}\")\n",
    "    \n",
    "    # Get the environment settings\n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    \n",
    "    # Configure rainbow agent\n",
    "    rainbow_agent = rainbow_policy(state_shape= state_shape,\n",
    "                                   action_shape= action_shape)\n",
    "    \n",
    "    if rainbow_starting_params:\n",
    "        rainbow_agent.load_state_dict(load_torch_dict(rainbow_starting_params))\n",
    "        \n",
    "    # Configure minimax agent\n",
    "    minimax_agent = TianshouMiniMaxConnectFourPolicy(coin= 1 if agent1_is_minimax else 2,\n",
    "                                                     oponent_coin= 2 if agent1_is_minimax else 1,\n",
    "                                                     minimax_depth= depth)\n",
    "        \n",
    "        \n",
    "    # Train the agent\n",
    "    off_policy_traininer_results, final_agent_player1, final_agent_player2 = train_agent(epochs= epochs,\n",
    "                                                                                         agent_player1= minimax_agent if agent1_is_minimax else rainbow_agent,\n",
    "                                                                                         agent_player1_frozen= True if agent1_is_minimax else False,\n",
    "                                                                                         agent_player2= rainbow_agent if agent1_is_minimax else minimax_agent,\n",
    "                                                                                         agent_player2_frozen= False if agent1_is_minimax else True,\n",
    "                                                                                         filename= filename,\n",
    "                                                                                         training_eps_init = training_eps_init,\n",
    "                                                                                         training_eps_final = training_eps_final,\n",
    "                                                                                         stopping_threshold= stopping_threshold,\n",
    "                                                                                         single_agent_score_as_reward = True)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d75d5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average steps of game:  42.0\n",
      "Final mean reward agent 1: 109.0, std: 0.0\n",
      "Final mean reward agent 2: 3.0, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# EXPERIMENT: VIEWING THE BEST LEARNED POLICY\n",
    "####################################################\n",
    "\n",
    "# settings\n",
    "depth = 1\n",
    "agent1_is_minimax = False\n",
    "\n",
    "# Get the environment settings\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "# Configure rainbow agent\n",
    "rainbow_agent = rainbow_policy(state_shape= state_shape,\n",
    "                               action_shape= action_shape)\n",
    "rainbow_agent.load_state_dict(load_torch_dict(\"./saved_variables/paper_notebooks/11/1-250epoch_5loop/looping-iteration-0/best_policy_agent1.pth\"))\n",
    "rainbow_agent.set_eps(0)\n",
    "      \n",
    "# Configure minimax agent\n",
    "minimax_agent = TianshouMiniMaxConnectFourPolicy(coin= 1 if agent1_is_minimax else 2,\n",
    "                                                oponent_coin= 2 if agent1_is_minimax else 1,\n",
    "                                                minimax_depth= depth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Watch the best agent at work\n",
    "watch(numer_of_games= 3,\n",
    "      render_speed= 0.3,\n",
    "      agent_player1= minimax_agent if agent1_is_minimax else rainbow_agent,\n",
    "      agent_player2= rainbow_agent if agent1_is_minimax else minimax_agent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84478519",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Discussion\n",
    "\n",
    "By playing against a considerably smart agent the rainbow algorithm starts to struggle since it fails to win.\n",
    "The trained behaviour is mostly defensive.\n",
    "Our minimax agent gets harder to beat as the iteration increases since the iteration number corresponds with the depth of the minimax agent.\n",
    "Since we re-optimize the rainbow agent on this, we should have an incrementally better rainbow agent given enough training.\n",
    "This would fulfil our goal of having a variable difficulty bot.\n",
    "This approach is not ideal, as the minimax algorithm is fixed so the model can train to the exact agents behaviour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9110be",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN VARIABLES\n",
    "####################################################\n",
    "\n",
    "del action_shape\n",
    "del agent1_is_minimax\n",
    "del depth\n",
    "del env\n",
    "del epochs\n",
    "del filename\n",
    "del filename_prefix\n",
    "del final_agent_player1\n",
    "del final_agent_player2\n",
    "del loop_idx\n",
    "del loops\n",
    "del minimax_agent\n",
    "del observation_space\n",
    "del off_policy_traininer_results\n",
    "del rainbow_agent\n",
    "del rainbow_starting_params\n",
    "del state_shape\n",
    "del stopping_threshold\n",
    "del training_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72b972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
