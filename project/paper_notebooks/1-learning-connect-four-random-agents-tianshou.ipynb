{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Connect four with random agents from Tianshou\n",
    "\n",
    "In this notebook the created custom gym environment of connect four is played using two random agents.\n",
    "We use the powerfull [Tianshou library](https://github.com/thu-ml/tianshou) for this.\n",
    "This notebook thus shows how the multi-agent environment can be configured using Tianshou for managing the agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training two random agents on connect four Gym\n",
    "  - Create the Gym environment\n",
    "  - Create a Multi Agent Policy Manager\n",
    "  - Create a data collector\n",
    "  - Collect data for a given amount of episodes\n",
    "- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following codeblock will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.21.0 recommended): 0.21.0\n",
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.11.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.11.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# PPrint is a pretty print for variables\n",
    "from pprint import pprint\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(cfgym)\n",
    "\n",
    "# Time for allowing \"freezes\" in execution\n",
    "import time;\n",
    "\n",
    "# Used for updating notebook display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following codeblock tests if this was done succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"Amount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training two random agents on connect four Gym\n",
    "\n",
    "Our connect four gym setup requires two agents, one for each player.\n",
    "To reduce complexity, agents will always play as the same player, e.g. always as player 1.\n",
    "It is important to note that connect four is a *solved game*.\n",
    "According to [The Washington Post](https://www.washingtonpost.com/news/wonk/wp/2015/05/08/how-to-win-any-popular-game-according-to-data-scientists/):\n",
    "\n",
    "> Connect Four is what mathematicians call a \"solved game,\" meaning you can play it perfectly every time, no matter what your opponent does. You will need to get the first move, but as long as you do so, you can always win within 41 moves.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Create the Gym environment\n",
    "\n",
    "Whilst our first connect four implementation (V1) was playable in a multi-agent manner through a manual game loop, as is done for random agents in the experimental notebook `2-testing-custom-gym-environment.ipynb`, it is hard to use libraries for this environment.\n",
    "That is because the Gym environment was originally made with single agent games in mind and there is no real standard on how to write multi-agent environments.\n",
    "Thus, Tianshou, which offers some multi-agent support, didn't work well with this version of the Gym environment.\n",
    "To tackle this, we created a V2, which is a rework of V1 to follow the standards of a *Petting Zoo* environment.\n",
    "[Petting zoo](https://www.pettingzoo.ml/) is a library that offers many Gym environments extended to be multi-agent which uses wrapper classes and base classes so that each multi-agent environment follows the same guidelines.\n",
    "The environment is now a subclass of `AECEnv` rather then a `gym.Env`, which follows a similar approach but requires far more attributes and more complex observation and action spaces so that each agent has their own, even if they are all equal.\n",
    "\n",
    "We note that we only found out about this library after running into troubles making our V1 work with libraries that support Gym training.\n",
    "We could have searched for online implementations of e.g. Deep Q-Network (DQN) that allows to train using the gym gaming loop used in `2-testing-custom-gym-environment.ipynb`. \n",
    "Nonetheless, this would require many manual work which is not really ideal for the goal of this paper, which focuses on reducing work for game developers.\n",
    "Thus, the decission was made to create this V2 of the environment based on Petting Zoo environments.\n",
    "As a final note, we noticed there was a Connect Four implementation of Petting Zoo.\n",
    "This implementation differs from ours in a number of ways.\n",
    "Their observation space consists of multiple variants of the board, namely one with only the oponents coin and the agent's coins and it uses action masks to not allow placing coins in full columns.\n",
    "We use a singular observation space of the complete board, which is the location of both agent's coins and the free spaces, corresponding to what a human would see.\n",
    "Adding to this, we also don't programmatically disallow placing a coin in a full column but rather punish the user for trying that action and leave the board unchanged and let the agent play again.\n",
    "There are more logic differences and the visual game is also completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a65ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict(action_mask:Box([0 0 0 0 0 0 0], [1 1 1 1 1 1 1], (7,), int8), observation:Box([[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]], [[2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2]], (6, 7), int8))\n",
      "\n",
      "Action space: Discrete(7)\n",
      "\n",
      " Initial player id:\n",
      "player_1\n",
      "\n",
      " Initial observation:\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " Initial mask:\n",
      "[True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# SETTING UP THE GYM ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "# Create an instance of the environment to be used\n",
    "# V2 is used as this contains edits for Tianshou\n",
    "# We use the PettingZooEnv wrapper for multiagent support\n",
    "env = ts.env.PettingZooEnv(cfgym.env())\n",
    "\n",
    "# Get information about the environment\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"\\n Initial player id:\")\n",
    "print(observation[\"agent_id\"])\n",
    "\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation[\"obs\"])\n",
    "\n",
    "print(\"\\n Initial mask:\")\n",
    "print(observation[\"mask\"])\n",
    "\n",
    "# Clean unused variables\n",
    "del observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d7c37",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Create a Multi Agent Policy Manager\n",
    "\n",
    "Since we have a multi-agent environment we need a multi-agent policy manager which manages a policy for each agent.\n",
    "For starters, this policy is a simple randompolicy.\n",
    "This is very much based on the [tic-tac-toe example from the Tianshou docs](https://tianshou.readthedocs.io/en/master/tutorials/tictactoe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d5e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETTING UP THE MULTI AGENT POLICY MANAGER\n",
    "####################################################\n",
    "\n",
    "# Create the multi agent policy manager\n",
    "multi_agent_policy_manager = ts.policy.MultiAgentPolicyManager([ts.policy.RandomPolicy(), ts.policy.RandomPolicy()], env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395ddff",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Create a data collector\n",
    "\n",
    "Having a policy, which is a multi agent policy, we can setup a Tianshou data collector to collect data using the provided policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c497fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETTING UP THE DATA COLLECTOR\n",
    "####################################################\n",
    "\n",
    "# Need to vectorize the environment for the collector\n",
    "vectorized_env = ts.env.DummyVectorEnv([lambda: env])\n",
    "\n",
    "# use collectors to collect episodes of trajectories\n",
    "collector = ts.data.Collector(multi_agent_policy_manager, vectorized_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2055003",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Collect data for a given amount of episodes\n",
    "\n",
    "Having set up the policy and data collector, we can start gathering results from playing the policy.\n",
    "Since this is a random policy, the agents don't learn and there is not much more we can do.\n",
    "Thus, we visualize this policy game playing for 3 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c5528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idxs': array([0, 0, 0]),\n",
      " 'len': 17.666666666666668,\n",
      " 'len_std': 4.189935029992178,\n",
      " 'lens': array([19, 22, 12]),\n",
      " 'n/ep': 3,\n",
      " 'n/st': 53,\n",
      " 'rew': 0.0,\n",
      " 'rew_std': 10.0,\n",
      " 'rews': array([[ 10., -10.],\n",
      "       [-10.,  10.],\n",
      "       [-10.,  10.]])}\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# COLLECTING DATA\n",
    "####################################################\n",
    "\n",
    "# Collect results over 3 episodes (complete games)\n",
    "# If the render option is set, a step is made every\n",
    "#   specified amount of seconds.\n",
    "results = collector.collect(n_episode=3, render=.15)\n",
    "\n",
    "# Close the environment aftering collecting the results\n",
    "# This closes the pygame window after completion\n",
    "env.close()\n",
    "\n",
    "# Show the obtained results\n",
    "pprint(results)\n",
    "\n",
    "# Remove unused variables\n",
    "del results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab7db8",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Discussion\n",
    "\n",
    "We see that our V2 gym environment, based on an `AECenv` object (Petting Zoo multi agent environment), works with the Tianshou library for multi-agent data collection using a random policy.\n",
    "In the next notebooks a DQN will be trained.\n",
    "We also see correct rewards are obtained, remember that the negative reward for placing a coin in a full column which can add up since we play with random agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea03143",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN VARIABLES\n",
    "####################################################\n",
    "\n",
    "del collector\n",
    "del env\n",
    "del multi_agent_policy_manager\n",
    "del vectorized_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae27d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
