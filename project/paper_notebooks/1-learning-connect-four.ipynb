{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Connect four with random agents from Tianshou\n",
    "\n",
    "In this notebook the created custom gym environment of connect four is played using two random agents.\n",
    "We use the powerfull [Tianshou library](https://github.com/thu-ml/tianshou) for this.\n",
    "This notebook thus shows how the multi-agent environment can be configured using Tianshou for managing the agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training two agents on connect four Gym\n",
    "  - Create the Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following codeblock will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Pygame version (2.1.2 recommended): 2.1.2\n",
      "Gym version (0.23.1 recommended): 0.23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.11.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\gym\\envs\\registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment lennert_bontinck/ConnectFour-v1\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n",
      "c:\\ProgramData\\Anaconda3\\envs\\rl-project\\lib\\site-packages\\gym\\envs\\registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment lennert_bontinck/ConnectFour-v2\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pygame\n",
    "import pygame; print(f\"Pygame version (2.1.2 recommended): {pygame.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.23.1 recommended): {gym.__version__}\")\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.11.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame\n",
    "importlib.invalidate_caches()\n",
    "importlib.reload(gym_connect4_pygame)\n",
    "\n",
    "# Time for allowing \"freezes\" in execution\n",
    "import time;\n",
    "\n",
    "# Used for updating notebook display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following codeblock tests if this was done succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"Amount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training two random agents on connect four Gym\n",
    "\n",
    "Our connect four gym setup requires two agents, one for each player.\n",
    "To reduce complexity, agents will always play as the same player, e.g. always as player 1.\n",
    "It is important to note that connect four is a *solved game*.\n",
    "According to [The Washington Post](https://www.washingtonpost.com/news/wonk/wp/2015/05/08/how-to-win-any-popular-game-according-to-data-scientists/):\n",
    "\n",
    "> Connect Four is what mathematicians call a \"solved game,\" meaning you can play it perfectly every time, no matter what your opponent does. You will need to get the first move, but as long as you do so, you can always win within 41 moves.\n",
    "\n",
    "### Create the Gym environment\n",
    "\n",
    "We start of by instatiating our previously created connect four gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a65ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETTING UP THE GYM ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "# Create an instance of the environment to be used\n",
    "# V2 is used as this contains edits for Tianshou\n",
    "env = gym.make('lennert_bontinck/ConnectFour-v2')\n",
    "\n",
    "# Get information about the environment\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "\n",
    "# Reset the environment to start from a clean state, returns the initial observation\n",
    "observation, info = env.reset(return_info= True)\n",
    "print(\"\\n Initial observation:\")\n",
    "print(observation)\n",
    "\n",
    "print(\"\\n Initial info:\")\n",
    "print(info)\n",
    "\n",
    "# Clean unused variables\n",
    "del observation\n",
    "del info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d7c37",
   "metadata": {},
   "source": [
    "### Create a Multi Agent Policy Manager\n",
    "\n",
    "Since we have a multi-agent environment we need a multi agent policy manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETTING UP THE MULTI AGENT POLICY MANAGER\n",
    "####################################################\n",
    "\n",
    "# Create the mutly agent policy manager\n",
    "multi_agent_policy_manager = ts.policy.MultiAgentPolicyManager([ts.policy.RandomPolicy(), ts.policy.RandomPolicy()], env)\n",
    "\n",
    "# need to vectorize the environment for the collector\n",
    "vectorized_env = ts.env.DummyVectorEnv([lambda: env])\n",
    "\n",
    "# use collectors to collect a episode of trajectories\n",
    "# the reward is a vector, so we need a scalar metric to monitor the training\n",
    "collector = ts.data.Collector(multi_agent_policy_manager, vectorized_env)\n",
    "\n",
    "\n",
    "result = collector.collect(n_episode=1, render=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ffd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('rl-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
