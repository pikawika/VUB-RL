{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Human vs bot game\n",
    "\n",
    "In this notebook, we give a tutorial on how to run a pygame between two differing agents, where there is an option to be the agent in a game yourself.\n",
    "Thus, this notebook guides you in using a bot as the opponent in the connect four game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Setup the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.11.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.11.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import human_vs_bot_connect4.human_vs_bot_connect_four as game\n",
    "importlib.invalidate_caches();\n",
    "importlib.reload(game);\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show current cuda device\n",
    "print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Show cuda device name\n",
    "print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Policies\n",
    "\n",
    "We need to specify the policies the trained agent's weight are for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a940ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY FROM PAPER NOTEBOOK 5\n",
    "####################################################\n",
    "\n",
    "class CustomDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(np.prod(state_shape), 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, np.prod(action_shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "def cf_custom_dqn_policy(state_shape: tuple,\n",
    "                         action_shape: tuple,\n",
    "                         learning_rate: float =  0.0001,\n",
    "                         gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                         n_step: int = 1, # Number of steps to look ahead\n",
    "                         target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CustomDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2eb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY FROM PAPER NOTEBOOK 7\n",
    "####################################################\n",
    "\n",
    "class CNNBasedDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        # Number of input channels\n",
    "        input_channels_cnn = 1\n",
    "        output_channels_cnn = 32\n",
    "        flatten_size = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        output_size= np.prod(action_shape)\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(0,-1),\n",
    "            torch.nn.Unflatten(0, (1, flatten_size)),\n",
    "            torch.nn.Linear(flatten_size, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        \n",
    "        logits = self.model(obs)\n",
    "        return logits, state\n",
    "\n",
    "    \n",
    "def cf_cnn_dqn_policy(state_shape: tuple,\n",
    "                      action_shape: tuple,\n",
    "                      optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                      learning_rate: float =  0.0001,\n",
    "                      gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                      n_step: int = 4, # Number of steps to look ahead\n",
    "                      frozen: bool = False,\n",
    "                      target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CNNBasedDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # If we are frozen, we use an optimizer that has learning rate 0\n",
    "    if frozen:\n",
    "        optim = torch.optim.SGD(net.parameters(), lr= 0)\n",
    "        \n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f748a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW POLICY FROM PAPER NOTEBOOK 9\n",
    "####################################################\n",
    "\n",
    "class DQNForRainbow(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN to be used as baseclass for the Rainbow algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        c: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        action_shape: typing.Sequence[int],\n",
    "        device: typing.Union[str, int, torch.device] = \"cpu\",\n",
    "        features_only: bool = False,\n",
    "        output_dim: typing.Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Number of input channels\n",
    "        input_channels_cnn = 1\n",
    "        output_channels_cnn = 32\n",
    "        self.output_dim = (h - 3) * (w - 3) * output_channels_cnn\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "        if not features_only:\n",
    "            self.net = torch.nn.Sequential(\n",
    "                self.net, torch.nn.Linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Linear(512, np.prod(action_shape))\n",
    "            )\n",
    "            self.output_dim = np.prod(action_shape)\n",
    "        elif output_dim is not None:\n",
    "            self.net = torch.nn.Sequential(\n",
    "                self.net, torch.nn.Linear(self.output_dim, output_dim),\n",
    "                torch.nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "        state: typing.Optional[typing.Any] = None,\n",
    "        info: typing.Dict[str, typing.Any] = {},\n",
    "    ) -> typing.Tuple[torch.Tensor, typing.Any]:\n",
    "        r\"\"\"Mapping: s -> Q(s, \\*).\"\"\"\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "            \n",
    "        # Make right shape\n",
    "        obs = obs[:, None, :, :]\n",
    "        \n",
    "        logits = self.net(obs)\n",
    "        return logits, state\n",
    "\n",
    "\n",
    "class Rainbow(DQNForRainbow):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        c: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        action_shape: typing.Sequence[int],\n",
    "        num_atoms: int = 51,\n",
    "        noisy_std: float = 0.5,\n",
    "        device: typing.Union[str, int, torch.device] = \"cpu\",\n",
    "        is_dueling: bool = True,\n",
    "        is_noisy: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__(c, h, w, action_shape, device, features_only=True)\n",
    "        self.action_num = np.prod(action_shape)\n",
    "        self.num_atoms = num_atoms\n",
    "\n",
    "        def linear(x, y):\n",
    "            if is_noisy:\n",
    "                return ts.utils.net.discrete.NoisyLinear(x, y, noisy_std)\n",
    "            else:\n",
    "                return torch.nn.Linear(x, y)\n",
    "\n",
    "        self.Q = torch.nn.Sequential(\n",
    "            linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "            linear(512, self.action_num * self.num_atoms)\n",
    "        )\n",
    "        self._is_dueling = is_dueling\n",
    "        if self._is_dueling:\n",
    "            self.V = torch.nn.Sequential(\n",
    "                linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                linear(512, self.num_atoms)\n",
    "            )\n",
    "        self.output_dim = self.action_num * self.num_atoms\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "        state: typing.Optional[typing.Any] = None,\n",
    "        info: typing.Dict[str, typing.Any] = {},\n",
    "    ) -> typing.Tuple[torch.Tensor, typing.Any]:\n",
    "        r\"\"\"Mapping: x -> Z(x, \\*).\"\"\"\n",
    "        obs, state = super().forward(obs)\n",
    "        q = self.Q(obs)\n",
    "        q = q.view(-1, self.action_num, self.num_atoms)\n",
    "        if self._is_dueling:\n",
    "            v = self.V(obs)\n",
    "            v = v.view(-1, 1, self.num_atoms)\n",
    "            logits = q - q.mean(dim=1, keepdim=True) + v\n",
    "        else:\n",
    "            logits = q\n",
    "        probs = logits.softmax(dim=2)\n",
    "        return probs, state\n",
    "    \n",
    "    \n",
    "def rainbow_policy(state_shape: tuple,\n",
    "                  action_shape: tuple,\n",
    "                  optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                  learning_rate: float =  0.0000625,\n",
    "                  gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                  n_step: int = 1, # Number of steps to look ahead\n",
    "                  num_atoms: int = 51,\n",
    "                  target_update_freq: int = 500):\n",
    "    \n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Rainbow network to be used by policy\n",
    "    net = Rainbow(c= 1,\n",
    "                  h= state_shape[0],\n",
    "                  w= state_shape[1],\n",
    "                  action_shape= action_shape,                  \n",
    "                  device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.RainbowPolicy(model= net,\n",
    "                                   optim= optim,\n",
    "                                   discount_factor= gamma,\n",
    "                                   num_atoms= num_atoms,\n",
    "                                   estimation_step= n_step,\n",
    "                                   target_update_freq= target_update_freq).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9d840",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Setup the game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETUP THE GAME\n",
    "####################################################\n",
    "\n",
    "# Specify either an instance of an object that can predict a move or \"me\" for player 1\n",
    "player1 = \"me\"\n",
    "\n",
    "# Specify either an instance of an object that can predict a move or \"me\" for player 2\n",
    "player2 = rainbow_policy(state_shape= (6, 7),\n",
    "                         action_shape= (7,))\n",
    "player2.load_state_dict(torch.load(\"../paper_notebooks/./saved_variables/paper_notebooks/9/rainbow_vs_rainbow/best_policy_agent2.pth\"))\n",
    "\n",
    "# Play the game\n",
    "game.play_game(player1= player1,\n",
    "               player2= player2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77fae5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# REMOVE UNUSED VARIABLES\n",
    "####################################################\n",
    "\n",
    "del player1\n",
    "del player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16d376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
