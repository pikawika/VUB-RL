{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Human vs bot game\n",
    "\n",
    "In this notebook, we give a tutorial on how to run a pygame between two differing agents, where there is an option to be the agent in a game yourself.\n",
    "Thus, this notebook guides you in using a bot as the opponent in the connect four game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Loading policies\n",
    "  - MLP based DQN\n",
    "  - CNN based DQN\n",
    "  - CNN based Rainbow\n",
    "  - MiniMax policy\n",
    "- Loading in torch dictionaries\n",
    "- Setup the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: rl-project\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tianshou version (0.4.8 recommended): 0.4.8\n",
      "Torch version (1.11.0 recommended): 1.12.0.dev20220520+cu116\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Tianshou for RL algorithms\n",
    "import tianshou as ts; print(f\"Tianshou version (0.4.8 recommended): {ts.__version__}\")\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.11.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import human_vs_bot_connect4.human_vs_bot_connect_four as game\n",
    "import minimax_agent.minimax_agent as minimaxbot\n",
    "importlib.invalidate_caches();\n",
    "importlib.reload(game);\n",
    "importlib.reload(minimaxbot);\n",
    "\n",
    "# More data types\n",
    "import typing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9f5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "\n",
      "Amount of connected devices supporting CUDA: 1\n",
      "\n",
      "Current CUDA device: 0\n",
      "Cuda device 0 name: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CUDA VALIDATION\n",
    "####################################################\n",
    "\n",
    "# Check cuda available\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Show cuda devices\n",
    "print(f\"\\nAmount of connected devices supporting CUDA: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Show current cuda device\n",
    "    print(f\"\\nCurrent CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "    # Show cuda device name\n",
    "    print(f\"Cuda device 0 name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826657c4",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Loading policies\n",
    "\n",
    "We need to specify the policies the trained agent's weight are for.\n",
    "These are taken from previous notebooks.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### MLP based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a940ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY FROM PAPER NOTEBOOK 5\n",
    "####################################################\n",
    "\n",
    "class CustomDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(np.prod(state_shape), 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, np.prod(action_shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "def cf_custom_dqn_policy(state_shape: tuple,\n",
    "                         action_shape: tuple,\n",
    "                         learning_rate: float =  0.0001,\n",
    "                         gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                         n_step: int = 1, # Number of steps to look ahead\n",
    "                         target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CustomDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618df158",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### CNN based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2eb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DQN POLICY FROM PAPER NOTEBOOK 7\n",
    "####################################################\n",
    "\n",
    "class CNNBasedDQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom DQN using a model based on CNN\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',):\n",
    "        # Parent call\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save device (e.g. cuda)\n",
    "        self.device = device\n",
    "        \n",
    "        # Number of input channels\n",
    "        input_channels_cnn = 1\n",
    "        output_channels_cnn = 32\n",
    "        flatten_size = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        output_size= np.prod(action_shape)\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(0,-1),\n",
    "            torch.nn.Unflatten(0, (1, flatten_size)),\n",
    "            torch.nn.Linear(flatten_size, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 128), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "        \n",
    "        logits = self.model(obs)\n",
    "        return logits, state\n",
    "\n",
    "    \n",
    "def cf_cnn_dqn_policy(state_shape: tuple,\n",
    "                      action_shape: tuple,\n",
    "                      optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                      learning_rate: float =  0.0001,\n",
    "                      gamma: float = 0.9, # Smaller gamma favours \"faster\" win\n",
    "                      n_step: int = 4, # Number of steps to look ahead\n",
    "                      frozen: bool = False,\n",
    "                      target_update_freq: int = 320):\n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Network to be used for DQN\n",
    "    net = CNNBasedDQN(state_shape, action_shape, device= device).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # If we are frozen, we use an optimizer that has learning rate 0\n",
    "    if frozen:\n",
    "        optim = torch.optim.SGD(net.parameters(), lr= 0)\n",
    "        \n",
    "        \n",
    "    # Our agent DQN policy\n",
    "    return ts.policy.DQNPolicy(model= net,\n",
    "                               optim= optim,\n",
    "                               discount_factor= gamma,\n",
    "                               estimation_step= n_step,\n",
    "                               target_update_freq= target_update_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56467c69",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### CNN based Rainbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66219e45",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### CNN based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f748a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RAINBOW POLICY FROM PAPER NOTEBOOK 9\n",
    "####################################################\n",
    "\n",
    "class CNNForRainbow(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN to be used as baseclass for the Rainbow algorithm.\n",
    "    Extracts \"feautures\" for the Rainbow algorithm by doing a 4x4 cnn kernel pass and providing 16 filters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \n",
    "        # Torch init\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store device to be used\n",
    "        self.device = device\n",
    "        \n",
    "        # The input layer is singular -> we have 1 board vector\n",
    "        input_channels_cnn = 1\n",
    "        \n",
    "        # We output 64/16 filters per kernel \n",
    "        output_channels_cnn = 64 # Increased from 16\n",
    "        \n",
    "        # We store the output dimension of the CNN \"feature\" layer\n",
    "        self.output_dim = (state_shape[0] - 3) * (state_shape[1] - 3) * output_channels_cnn\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels= input_channels_cnn, out_channels= output_channels_cnn, kernel_size= 4, stride= 1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "                state: typing.Optional[typing.Any] = None,\n",
    "                info: typing.Dict[str, typing.Any] = {}):\n",
    "        # Make a torch instance (from regular vector of board)\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float, device=self.device)\n",
    "            \n",
    "        # Tianshou bugs the batch output, reshape to work properly with our torch version\n",
    "        if (len(np.shape(obs)) != 4):\n",
    "            obs = obs[:, None, :, :]\n",
    "        \n",
    "        # Return what is needed (network output & state)\n",
    "        return self.net(obs), state\n",
    "\n",
    "class Rainbow(CNNForRainbow):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow algorithm making using of the CNNForRainbow baseclass.\n",
    "    Default parameters adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_shape: typing.Sequence[int],\n",
    "                 action_shape: typing.Sequence[int],\n",
    "                 device: typing.Union[str, int, torch.device] = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                 num_atoms: int = 51,\n",
    "                 is_noisy: bool = True,\n",
    "                 noisy_std: float = 0.1,\n",
    "                 is_dueling: bool = True):\n",
    "        \n",
    "        # Init CNN feature extraction parent class\n",
    "        super().__init__(state_shape= state_shape, device= device)\n",
    "        \n",
    "        # the amount of actions we have is just the action shape\n",
    "        self.action_num = np.prod(action_shape)\n",
    "        \n",
    "        # Store class specific info\n",
    "        self.num_atoms = num_atoms\n",
    "        self._is_dueling = is_dueling\n",
    "\n",
    "        # Our linear layer depends on wether or not we want to use a noisy environment\n",
    "        # Noisy implementation based on https://arxiv.org/abs/1706.10295\n",
    "        def linear(x, y):\n",
    "            if is_noisy:\n",
    "                return ts.utils.net.discrete.NoisyLinear(x, y, noisy_std)\n",
    "            else:\n",
    "                return torch.nn.Linear(x, y)\n",
    "            \n",
    "        # Specify Q and V based on wether or not agent is dueling\n",
    "        # Setting agent on dueling mode should help generalisation according to rainbow paper\n",
    "        # NOTE: this uses the output dim from the feature extraction CNN\n",
    "        self.Q = torch.nn.Sequential(\n",
    "            linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "            linear(512, self.action_num * self.num_atoms))\n",
    "        \n",
    "        if self._is_dueling:\n",
    "            self.V = torch.nn.Sequential(\n",
    "                linear(self.output_dim, 512), torch.nn.ReLU(inplace=True),\n",
    "                linear(512, self.num_atoms))\n",
    "            \n",
    "        # New output dim for this rainbow network\n",
    "        self.output_dim = self.action_num * self.num_atoms\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                obs: typing.Union[np.ndarray, torch.Tensor],\n",
    "                state: typing.Optional[typing.Any] = None,\n",
    "                info: typing.Dict[str, typing.Any] = {}):\n",
    "        \n",
    "        # Use our parent CNN based network to get \"features\"\n",
    "        obs, state = super().forward(obs)\n",
    "        \n",
    "        # Get our Rainbow specific values\n",
    "        q = self.Q(obs)\n",
    "        q = q.view(-1, self.action_num, self.num_atoms)\n",
    "        \n",
    "        if self._is_dueling:\n",
    "            v = self.V(obs)\n",
    "            v = v.view(-1, 1, self.num_atoms)\n",
    "            logits = q - q.mean(dim=1, keepdim=True) + v\n",
    "        else:\n",
    "            logits = q\n",
    "        \n",
    "        # We need to go from our logits to an accepted dimension of probability outputs\n",
    "        probs = logits.softmax(dim=2)\n",
    "        \n",
    "        return probs, state\n",
    "    \n",
    "    \n",
    "def rainbow_policy(state_shape: tuple,\n",
    "                   action_shape: tuple,\n",
    "                   optim: typing.Optional[torch.optim.Optimizer] = None,\n",
    "                   learning_rate: float =  0.0000625,\n",
    "                   gamma: float = 0.9,\n",
    "                   n_step: int = 3, # Number of steps to look ahead\n",
    "                   num_atoms: int = 51,\n",
    "                   is_noisy: bool = True,\n",
    "                   noisy_std: float = 0.1,\n",
    "                   is_dueling: bool = True,\n",
    "                   target_update_freq: int = 500):\n",
    "    \"\"\"\n",
    "    Implementation of the Rainbow policy.\n",
    "    Default parameters adopted from: https://github.com/thu-ml/tianshou/blob/master/examples/atari/atari_rainbow.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use cuda device if possible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Rainbow network to be used by policy\n",
    "    net = Rainbow(state_shape= state_shape,\n",
    "                  action_shape= action_shape,\n",
    "                  device= device,\n",
    "                  num_atoms= num_atoms,\n",
    "                  is_noisy= is_noisy,\n",
    "                  noisy_std= noisy_std,\n",
    "                  is_dueling= is_dueling).to(device)\n",
    "    \n",
    "    # Default optimizer is an adam optimizer with the argparser learning rate\n",
    "    if optim is None:\n",
    "        optim = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "        \n",
    "    # Our agents Rainbow policy\n",
    "    return ts.policy.RainbowPolicy(model= net,\n",
    "                                   optim= optim,\n",
    "                                   discount_factor= gamma,\n",
    "                                   num_atoms= num_atoms,\n",
    "                                   estimation_step= n_step,\n",
    "                                   target_update_freq= target_update_freq).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ffe2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### MiniMax policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4213b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CUSTOM MINIMAX TIANSHOU POLICY\n",
    "####################################################\n",
    "\n",
    "class TianshouMiniMaxConnectFourPolicy(ts.policy.BasePolicy):\n",
    "    \"\"\"\n",
    "    Tianshou compatible MiniMax policy for connect four.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 coin: int,\n",
    "                 oponent_coin: int,\n",
    "                 minimax_depth: int,\n",
    "                 column_count: int = 7,\n",
    "                 row_count: int = 6,\n",
    "                 **kwargs: typing.Any):\n",
    "        # Init base policy\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Configure minimax bot\n",
    "        self.bot = minimaxbot.MiniMaxConnectFourBot(coin= coin,\n",
    "                                                    oponent_coin= oponent_coin,\n",
    "                                                    column_count= column_count,\n",
    "                                                    row_count= row_count,\n",
    "                                                    minimax_depth= minimax_depth)\n",
    "\n",
    "    def forward(self,\n",
    "                batch: ts.data.Batch,\n",
    "                state: typing.Optional[typing.Union[dict, ts.data.Batch, np.ndarray]] = None,\n",
    "                **kwargs: typing.Any):\n",
    "        \"\"\"\n",
    "        Compute minimax action over the given batch data.\n",
    "        \"\"\"\n",
    "        boards = batch[\"obs\"]\n",
    "        \n",
    "        # Can be nested in Tianshou\n",
    "        while isinstance(boards, ts.data.Batch):\n",
    "            boards = boards[\"obs\"]\n",
    "        \n",
    "        preds = [None] * len(boards)        \n",
    "        \n",
    "        for i in range(len(boards)):\n",
    "            preds[i] = self.bot.predict(board= boards[i])\n",
    "            \n",
    "        \n",
    "        return ts.data.Batch(act=preds, state=state)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        # No learning needed\n",
    "        return {}\n",
    "    \n",
    "    def set_eps(self, eps):\n",
    "        # Not needed\n",
    "        return\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffc0e6",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Loading in torch dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4c5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# FUNCTION FOR LOADING IN TORCH DICTIONARIIES\n",
    "####################################################\n",
    "\n",
    "def load_torch_dict(filename):\n",
    "    \"\"\"\n",
    "    Loads in torch dictionary using correct cuda settings for current device\n",
    "    \"\"\"   \n",
    "    if torch.cuda.is_available():\n",
    "        return torch.load(filename)\n",
    "    else:\n",
    "        return torch.load(filename, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9d840",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Setup the game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b47a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# SETUP THE GAME\n",
    "####################################################\n",
    "\n",
    "if (True):\n",
    "    # Player 1 is a pytorch bot\n",
    "    player1 = rainbow_policy(state_shape= (6, 7),\n",
    "                            action_shape= (7,),\n",
    "                            learning_rate=  0.0000625,\n",
    "                            gamma= 0.9,\n",
    "                            n_step= 3, # Number of steps to look ahead\n",
    "                            num_atoms= 51,\n",
    "                            is_noisy= True,\n",
    "                            noisy_std= 0.1,\n",
    "                            is_dueling= True)\n",
    "    player1.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/11/1-250epoch_5loop/looping-iteration-4/best_policy_agent1.pth\"))\n",
    "    player1.set_eps(0)\n",
    "\n",
    "if (False):\n",
    "    # Player 1 is a minimax bot\n",
    "    player1 = TianshouMiniMaxConnectFourPolicy(coin= 1,\n",
    "                                               oponent_coin= 2,\n",
    "                                               column_count= game.GRID_COLUMN_COUNT,\n",
    "                                               row_count= game.GRID_ROW_COUNT,\n",
    "                                               minimax_depth= 5)\n",
    "    \n",
    "if (False):\n",
    "    # We are player 1\n",
    "    player1 = \"me\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Specify either an instance of an object that can predict a move or \"me\" for player 2\n",
    "if (False):\n",
    "    # Player 2 is a pytorch bot\n",
    "    player2 = rainbow_policy(state_shape= (6, 7),\n",
    "                            action_shape= (7,),\n",
    "                            learning_rate=  0.0000625,\n",
    "                            gamma= 0.9,\n",
    "                            n_step= 3, # Number of steps to look ahead\n",
    "                            num_atoms= 51,\n",
    "                            is_noisy= True,\n",
    "                            noisy_std= 0.1,\n",
    "                            is_dueling= True)\n",
    "    player2.load_state_dict(load_torch_dict(\"../paper_notebooks/./saved_variables/paper_notebooks/10/1-500epoch_20loop/looping-iteration-9/best_policy_agent2.pth\"))\n",
    "    player2.set_eps(0)\n",
    "\n",
    "if (False):\n",
    "    # Player 2 is a minimax bot\n",
    "    player2 = minimaxbot.MiniMaxConnectFourBot(coin= 2,\n",
    "                                               oponent_coin= 1,\n",
    "                                               column_count= game.GRID_COLUMN_COUNT,\n",
    "                                               row_count= game.GRID_ROW_COUNT,\n",
    "                                               minimax_depth= 5)  \n",
    "if (True):\n",
    "    # We are player 2\n",
    "    player2 = \"me\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Play the game\n",
    "game.play_game(player1= player1,\n",
    "               player2= player2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fae5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# REMOVE UNUSED VARIABLES\n",
    "####################################################\n",
    "\n",
    "del player1\n",
    "del player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16d376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
