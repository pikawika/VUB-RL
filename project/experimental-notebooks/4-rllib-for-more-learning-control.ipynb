{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Using RLlib for more multi-agent learning control\n",
    "\n",
    "As discussed in `5-improving-dqn-architecture.ipynb` we thought of three aspects that might be the root of the agent's not learning to play the game pleasingly:\n",
    "- Training two DQN agents simultaneously is known to be though, especially when starting from a random initialisation\n",
    "- The network used was a simple MLP\n",
    "- The training is not done over enough iterations\n",
    "\n",
    "In the notebooks `5-improving-dqn-architecture.ipynb` and `6-dqn-using-a-cnn.ipynb`, two alternative networks besides MLP were used.\n",
    "Whilst these give somewhat satisfactory results when trained for long enough and incentivising moves by giving a reward for making a move, it is still far from perfect.\n",
    "The iterations were also boosted to a couple of hours on a CUDA GPU, which didn't improve things all that much.\n",
    "\n",
    "Thus, what is most likely to be an issue is the fact that we are training two agents simultaneously.\n",
    "This makes it hard to get a good performing agent.\n",
    "An alternative to this is training an agent for a couple of epochs whilst freezing the other and alternating this between the agents.\n",
    "This makes the problem to learn \"stationary\" in a certain way and is known to make learning easier.\n",
    "What is also done, often in very complex games, is starting from a somewhat smart agent instead of a random one.\n",
    "\n",
    "This notebook will use [Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html), which is better documented for use in multi-agent environments and PettingZoo like environments in particular.\n",
    "They also note that zero-sum environments are harder to learn in multi-agent settings.\n",
    "That is why we introduce a reward for making moves and a high reward for playing a tie game.\n",
    "We hope to create agents that are capable of reaching a tie board or extending losses maximally in this manner.\n",
    "\n",
    "We will use portions of the [Ray documentation and examples in this notebook](https://docs.ray.io/en/latest/rllib/rllib-examples.html).\n",
    "This includes following files on public GitHub repositories:\n",
    "- `multi_agent_independent_learning.py` from the [Ray GitHub repository](https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_independent_learning.py).\n",
    "- `multi_agent_parameter_sharing.py` from the [Ray GitHub repository](https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_parameter_sharing.py).\n",
    "- `rllib_pistonball.py` from the [Petting Zoo GitHub repository](https://github.com/Farama-Foundation/PettingZoo/blob/master/tutorials/rllib_pistonball.py).\n",
    "\n",
    "Alongside these documents and files, a tutorial by[ J K Terry on using RLlib in Petting Zoo environments](https://towardsdatascience.com/using-pettingzoo-with-rllib-for-multi-agent-deep-reinforcement-learning-5ff47c677abd) was also used.\n",
    "\n",
    "# IMPORTANT: BUGGY NOTEBOOK\n",
    "This notebook doesn't work due to issues related to the one reported [here](https://github.com/ray-project/ray/issues/22976).\n",
    "This along with the fact that working with custom Petting Zoo like environment throws random errors, left us to beleive that the Ray RL Lib is sadly not the way to go.\n",
    "\n",
    "Indeed, our model has values that are `None` which throws the following error:\n",
    "\n",
    "> Can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\n",
    "\n",
    "Since editing the source code of Ray RL lib is asking for troubles we leave our exploration of this library as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Contact information\n",
    "- Checking requirements\n",
    "  - Correct Anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct CUDA access\n",
    "- Training Connect Four agents with Ray RLlib\n",
    "  - Trying out Ray RL lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Contact information\n",
    "\n",
    "| Name             | Student ID | VUB mail                                                  | Personal mail                                               |\n",
    "| ---------------- | ---------- | --------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Lennert Bontinck | 0568702    | [lennert.bontinck@vub.be](mailto:lennert.bontinck@vub.be) | [info@lennertbontinck.com](mailto:info@lennertbontinck.com) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a405d24",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements\n",
    "\n",
    "### Correct Anaconda environment\n",
    "\n",
    "The `rl-project` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the RL course project and homeworks](https://github.com/pikawika/vub-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'rl-project'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab632204",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Allow reloading of libraries\n",
    "import importlib\n",
    "# Ray RLlib for RL algorithms instead of Tianshou\n",
    "import ray; print(f\"Ray version (1.12.1 recommended): {ray.__version__}\")\n",
    "import ray.rllib\n",
    "\n",
    "# Torch is a popular DL framework\n",
    "import torch; print(f\"Torch version (1.12.0 recommended): {torch.__version__}\")\n",
    "\n",
    "# Gym environment\n",
    "import gym; print(f\"Gym version (0.21.0 recommended): {gym.__version__}\")\n",
    "\n",
    "# Our custom connect four gym environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gym_connect4_pygame.envs.ConnectFourPygameEnvV2 as cfgym;\n",
    "importlib.invalidate_caches();\n",
    "importlib.reload(cfgym);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617a21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct CUDA access\n",
    "\n",
    "The installation instructions specify how to install PyTorch with CUDA 11.6.\n",
    "The following code block tests if this was done successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac22356",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Training Connect Four agents with Ray RLlib\n",
    "\n",
    "As discussed, this notebook will use Ray RLlib to train two agents for Connect four.\n",
    "\n",
    "### Trying out Ray RL lib\n",
    "\n",
    "We try out the Ray RL lib and do this on the Petting Zoo provided Connect Four game.\n",
    "Whilst the training works, the saved files cause an issue for loading and thus for replaying.\n",
    "Becuase this is a straight copy from the documentation with only the environment changed, we see no reason why it should not work and discard further experiments with this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import ray\n",
    "from gym.spaces import Box\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_torch_model import DQNTorchModel\n",
    "from ray.rllib.agents.registry import get_trainer_class\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class TorchMaskedActions(DQNTorchModel):\n",
    "    \"\"\"PyTorch version of above ParametricActionsModel.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, **kw):\n",
    "        DQNTorchModel.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name, **kw\n",
    "        )\n",
    "\n",
    "        obs_len = obs_space.shape[0] - action_space.n\n",
    "\n",
    "        orig_obs_space = Box(\n",
    "            shape=(obs_len,), low=obs_space.low[:obs_len], high=obs_space.high[:obs_len]\n",
    "        )\n",
    "        self.action_embed_model = TorchFC(\n",
    "            orig_obs_space,\n",
    "            action_space,\n",
    "            action_space.n,\n",
    "            model_config,\n",
    "            name + \"_action_embed\",\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "\n",
    "        # Compute the predicted action embedding\n",
    "        action_logits, _ = self.action_embed_model(\n",
    "            {\"obs\": input_dict[\"obs\"][\"observation\"]}\n",
    "        )\n",
    "        # turns probit action mask into logit action mask\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), -1e10, FLOAT_MAX)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e69fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = \"DQN\"\n",
    "ModelCatalog.register_custom_model(\"pa_model\", TorchMaskedActions)\n",
    "# function that outputs the environment you wish to register.\n",
    "\n",
    "my_env = cfgym.env()\n",
    "\n",
    "def env_creator():\n",
    "    env = connect_four_v3.env()\n",
    "    return env\n",
    "\n",
    "num_cpus = 1\n",
    "\n",
    "config = deepcopy(get_trainer_class(alg_name)._default_config)\n",
    "\n",
    "register_env(\"leduc_holdem\", lambda config: PettingZooEnv(env_creator()))\n",
    "\n",
    "test_env = PettingZooEnv(env_creator())\n",
    "obs_space = test_env.observation_space\n",
    "print(obs_space)\n",
    "act_space = test_env.action_space\n",
    "\n",
    "config[\"multiagent\"] = {\n",
    "    \"policies\": {\n",
    "        \"player_0\": (None, obs_space, act_space, {}),\n",
    "        \"player_1\": (None, obs_space, act_space, {}),\n",
    "    },\n",
    "    \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
    "}\n",
    "\n",
    "config[\"num_gpus\"] = int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "config[\"log_level\"] = \"DEBUG\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"rollout_fragment_length\"] = 30\n",
    "config[\"train_batch_size\"] = 200\n",
    "config[\"horizon\"] = 200\n",
    "config[\"no_done_at_end\"] = False\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"model\"] = {\n",
    "    \"custom_model\": \"pa_model\",\n",
    "}\n",
    "config[\"n_step\"] = 1\n",
    "\n",
    "config[\"exploration_config\"] = {\n",
    "    # The Exploration class to use.\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    # Config for the Exploration class' constructor:\n",
    "    \"initial_epsilon\": 0.1,\n",
    "    \"final_epsilon\": 0.0,\n",
    "    \"epsilon_timesteps\": 100000,  # Timesteps over which to anneal epsilon.\n",
    "}\n",
    "config[\"hiddens\"] = []\n",
    "config[\"dueling\"] = False\n",
    "config[\"env\"] = \"leduc_holdem\"\n",
    "\n",
    "ray.init(num_cpus=num_cpus + 1)\n",
    "\n",
    "tune.run(\n",
    "    alg_name,\n",
    "    name=\"DQN\",\n",
    "    stop={\"timesteps_total\": 5000},\n",
    "    checkpoint_freq=10,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62207afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import PIL\n",
    "import ray\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.registry import get_trainer_class\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = os.path.expanduser(\"C:/Users/Lennert/ray_results/DQN/DQN_leduc_holdem_63720_00000_0_2022-05-26_15-04-34/checkpoint_000130/checkpoint-130\")\n",
    "params_path = Path(checkpoint_path).parent.parent / \"params.pkl\"\n",
    "\n",
    "\n",
    "alg_name = \"DQN\"\n",
    "ModelCatalog.register_custom_model(\"pa_model\", TorchMaskedActions)\n",
    "# function that outputs the environment you wish to register.\n",
    "\n",
    "\n",
    "def env_creator():\n",
    "    env = connect_four_v3.env()\n",
    "    return env\n",
    "\n",
    "\n",
    "num_cpus = 1\n",
    "\n",
    "config = deepcopy(get_trainer_class(alg_name)._default_config)\n",
    "\n",
    "register_env(\"leduc_holdem\", lambda config: PettingZooEnv(env_creator()))\n",
    "\n",
    "env = env_creator()\n",
    "# obs_space = env.observation_space\n",
    "# print(obs_space)\n",
    "# act_space = test_env.action_space\n",
    "\n",
    "with open(params_path, \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "    # num_workers not needed since we are not training\n",
    "    del config[\"num_workers\"]\n",
    "    del config[\"num_gpus\"]\n",
    "\n",
    "#ray.init(num_cpus=8, num_gpus=0)\n",
    "DQNAgent = DQNTrainer(env=\"leduc_holdem\", config=config)\n",
    "DQNAgent.restore(checkpoint_path)\n",
    "\n",
    "reward_sums = {a: 0 for a in env.possible_agents}\n",
    "i = 0\n",
    "env.reset()\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, done, info = env.last()\n",
    "    obs = observation[\"observation\"]\n",
    "    reward_sums[agent] += reward\n",
    "    if done:\n",
    "        action = None\n",
    "    else:\n",
    "        print(DQNAgent.get_policy(agent))\n",
    "        policy = DQNAgent.get_policy(agent)\n",
    "        batch_obs = {\n",
    "            \"obs\": {\n",
    "                \"observation\": np.expand_dims(observation[\"observation\"], 0),\n",
    "                \"action_mask\": np.expand_dims(observation[\"action_mask\"], 0),\n",
    "            }\n",
    "        }\n",
    "        batched_action, state_out, info = policy.compute_actions_from_input_dict(\n",
    "            batch_obs\n",
    "        )\n",
    "        single_action = batched_action[0]\n",
    "        action = single_action\n",
    "\n",
    "    env.step(action)\n",
    "    i += 1\n",
    "    env.render()\n",
    "\n",
    "print(\"rewards:\")\n",
    "print(reward_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c15f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b88c88564fefe7444548986d165ad8d7f764d0079ffa923785a3f5a89d52c74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
